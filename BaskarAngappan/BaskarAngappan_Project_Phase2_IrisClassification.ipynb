{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length</th>\n",
       "      <th>sepal width</th>\n",
       "      <th>petal length</th>\n",
       "      <th>petal width</th>\n",
       "      <th>iris</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "      <th>Unnamed: 6</th>\n",
       "      <th>sepal length.1</th>\n",
       "      <th>Unnamed: 8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Count</td>\n",
       "      <td>150.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Minimum</td>\n",
       "      <td>4.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Maximum</td>\n",
       "      <td>7.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mean</td>\n",
       "      <td>5.843333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Median</td>\n",
       "      <td>5.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>Iris-virginica</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Iris-virginica</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal length  sepal width  petal length  petal width            iris  \\\n",
       "0             5.1          3.5           1.4          0.2     Iris-setosa   \n",
       "1             4.9          3.0           1.4          0.2     Iris-setosa   \n",
       "2             4.7          3.2           1.3          0.2     Iris-setosa   \n",
       "3             4.6          3.1           1.5          0.2     Iris-setosa   \n",
       "4             5.0          3.6           1.4          0.2     Iris-setosa   \n",
       "..            ...          ...           ...          ...             ...   \n",
       "145           6.7          3.0           5.2          2.3  Iris-virginica   \n",
       "146           6.3          2.5           5.0          1.9  Iris-virginica   \n",
       "147           6.5          3.0           5.2          2.0  Iris-virginica   \n",
       "148           6.2          3.4           5.4          2.3  Iris-virginica   \n",
       "149           5.9          3.0           5.1          1.8  Iris-virginica   \n",
       "\n",
       "     Unnamed: 5  Unnamed: 6 sepal length.1  Unnamed: 8  \n",
       "0           NaN         NaN          Count  150.000000  \n",
       "1           NaN         NaN        Minimum    4.300000  \n",
       "2           NaN         NaN        Maximum    7.900000  \n",
       "3           NaN         NaN           Mean    5.843333  \n",
       "4           NaN         NaN         Median    5.800000  \n",
       "..          ...         ...            ...         ...  \n",
       "145         NaN         NaN            NaN         NaN  \n",
       "146         NaN         NaN            NaN         NaN  \n",
       "147         NaN         NaN            NaN         NaN  \n",
       "148         NaN         NaN            NaN         NaN  \n",
       "149         NaN         NaN            NaN         NaN  \n",
       "\n",
       "[150 rows x 9 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import style\n",
    "\n",
    "style.use('ggplot')\n",
    "\n",
    "df = pd.read_excel('iris.xls')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length</th>\n",
       "      <th>sepal width</th>\n",
       "      <th>petal length</th>\n",
       "      <th>petal width</th>\n",
       "      <th>iris</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal length  sepal width  petal length  petal width            iris\n",
       "0             5.1          3.5           1.4          0.2     Iris-setosa\n",
       "1             4.9          3.0           1.4          0.2     Iris-setosa\n",
       "2             4.7          3.2           1.3          0.2     Iris-setosa\n",
       "3             4.6          3.1           1.5          0.2     Iris-setosa\n",
       "4             5.0          3.6           1.4          0.2     Iris-setosa\n",
       "..            ...          ...           ...          ...             ...\n",
       "145           6.7          3.0           5.2          2.3  Iris-virginica\n",
       "146           6.3          2.5           5.0          1.9  Iris-virginica\n",
       "147           6.5          3.0           5.2          2.0  Iris-virginica\n",
       "148           6.2          3.4           5.4          2.3  Iris-virginica\n",
       "149           5.9          3.0           5.1          1.8  Iris-virginica\n",
       "\n",
       "[150 rows x 5 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(['Unnamed: 5', 'Unnamed: 6', 'sepal length.1', 'Unnamed: 8'], 1, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using KNN is :  0.9333333333333333\n",
      "[[5.1 3.4 1.4 1.6]]\n",
      "['Iris-setosa']\n"
     ]
    }
   ],
   "source": [
    "# KNN\n",
    "from sklearn import preprocessing,model_selection as cross_validation,neighbors\n",
    "\n",
    "X = np.array(df.drop(['iris'], 1))\n",
    "y = np.array(df['iris'])\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size=0.2)\n",
    "clf = neighbors.KNeighborsClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "accuracy = clf.score(X_test, y_test)\n",
    "print(\"Accuracy using KNN is : \", accuracy)\n",
    "example_measures = np.array([5.1, 3.4, 1.4, 1.6]).reshape(1, -1)\n",
    "print(example_measures)\n",
    "prediction = clf.predict(example_measures)\n",
    "print(prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "                       max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort=False,\n",
      "                       random_state=0, splitter='best')\n",
      "Accuracy on the training subset: 1.000\n",
      "Accuracy using Decision Tree is : 0.921\n",
      "Predict:  [[5.1 3.4 1.4 1.6]]\n",
      "['Iris-versicolor']\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Decision Tree Classifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, stratify=y, random_state=42)\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf)\n",
    "\n",
    "print('Accuracy on the training subset: {:.3f}'.format(clf.score(X_train, y_train)))\n",
    "print('Accuracy using Decision Tree is : {:.3f}'.format(clf.score(X_test, y_test)))\n",
    "example_measures = np.array([5.1, 3.4, 1.4, 1.6]).reshape(1, -1)\n",
    "print(\"Predict: \", example_measures)\n",
    "prediction = clf.predict(example_measures)\n",
    "print(prediction)\n",
    "\n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus\n",
    "from sklearn.externals.six import StringIO  \n",
    "from IPython.display import Image\n",
    "\n",
    "dot_data = StringIO()\n",
    "export_graphviz(clf, out_file=dot_data,  \n",
    "                filled=True, rounded=True,\n",
    "                special_characters=True)\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "Image(graph.create_png())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using SVM is :  0.9\n",
      "Predict:  [[5.1 3.4 1.4 1.6]]\n",
      "['Iris-setosa']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# SVM\n",
    "from sklearn import preprocessing, model_selection as cross_validation, neighbors, svm\n",
    "\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "clf = svm.SVC()\n",
    "#Run SVM and calculate confidence\n",
    "clf.fit(X_train, y_train)\n",
    "confidence = clf.score(X_test, y_test)\n",
    "print(\"Accuracy using SVM is : \", confidence)\n",
    "\n",
    "example_measures = np.array([[5.1, 3.4, 1.4, 1.6]])\n",
    "#-1 in reshape means we want numpy to figure out this dimension\n",
    "example_measures = example_measures.reshape(len(example_measures), -1)\n",
    "print(\"Predict: \", example_measures)\n",
    "prediction = clf.predict(example_measures)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy is : 0.9666666666666667\n",
      "Decision Tree Accuracy is : 0.8666666666666667\n",
      "Voting Classifier Accuracy is : 0.9\n",
      "Predict:  [[5.1 3.4 1.4 1.6]]\n",
      "['Iris-setosa']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Voting Classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from statistics import mode\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "\n",
    "model1 = LogisticRegression(random_state=1)\n",
    "model2 = DecisionTreeClassifier(random_state=1)\n",
    "\n",
    "model1.fit(X_train,y_train)\n",
    "print('Logistic Regression Accuracy is :',model1.score(X_test,y_test))\n",
    "\n",
    "model2.fit(X_train,y_train)\n",
    "print('Decision Tree Accuracy is :',model2.score(X_test,y_test))\n",
    "\n",
    "# hard means all of them have to vote\n",
    "model = VotingClassifier(estimators=[('lr', model1), ('dt', model2)], voting='hard')\n",
    "model.fit(X_train,y_train)\n",
    "print('Voting Classifier Accuracy is :', model.score(X_test,y_test))\n",
    "\n",
    "print(\"Predict: \", example_measures)\n",
    "prediction = clf.predict(example_measures)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Classifier Accuracy is :  0.8666666666666667\n",
      "Predict:  [[5.1 3.4 1.4 1.6]]\n",
      "['Iris-setosa']\n",
      "Random Forest Accuracy is : 0.8666666666666667\n",
      "Predict:  [[5.1 3.4 1.4 1.6]]\n",
      "['Iris-setosa']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Bagging Classifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "model = BaggingClassifier(DecisionTreeClassifier(random_state=1))\n",
    "model.fit(X_train, y_train)\n",
    "print('Bagging Classifier Accuracy is : ', model.score(X_test, y_test))\n",
    "print(\"Predict: \", example_measures)\n",
    "prediction = clf.predict(example_measures)\n",
    "print(prediction)\n",
    "\n",
    "# Random Forest Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model= RandomForestClassifier(random_state=1)\n",
    "model.fit(X_train, y_train)\n",
    "print('Random Forest Accuracy is :', model.score(X_test,y_test))\n",
    "print(\"Predict: \", example_measures)\n",
    "prediction = clf.predict(example_measures)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ada Boost Accuracy is :  0.8666666666666667\n",
      "Predict:  [[5.1 3.4 1.4 1.6]]\n",
      "['Iris-setosa']\n",
      "XGBoost Accuracy is : 0.8666666666666667\n",
      "Predict:  [[5.1 3.4 1.4 1.6]]\n",
      "['Iris-setosa']\n"
     ]
    }
   ],
   "source": [
    "#AdaBoost - Adaptive boosting\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "model = AdaBoostClassifier(random_state=1)\n",
    "model.fit(X_train, y_train)\n",
    "print('Ada Boost Accuracy is : ',model.score(X_test,y_test))\n",
    "print(\"Predict: \", example_measures)\n",
    "prediction = clf.predict(example_measures)\n",
    "print(prediction)\n",
    "\n",
    "#Extreme Gradient Boosting has pruning and regularization\n",
    "import xgboost as xgb\n",
    "model=xgb.XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "print('XGBoost Accuracy is :',model.score(X_test,y_test))\n",
    "print(\"Predict: \", example_measures)\n",
    "prediction = clf.predict(example_measures)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Altered data set:\n",
      "     sepal length  sepal width  petal length  petal width  iris\n",
      "0             5.1          3.5           1.4          0.2     2\n",
      "1             4.9          3.0           1.4          0.2     2\n",
      "2             4.7          3.2           1.3          0.2     2\n",
      "3             4.6          3.1           1.5          0.2     2\n",
      "4             5.0          3.6           1.4          0.2     2\n",
      "..            ...          ...           ...          ...   ...\n",
      "145           6.7          3.0           5.2          2.3     1\n",
      "146           6.3          2.5           5.0          1.9     1\n",
      "147           6.5          3.0           5.2          2.0     1\n",
      "148           6.2          3.4           5.4          2.3     1\n",
      "149           5.9          3.0           5.1          1.8     1\n",
      "\n",
      "[150 rows x 5 columns]\n",
      "Train on 120 samples, validate on 30 samples\n",
      "Epoch 1/2500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.6363 - accuracy: 0.6500 - val_loss: 0.5545 - val_accuracy: 0.7333\n",
      "Epoch 2/2500\n",
      "120/120 [==============================] - 0s 130us/step - loss: 0.6282 - accuracy: 0.6500 - val_loss: 0.5545 - val_accuracy: 0.7333\n",
      "Epoch 3/2500\n",
      "120/120 [==============================] - 0s 128us/step - loss: 0.6239 - accuracy: 0.6500 - val_loss: 0.5547 - val_accuracy: 0.7333\n",
      "Epoch 4/2500\n",
      "120/120 [==============================] - 0s 137us/step - loss: 0.6197 - accuracy: 0.6500 - val_loss: 0.5544 - val_accuracy: 0.7333\n",
      "Epoch 5/2500\n",
      "120/120 [==============================] - 0s 139us/step - loss: 0.6170 - accuracy: 0.6500 - val_loss: 0.5533 - val_accuracy: 0.7333\n",
      "Epoch 6/2500\n",
      "120/120 [==============================] - 0s 130us/step - loss: 0.6150 - accuracy: 0.6500 - val_loss: 0.5531 - val_accuracy: 0.7333\n",
      "Epoch 7/2500\n",
      "120/120 [==============================] - 0s 129us/step - loss: 0.6121 - accuracy: 0.6500 - val_loss: 0.5519 - val_accuracy: 0.7333\n",
      "Epoch 8/2500\n",
      "120/120 [==============================] - 0s 141us/step - loss: 0.6097 - accuracy: 0.6500 - val_loss: 0.5496 - val_accuracy: 0.7333\n",
      "Epoch 9/2500\n",
      "120/120 [==============================] - 0s 140us/step - loss: 0.6071 - accuracy: 0.6500 - val_loss: 0.5473 - val_accuracy: 0.7333\n",
      "Epoch 10/2500\n",
      "120/120 [==============================] - 0s 147us/step - loss: 0.6045 - accuracy: 0.6500 - val_loss: 0.5453 - val_accuracy: 0.7333\n",
      "Epoch 11/2500\n",
      "120/120 [==============================] - 0s 151us/step - loss: 0.6015 - accuracy: 0.6500 - val_loss: 0.5421 - val_accuracy: 0.7333\n",
      "Epoch 12/2500\n",
      "120/120 [==============================] - 0s 127us/step - loss: 0.5989 - accuracy: 0.6500 - val_loss: 0.5397 - val_accuracy: 0.7333\n",
      "Epoch 13/2500\n",
      "120/120 [==============================] - 0s 134us/step - loss: 0.5952 - accuracy: 0.6500 - val_loss: 0.5357 - val_accuracy: 0.7333\n",
      "Epoch 14/2500\n",
      "120/120 [==============================] - 0s 139us/step - loss: 0.5917 - accuracy: 0.6500 - val_loss: 0.5319 - val_accuracy: 0.7333\n",
      "Epoch 15/2500\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.5881 - accuracy: 0.6500 - val_loss: 0.5272 - val_accuracy: 0.7333\n",
      "Epoch 16/2500\n",
      "120/120 [==============================] - 0s 133us/step - loss: 0.5846 - accuracy: 0.6500 - val_loss: 0.5235 - val_accuracy: 0.7333\n",
      "Epoch 17/2500\n",
      "120/120 [==============================] - 0s 120us/step - loss: 0.5801 - accuracy: 0.6500 - val_loss: 0.5193 - val_accuracy: 0.7333\n",
      "Epoch 18/2500\n",
      "120/120 [==============================] - 0s 133us/step - loss: 0.5755 - accuracy: 0.6500 - val_loss: 0.5137 - val_accuracy: 0.7333\n",
      "Epoch 19/2500\n",
      "120/120 [==============================] - 0s 154us/step - loss: 0.5708 - accuracy: 0.6500 - val_loss: 0.5091 - val_accuracy: 0.7333\n",
      "Epoch 20/2500\n",
      "120/120 [==============================] - 0s 135us/step - loss: 0.5658 - accuracy: 0.6500 - val_loss: 0.5034 - val_accuracy: 0.7333\n",
      "Epoch 21/2500\n",
      "120/120 [==============================] - 0s 116us/step - loss: 0.5607 - accuracy: 0.6500 - val_loss: 0.4985 - val_accuracy: 0.7333\n",
      "Epoch 22/2500\n",
      "120/120 [==============================] - 0s 155us/step - loss: 0.5550 - accuracy: 0.6500 - val_loss: 0.4931 - val_accuracy: 0.7333\n",
      "Epoch 23/2500\n",
      "120/120 [==============================] - 0s 115us/step - loss: 0.5493 - accuracy: 0.6500 - val_loss: 0.4862 - val_accuracy: 0.7333\n",
      "Epoch 24/2500\n",
      "120/120 [==============================] - 0s 129us/step - loss: 0.5438 - accuracy: 0.6583 - val_loss: 0.4808 - val_accuracy: 0.7333\n",
      "Epoch 25/2500\n",
      "120/120 [==============================] - 0s 155us/step - loss: 0.5368 - accuracy: 0.6583 - val_loss: 0.4743 - val_accuracy: 0.7333\n",
      "Epoch 26/2500\n",
      "120/120 [==============================] - 0s 160us/step - loss: 0.5304 - accuracy: 0.6583 - val_loss: 0.4676 - val_accuracy: 0.7333\n",
      "Epoch 27/2500\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.5242 - accuracy: 0.6667 - val_loss: 0.4605 - val_accuracy: 0.7667\n",
      "Epoch 28/2500\n",
      "120/120 [==============================] - 0s 141us/step - loss: 0.5174 - accuracy: 0.6750 - val_loss: 0.4531 - val_accuracy: 0.7667\n",
      "Epoch 29/2500\n",
      "120/120 [==============================] - 0s 145us/step - loss: 0.5106 - accuracy: 0.6750 - val_loss: 0.4465 - val_accuracy: 0.8000\n",
      "Epoch 30/2500\n",
      "120/120 [==============================] - 0s 137us/step - loss: 0.5043 - accuracy: 0.7083 - val_loss: 0.4404 - val_accuracy: 0.8333\n",
      "Epoch 31/2500\n",
      "120/120 [==============================] - 0s 141us/step - loss: 0.4981 - accuracy: 0.7250 - val_loss: 0.4336 - val_accuracy: 0.8333\n",
      "Epoch 32/2500\n",
      "120/120 [==============================] - 0s 122us/step - loss: 0.4920 - accuracy: 0.7250 - val_loss: 0.4274 - val_accuracy: 0.8333\n",
      "Epoch 33/2500\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.4870 - accuracy: 0.7250 - val_loss: 0.4193 - val_accuracy: 0.8333\n",
      "Epoch 34/2500\n",
      "120/120 [==============================] - 0s 120us/step - loss: 0.4807 - accuracy: 0.7417 - val_loss: 0.4131 - val_accuracy: 0.8333\n",
      "Epoch 35/2500\n",
      "120/120 [==============================] - 0s 123us/step - loss: 0.4758 - accuracy: 0.7500 - val_loss: 0.4082 - val_accuracy: 0.8333\n",
      "Epoch 36/2500\n",
      "120/120 [==============================] - 0s 120us/step - loss: 0.4709 - accuracy: 0.7583 - val_loss: 0.4012 - val_accuracy: 0.8333\n",
      "Epoch 37/2500\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.4657 - accuracy: 0.7833 - val_loss: 0.3964 - val_accuracy: 0.8333\n",
      "Epoch 38/2500\n",
      "120/120 [==============================] - 0s 145us/step - loss: 0.4607 - accuracy: 0.7917 - val_loss: 0.3917 - val_accuracy: 0.8333\n",
      "Epoch 39/2500\n",
      "120/120 [==============================] - 0s 135us/step - loss: 0.4562 - accuracy: 0.8000 - val_loss: 0.3859 - val_accuracy: 0.8333\n",
      "Epoch 40/2500\n",
      "120/120 [==============================] - 0s 110us/step - loss: 0.4520 - accuracy: 0.7917 - val_loss: 0.3807 - val_accuracy: 0.8333\n",
      "Epoch 41/2500\n",
      "120/120 [==============================] - 0s 126us/step - loss: 0.4480 - accuracy: 0.8000 - val_loss: 0.3768 - val_accuracy: 0.8333\n",
      "Epoch 42/2500\n",
      "120/120 [==============================] - 0s 126us/step - loss: 0.4436 - accuracy: 0.8083 - val_loss: 0.3722 - val_accuracy: 0.8333\n",
      "Epoch 43/2500\n",
      "120/120 [==============================] - 0s 116us/step - loss: 0.4394 - accuracy: 0.8250 - val_loss: 0.3674 - val_accuracy: 0.8333\n",
      "Epoch 44/2500\n",
      "120/120 [==============================] - 0s 124us/step - loss: 0.4359 - accuracy: 0.8167 - val_loss: 0.3633 - val_accuracy: 0.8333\n",
      "Epoch 45/2500\n",
      "120/120 [==============================] - 0s 120us/step - loss: 0.4325 - accuracy: 0.8000 - val_loss: 0.3589 - val_accuracy: 0.8333\n",
      "Epoch 46/2500\n",
      "120/120 [==============================] - 0s 104us/step - loss: 0.4286 - accuracy: 0.8083 - val_loss: 0.3553 - val_accuracy: 0.8667\n",
      "Epoch 47/2500\n",
      "120/120 [==============================] - 0s 110us/step - loss: 0.4254 - accuracy: 0.8250 - val_loss: 0.3519 - val_accuracy: 0.8667\n",
      "Epoch 48/2500\n",
      "120/120 [==============================] - 0s 131us/step - loss: 0.4222 - accuracy: 0.8417 - val_loss: 0.3485 - val_accuracy: 0.8667\n",
      "Epoch 49/2500\n",
      "120/120 [==============================] - 0s 109us/step - loss: 0.4189 - accuracy: 0.8417 - val_loss: 0.3451 - val_accuracy: 0.8667\n",
      "Epoch 50/2500\n",
      "120/120 [==============================] - 0s 110us/step - loss: 0.4161 - accuracy: 0.8333 - val_loss: 0.3414 - val_accuracy: 0.8667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/2500\n",
      "120/120 [==============================] - 0s 131us/step - loss: 0.4128 - accuracy: 0.8417 - val_loss: 0.3384 - val_accuracy: 0.8667\n",
      "Epoch 52/2500\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.4102 - accuracy: 0.8500 - val_loss: 0.3352 - val_accuracy: 0.8667\n",
      "Epoch 53/2500\n",
      "120/120 [==============================] - 0s 104us/step - loss: 0.4070 - accuracy: 0.8417 - val_loss: 0.3326 - val_accuracy: 0.8667\n",
      "Epoch 54/2500\n",
      "120/120 [==============================] - 0s 112us/step - loss: 0.4043 - accuracy: 0.8417 - val_loss: 0.3298 - val_accuracy: 0.8667\n",
      "Epoch 55/2500\n",
      "120/120 [==============================] - 0s 116us/step - loss: 0.4013 - accuracy: 0.8417 - val_loss: 0.3272 - val_accuracy: 0.8667\n",
      "Epoch 56/2500\n",
      "120/120 [==============================] - 0s 98us/step - loss: 0.3987 - accuracy: 0.8417 - val_loss: 0.3245 - val_accuracy: 0.8667\n",
      "Epoch 57/2500\n",
      "120/120 [==============================] - 0s 109us/step - loss: 0.3959 - accuracy: 0.8417 - val_loss: 0.3219 - val_accuracy: 0.8667\n",
      "Epoch 58/2500\n",
      "120/120 [==============================] - 0s 121us/step - loss: 0.3932 - accuracy: 0.8500 - val_loss: 0.3193 - val_accuracy: 0.8667\n",
      "Epoch 59/2500\n",
      "120/120 [==============================] - 0s 107us/step - loss: 0.3908 - accuracy: 0.8500 - val_loss: 0.3168 - val_accuracy: 0.8667\n",
      "Epoch 60/2500\n",
      "120/120 [==============================] - 0s 104us/step - loss: 0.3881 - accuracy: 0.8417 - val_loss: 0.3144 - val_accuracy: 0.8667\n",
      "Epoch 61/2500\n",
      "120/120 [==============================] - 0s 120us/step - loss: 0.3857 - accuracy: 0.8417 - val_loss: 0.3120 - val_accuracy: 0.8667\n",
      "Epoch 62/2500\n",
      "120/120 [==============================] - 0s 105us/step - loss: 0.3828 - accuracy: 0.8417 - val_loss: 0.3101 - val_accuracy: 0.9000\n",
      "Epoch 63/2500\n",
      "120/120 [==============================] - 0s 96us/step - loss: 0.3807 - accuracy: 0.8417 - val_loss: 0.3080 - val_accuracy: 0.9000\n",
      "Epoch 64/2500\n",
      "120/120 [==============================] - 0s 100us/step - loss: 0.3779 - accuracy: 0.8750 - val_loss: 0.3060 - val_accuracy: 0.9000\n",
      "Epoch 65/2500\n",
      "120/120 [==============================] - 0s 110us/step - loss: 0.3754 - accuracy: 0.8750 - val_loss: 0.3038 - val_accuracy: 0.9000\n",
      "Epoch 66/2500\n",
      "120/120 [==============================] - 0s 107us/step - loss: 0.3730 - accuracy: 0.8750 - val_loss: 0.3016 - val_accuracy: 0.9000\n",
      "Epoch 67/2500\n",
      "120/120 [==============================] - 0s 115us/step - loss: 0.3708 - accuracy: 0.8750 - val_loss: 0.2995 - val_accuracy: 0.9000\n",
      "Epoch 68/2500\n",
      "120/120 [==============================] - 0s 136us/step - loss: 0.3683 - accuracy: 0.8750 - val_loss: 0.2974 - val_accuracy: 0.9000\n",
      "Epoch 69/2500\n",
      "120/120 [==============================] - 0s 112us/step - loss: 0.3663 - accuracy: 0.8750 - val_loss: 0.2954 - val_accuracy: 0.9000\n",
      "Epoch 70/2500\n",
      "120/120 [==============================] - 0s 101us/step - loss: 0.3636 - accuracy: 0.8833 - val_loss: 0.2938 - val_accuracy: 0.9000\n",
      "Epoch 71/2500\n",
      "120/120 [==============================] - 0s 98us/step - loss: 0.3622 - accuracy: 0.9000 - val_loss: 0.2927 - val_accuracy: 0.9667\n",
      "Epoch 72/2500\n",
      "120/120 [==============================] - 0s 109us/step - loss: 0.3599 - accuracy: 0.8917 - val_loss: 0.2898 - val_accuracy: 0.9000\n",
      "Epoch 73/2500\n",
      "120/120 [==============================] - 0s 102us/step - loss: 0.3570 - accuracy: 0.8917 - val_loss: 0.2881 - val_accuracy: 0.9000\n",
      "Epoch 74/2500\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.3549 - accuracy: 0.8833 - val_loss: 0.2860 - val_accuracy: 0.9000\n",
      "Epoch 75/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.3530 - accuracy: 0.8750 - val_loss: 0.2843 - val_accuracy: 0.9000\n",
      "Epoch 76/2500\n",
      "120/120 [==============================] - 0s 109us/step - loss: 0.3500 - accuracy: 0.8917 - val_loss: 0.2830 - val_accuracy: 0.9000\n",
      "Epoch 77/2500\n",
      "120/120 [==============================] - 0s 96us/step - loss: 0.3505 - accuracy: 0.9000 - val_loss: 0.2823 - val_accuracy: 0.9333\n",
      "Epoch 78/2500\n",
      "120/120 [==============================] - 0s 98us/step - loss: 0.3462 - accuracy: 0.9083 - val_loss: 0.2798 - val_accuracy: 0.9333\n",
      "Epoch 79/2500\n",
      "120/120 [==============================] - 0s 119us/step - loss: 0.3437 - accuracy: 0.8917 - val_loss: 0.2777 - val_accuracy: 0.9000\n",
      "Epoch 80/2500\n",
      "120/120 [==============================] - 0s 102us/step - loss: 0.3427 - accuracy: 0.8833 - val_loss: 0.2759 - val_accuracy: 0.9000\n",
      "Epoch 81/2500\n",
      "120/120 [==============================] - 0s 100us/step - loss: 0.3398 - accuracy: 0.8917 - val_loss: 0.2746 - val_accuracy: 0.9000\n",
      "Epoch 82/2500\n",
      "120/120 [==============================] - 0s 122us/step - loss: 0.3374 - accuracy: 0.9000 - val_loss: 0.2729 - val_accuracy: 0.9000\n",
      "Epoch 83/2500\n",
      "120/120 [==============================] - 0s 107us/step - loss: 0.3352 - accuracy: 0.9000 - val_loss: 0.2714 - val_accuracy: 0.9333\n",
      "Epoch 84/2500\n",
      "120/120 [==============================] - 0s 96us/step - loss: 0.3333 - accuracy: 0.9000 - val_loss: 0.2698 - val_accuracy: 0.9333\n",
      "Epoch 85/2500\n",
      "120/120 [==============================] - 0s 102us/step - loss: 0.3313 - accuracy: 0.9000 - val_loss: 0.2683 - val_accuracy: 0.9333\n",
      "Epoch 86/2500\n",
      "120/120 [==============================] - 0s 120us/step - loss: 0.3290 - accuracy: 0.9000 - val_loss: 0.2666 - val_accuracy: 0.9333\n",
      "Epoch 87/2500\n",
      "120/120 [==============================] - 0s 109us/step - loss: 0.3271 - accuracy: 0.9000 - val_loss: 0.2650 - val_accuracy: 0.9333\n",
      "Epoch 88/2500\n",
      "120/120 [==============================] - 0s 102us/step - loss: 0.3250 - accuracy: 0.9000 - val_loss: 0.2635 - val_accuracy: 0.9333\n",
      "Epoch 89/2500\n",
      "120/120 [==============================] - 0s 130us/step - loss: 0.3229 - accuracy: 0.9000 - val_loss: 0.2619 - val_accuracy: 0.9333\n",
      "Epoch 90/2500\n",
      "120/120 [==============================] - 0s 107us/step - loss: 0.3210 - accuracy: 0.9000 - val_loss: 0.2603 - val_accuracy: 0.9333\n",
      "Epoch 91/2500\n",
      "120/120 [==============================] - 0s 100us/step - loss: 0.3197 - accuracy: 0.9000 - val_loss: 0.2588 - val_accuracy: 0.9333\n",
      "Epoch 92/2500\n",
      "120/120 [==============================] - 0s 131us/step - loss: 0.3169 - accuracy: 0.9083 - val_loss: 0.2574 - val_accuracy: 0.9333\n",
      "Epoch 93/2500\n",
      "120/120 [==============================] - 0s 122us/step - loss: 0.3147 - accuracy: 0.9167 - val_loss: 0.2559 - val_accuracy: 0.9333\n",
      "Epoch 94/2500\n",
      "120/120 [==============================] - 0s 119us/step - loss: 0.3130 - accuracy: 0.9083 - val_loss: 0.2544 - val_accuracy: 0.9333\n",
      "Epoch 95/2500\n",
      "120/120 [==============================] - 0s 126us/step - loss: 0.3106 - accuracy: 0.9083 - val_loss: 0.2530 - val_accuracy: 0.9333\n",
      "Epoch 96/2500\n",
      "120/120 [==============================] - 0s 127us/step - loss: 0.3090 - accuracy: 0.9167 - val_loss: 0.2515 - val_accuracy: 0.9333\n",
      "Epoch 97/2500\n",
      "120/120 [==============================] - 0s 113us/step - loss: 0.3067 - accuracy: 0.9167 - val_loss: 0.2502 - val_accuracy: 0.9667\n",
      "Epoch 98/2500\n",
      "120/120 [==============================] - 0s 105us/step - loss: 0.3049 - accuracy: 0.9250 - val_loss: 0.2486 - val_accuracy: 0.9667\n",
      "Epoch 99/2500\n",
      "120/120 [==============================] - 0s 116us/step - loss: 0.3036 - accuracy: 0.9250 - val_loss: 0.2474 - val_accuracy: 0.9667\n",
      "Epoch 100/2500\n",
      "120/120 [==============================] - 0s 105us/step - loss: 0.3008 - accuracy: 0.9250 - val_loss: 0.2457 - val_accuracy: 0.9667\n",
      "Epoch 101/2500\n",
      "120/120 [==============================] - 0s 103us/step - loss: 0.2990 - accuracy: 0.9250 - val_loss: 0.2442 - val_accuracy: 0.9667\n",
      "Epoch 102/2500\n",
      "120/120 [==============================] - 0s 127us/step - loss: 0.2970 - accuracy: 0.9167 - val_loss: 0.2428 - val_accuracy: 0.9333\n",
      "Epoch 103/2500\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.2953 - accuracy: 0.9167 - val_loss: 0.2414 - val_accuracy: 0.9333\n",
      "Epoch 104/2500\n",
      "120/120 [==============================] - 0s 98us/step - loss: 0.2954 - accuracy: 0.9250 - val_loss: 0.2403 - val_accuracy: 0.9667\n",
      "Epoch 105/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.2919 - accuracy: 0.9417 - val_loss: 0.2388 - val_accuracy: 0.9667\n",
      "Epoch 106/2500\n",
      "120/120 [==============================] - 0s 115us/step - loss: 0.2893 - accuracy: 0.9333 - val_loss: 0.2373 - val_accuracy: 0.9667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 107/2500\n",
      "120/120 [==============================] - 0s 99us/step - loss: 0.2880 - accuracy: 0.9250 - val_loss: 0.2359 - val_accuracy: 0.9667\n",
      "Epoch 108/2500\n",
      "120/120 [==============================] - 0s 99us/step - loss: 0.2855 - accuracy: 0.9250 - val_loss: 0.2345 - val_accuracy: 0.9667\n",
      "Epoch 109/2500\n",
      "120/120 [==============================] - 0s 116us/step - loss: 0.2838 - accuracy: 0.9333 - val_loss: 0.2331 - val_accuracy: 0.9667\n",
      "Epoch 110/2500\n",
      "120/120 [==============================] - 0s 116us/step - loss: 0.2828 - accuracy: 0.9333 - val_loss: 0.2318 - val_accuracy: 0.9667\n",
      "Epoch 111/2500\n",
      "120/120 [==============================] - 0s 91us/step - loss: 0.2825 - accuracy: 0.9333 - val_loss: 0.2309 - val_accuracy: 0.9333\n",
      "Epoch 112/2500\n",
      "120/120 [==============================] - 0s 96us/step - loss: 0.2783 - accuracy: 0.9417 - val_loss: 0.2292 - val_accuracy: 0.9667\n",
      "Epoch 113/2500\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.2769 - accuracy: 0.9333 - val_loss: 0.2279 - val_accuracy: 0.9667\n",
      "Epoch 114/2500\n",
      "120/120 [==============================] - 0s 105us/step - loss: 0.2756 - accuracy: 0.9417 - val_loss: 0.2264 - val_accuracy: 0.9667\n",
      "Epoch 115/2500\n",
      "120/120 [==============================] - 0s 105us/step - loss: 0.2731 - accuracy: 0.9417 - val_loss: 0.2251 - val_accuracy: 0.9667\n",
      "Epoch 116/2500\n",
      "120/120 [==============================] - 0s 113us/step - loss: 0.2710 - accuracy: 0.9417 - val_loss: 0.2239 - val_accuracy: 0.9667\n",
      "Epoch 117/2500\n",
      "120/120 [==============================] - 0s 104us/step - loss: 0.2694 - accuracy: 0.9417 - val_loss: 0.2225 - val_accuracy: 0.9667\n",
      "Epoch 118/2500\n",
      "120/120 [==============================] - 0s 102us/step - loss: 0.2678 - accuracy: 0.9417 - val_loss: 0.2211 - val_accuracy: 0.9667\n",
      "Epoch 119/2500\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.2659 - accuracy: 0.9417 - val_loss: 0.2199 - val_accuracy: 0.9667\n",
      "Epoch 120/2500\n",
      "120/120 [==============================] - 0s 119us/step - loss: 0.2641 - accuracy: 0.9417 - val_loss: 0.2186 - val_accuracy: 0.9667\n",
      "Epoch 121/2500\n",
      "120/120 [==============================] - 0s 99us/step - loss: 0.2626 - accuracy: 0.9417 - val_loss: 0.2174 - val_accuracy: 0.9667\n",
      "Epoch 122/2500\n",
      "120/120 [==============================] - 0s 101us/step - loss: 0.2610 - accuracy: 0.9417 - val_loss: 0.2161 - val_accuracy: 0.9667\n",
      "Epoch 123/2500\n",
      "120/120 [==============================] - 0s 100us/step - loss: 0.2593 - accuracy: 0.9417 - val_loss: 0.2149 - val_accuracy: 0.9667\n",
      "Epoch 124/2500\n",
      "120/120 [==============================] - 0s 104us/step - loss: 0.2582 - accuracy: 0.9417 - val_loss: 0.2137 - val_accuracy: 0.9667\n",
      "Epoch 125/2500\n",
      "120/120 [==============================] - 0s 100us/step - loss: 0.2561 - accuracy: 0.9417 - val_loss: 0.2125 - val_accuracy: 0.9667\n",
      "Epoch 126/2500\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.2543 - accuracy: 0.9417 - val_loss: 0.2113 - val_accuracy: 0.9667\n",
      "Epoch 127/2500\n",
      "120/120 [==============================] - 0s 124us/step - loss: 0.2528 - accuracy: 0.9417 - val_loss: 0.2101 - val_accuracy: 0.9667\n",
      "Epoch 128/2500\n",
      "120/120 [==============================] - 0s 102us/step - loss: 0.2512 - accuracy: 0.9417 - val_loss: 0.2089 - val_accuracy: 0.9667\n",
      "Epoch 129/2500\n",
      "120/120 [==============================] - 0s 103us/step - loss: 0.2498 - accuracy: 0.9417 - val_loss: 0.2083 - val_accuracy: 0.9667\n",
      "Epoch 130/2500\n",
      "120/120 [==============================] - 0s 109us/step - loss: 0.2488 - accuracy: 0.9417 - val_loss: 0.2065 - val_accuracy: 0.9667\n",
      "Epoch 131/2500\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.2470 - accuracy: 0.9417 - val_loss: 0.2060 - val_accuracy: 0.9667\n",
      "Epoch 132/2500\n",
      "120/120 [==============================] - 0s 105us/step - loss: 0.2459 - accuracy: 0.9417 - val_loss: 0.2055 - val_accuracy: 0.9333\n",
      "Epoch 133/2500\n",
      "120/120 [==============================] - 0s 109us/step - loss: 0.2438 - accuracy: 0.9417 - val_loss: 0.2032 - val_accuracy: 0.9667\n",
      "Epoch 134/2500\n",
      "120/120 [==============================] - 0s 126us/step - loss: 0.2420 - accuracy: 0.9417 - val_loss: 0.2023 - val_accuracy: 0.9667\n",
      "Epoch 135/2500\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.2405 - accuracy: 0.9417 - val_loss: 0.2009 - val_accuracy: 0.9667\n",
      "Epoch 136/2500\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.2392 - accuracy: 0.9417 - val_loss: 0.2005 - val_accuracy: 0.9667\n",
      "Epoch 137/2500\n",
      "120/120 [==============================] - 0s 116us/step - loss: 0.2374 - accuracy: 0.9417 - val_loss: 0.1992 - val_accuracy: 0.9667\n",
      "Epoch 138/2500\n",
      "120/120 [==============================] - 0s 99us/step - loss: 0.2361 - accuracy: 0.9417 - val_loss: 0.1979 - val_accuracy: 0.9667\n",
      "Epoch 139/2500\n",
      "120/120 [==============================] - 0s 98us/step - loss: 0.2359 - accuracy: 0.9417 - val_loss: 0.1978 - val_accuracy: 0.9333\n",
      "Epoch 140/2500\n",
      "120/120 [==============================] - 0s 101us/step - loss: 0.2348 - accuracy: 0.9417 - val_loss: 0.1955 - val_accuracy: 0.9667\n",
      "Epoch 141/2500\n",
      "120/120 [==============================] - 0s 113us/step - loss: 0.2334 - accuracy: 0.9417 - val_loss: 0.1953 - val_accuracy: 0.9667\n",
      "Epoch 142/2500\n",
      "120/120 [==============================] - 0s 104us/step - loss: 0.2298 - accuracy: 0.9417 - val_loss: 0.1939 - val_accuracy: 0.9667\n",
      "Epoch 143/2500\n",
      "120/120 [==============================] - 0s 95us/step - loss: 0.2286 - accuracy: 0.9417 - val_loss: 0.1928 - val_accuracy: 0.9667\n",
      "Epoch 144/2500\n",
      "120/120 [==============================] - 0s 105us/step - loss: 0.2273 - accuracy: 0.9417 - val_loss: 0.1918 - val_accuracy: 0.9667\n",
      "Epoch 145/2500\n",
      "120/120 [==============================] - 0s 119us/step - loss: 0.2272 - accuracy: 0.9417 - val_loss: 0.1902 - val_accuracy: 0.9667\n",
      "Epoch 146/2500\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.2246 - accuracy: 0.9417 - val_loss: 0.1896 - val_accuracy: 0.9667\n",
      "Epoch 147/2500\n",
      "120/120 [==============================] - 0s 105us/step - loss: 0.2232 - accuracy: 0.9417 - val_loss: 0.1892 - val_accuracy: 0.9667\n",
      "Epoch 148/2500\n",
      "120/120 [==============================] - 0s 123us/step - loss: 0.2221 - accuracy: 0.9417 - val_loss: 0.1875 - val_accuracy: 0.9667\n",
      "Epoch 149/2500\n",
      "120/120 [==============================] - 0s 107us/step - loss: 0.2204 - accuracy: 0.9417 - val_loss: 0.1863 - val_accuracy: 0.9667\n",
      "Epoch 150/2500\n",
      "120/120 [==============================] - 0s 107us/step - loss: 0.2190 - accuracy: 0.9417 - val_loss: 0.1855 - val_accuracy: 0.9667\n",
      "Epoch 151/2500\n",
      "120/120 [==============================] - 0s 120us/step - loss: 0.2188 - accuracy: 0.9500 - val_loss: 0.1852 - val_accuracy: 0.9667\n",
      "Epoch 152/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.2166 - accuracy: 0.9417 - val_loss: 0.1835 - val_accuracy: 0.9667\n",
      "Epoch 153/2500\n",
      "120/120 [==============================] - 0s 109us/step - loss: 0.2155 - accuracy: 0.9417 - val_loss: 0.1824 - val_accuracy: 0.9667\n",
      "Epoch 154/2500\n",
      "120/120 [==============================] - 0s 103us/step - loss: 0.2141 - accuracy: 0.9417 - val_loss: 0.1813 - val_accuracy: 0.9667\n",
      "Epoch 155/2500\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.2127 - accuracy: 0.9417 - val_loss: 0.1806 - val_accuracy: 0.9667\n",
      "Epoch 156/2500\n",
      "120/120 [==============================] - 0s 103us/step - loss: 0.2116 - accuracy: 0.9417 - val_loss: 0.1797 - val_accuracy: 0.9667\n",
      "Epoch 157/2500\n",
      "120/120 [==============================] - 0s 96us/step - loss: 0.2105 - accuracy: 0.9417 - val_loss: 0.1786 - val_accuracy: 0.9667\n",
      "Epoch 158/2500\n",
      "120/120 [==============================] - 0s 110us/step - loss: 0.2090 - accuracy: 0.9500 - val_loss: 0.1780 - val_accuracy: 0.9667\n",
      "Epoch 159/2500\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.2083 - accuracy: 0.9500 - val_loss: 0.1773 - val_accuracy: 0.9667\n",
      "Epoch 160/2500\n",
      "120/120 [==============================] - 0s 104us/step - loss: 0.2075 - accuracy: 0.9500 - val_loss: 0.1756 - val_accuracy: 0.9667\n",
      "Epoch 161/2500\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.2060 - accuracy: 0.9417 - val_loss: 0.1749 - val_accuracy: 0.9667\n",
      "Epoch 162/2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 123us/step - loss: 0.2048 - accuracy: 0.9417 - val_loss: 0.1737 - val_accuracy: 0.9667\n",
      "Epoch 163/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.2056 - accuracy: 0.9417 - val_loss: 0.1728 - val_accuracy: 0.9667\n",
      "Epoch 164/2500\n",
      "120/120 [==============================] - 0s 103us/step - loss: 0.2019 - accuracy: 0.9417 - val_loss: 0.1725 - val_accuracy: 0.9667\n",
      "Epoch 165/2500\n",
      "120/120 [==============================] - 0s 118us/step - loss: 0.2011 - accuracy: 0.9583 - val_loss: 0.1721 - val_accuracy: 1.0000\n",
      "Epoch 166/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.2002 - accuracy: 0.9583 - val_loss: 0.1707 - val_accuracy: 0.9667\n",
      "Epoch 167/2500\n",
      "120/120 [==============================] - 0s 101us/step - loss: 0.1991 - accuracy: 0.9500 - val_loss: 0.1692 - val_accuracy: 0.9667\n",
      "Epoch 168/2500\n",
      "120/120 [==============================] - 0s 100us/step - loss: 0.1992 - accuracy: 0.9417 - val_loss: 0.1684 - val_accuracy: 0.9667\n",
      "Epoch 169/2500\n",
      "120/120 [==============================] - 0s 105us/step - loss: 0.1967 - accuracy: 0.9417 - val_loss: 0.1677 - val_accuracy: 0.9667\n",
      "Epoch 170/2500\n",
      "120/120 [==============================] - 0s 101us/step - loss: 0.1961 - accuracy: 0.9500 - val_loss: 0.1678 - val_accuracy: 1.0000\n",
      "Epoch 171/2500\n",
      "120/120 [==============================] - 0s 102us/step - loss: 0.1949 - accuracy: 0.9583 - val_loss: 0.1660 - val_accuracy: 0.9667\n",
      "Epoch 172/2500\n",
      "120/120 [==============================] - 0s 135us/step - loss: 0.1938 - accuracy: 0.9500 - val_loss: 0.1651 - val_accuracy: 0.9667\n",
      "Epoch 173/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.1929 - accuracy: 0.9417 - val_loss: 0.1641 - val_accuracy: 0.9667\n",
      "Epoch 174/2500\n",
      "120/120 [==============================] - 0s 97us/step - loss: 0.1918 - accuracy: 0.9500 - val_loss: 0.1634 - val_accuracy: 0.9667\n",
      "Epoch 175/2500\n",
      "120/120 [==============================] - 0s 116us/step - loss: 0.1906 - accuracy: 0.9500 - val_loss: 0.1632 - val_accuracy: 1.0000\n",
      "Epoch 176/2500\n",
      "120/120 [==============================] - 0s 112us/step - loss: 0.1905 - accuracy: 0.9583 - val_loss: 0.1621 - val_accuracy: 1.0000\n",
      "Epoch 177/2500\n",
      "120/120 [==============================] - 0s 93us/step - loss: 0.1888 - accuracy: 0.9500 - val_loss: 0.1608 - val_accuracy: 0.9667\n",
      "Epoch 178/2500\n",
      "120/120 [==============================] - 0s 105us/step - loss: 0.1878 - accuracy: 0.9500 - val_loss: 0.1600 - val_accuracy: 0.9667\n",
      "Epoch 179/2500\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.1869 - accuracy: 0.9500 - val_loss: 0.1593 - val_accuracy: 0.9667\n",
      "Epoch 180/2500\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.1860 - accuracy: 0.9500 - val_loss: 0.1587 - val_accuracy: 1.0000\n",
      "Epoch 181/2500\n",
      "120/120 [==============================] - 0s 102us/step - loss: 0.1853 - accuracy: 0.9500 - val_loss: 0.1576 - val_accuracy: 0.9667\n",
      "Epoch 182/2500\n",
      "120/120 [==============================] - 0s 93us/step - loss: 0.1841 - accuracy: 0.9500 - val_loss: 0.1569 - val_accuracy: 0.9667\n",
      "Epoch 183/2500\n",
      "120/120 [==============================] - 0s 101us/step - loss: 0.1834 - accuracy: 0.9500 - val_loss: 0.1561 - val_accuracy: 0.9667\n",
      "Epoch 184/2500\n",
      "120/120 [==============================] - 0s 105us/step - loss: 0.1826 - accuracy: 0.9500 - val_loss: 0.1555 - val_accuracy: 1.0000\n",
      "Epoch 185/2500\n",
      "120/120 [==============================] - 0s 103us/step - loss: 0.1839 - accuracy: 0.9500 - val_loss: 0.1551 - val_accuracy: 1.0000\n",
      "Epoch 186/2500\n",
      "120/120 [==============================] - 0s 124us/step - loss: 0.1816 - accuracy: 0.9417 - val_loss: 0.1540 - val_accuracy: 0.9667\n",
      "Epoch 187/2500\n",
      "120/120 [==============================] - 0s 107us/step - loss: 0.1799 - accuracy: 0.9417 - val_loss: 0.1531 - val_accuracy: 0.9667\n",
      "Epoch 188/2500\n",
      "120/120 [==============================] - 0s 102us/step - loss: 0.1796 - accuracy: 0.9500 - val_loss: 0.1523 - val_accuracy: 0.9667\n",
      "Epoch 189/2500\n",
      "120/120 [==============================] - 0s 98us/step - loss: 0.1780 - accuracy: 0.9500 - val_loss: 0.1518 - val_accuracy: 1.0000\n",
      "Epoch 190/2500\n",
      "120/120 [==============================] - 0s 103us/step - loss: 0.1776 - accuracy: 0.9500 - val_loss: 0.1509 - val_accuracy: 1.0000\n",
      "Epoch 191/2500\n",
      "120/120 [==============================] - 0s 99us/step - loss: 0.1772 - accuracy: 0.9500 - val_loss: 0.1503 - val_accuracy: 1.0000\n",
      "Epoch 192/2500\n",
      "120/120 [==============================] - 0s 90us/step - loss: 0.1759 - accuracy: 0.9500 - val_loss: 0.1494 - val_accuracy: 0.9667\n",
      "Epoch 193/2500\n",
      "120/120 [==============================] - 0s 93us/step - loss: 0.1752 - accuracy: 0.9500 - val_loss: 0.1487 - val_accuracy: 1.0000\n",
      "Epoch 194/2500\n",
      "120/120 [==============================] - 0s 94us/step - loss: 0.1750 - accuracy: 0.9500 - val_loss: 0.1483 - val_accuracy: 1.0000\n",
      "Epoch 195/2500\n",
      "120/120 [==============================] - 0s 97us/step - loss: 0.1738 - accuracy: 0.9583 - val_loss: 0.1475 - val_accuracy: 1.0000\n",
      "Epoch 196/2500\n",
      "120/120 [==============================] - 0s 105us/step - loss: 0.1731 - accuracy: 0.9500 - val_loss: 0.1467 - val_accuracy: 1.0000\n",
      "Epoch 197/2500\n",
      "120/120 [==============================] - 0s 99us/step - loss: 0.1748 - accuracy: 0.9417 - val_loss: 0.1465 - val_accuracy: 0.9667\n",
      "Epoch 198/2500\n",
      "120/120 [==============================] - 0s 101us/step - loss: 0.1718 - accuracy: 0.9417 - val_loss: 0.1454 - val_accuracy: 1.0000\n",
      "Epoch 199/2500\n",
      "120/120 [==============================] - 0s 101us/step - loss: 0.1707 - accuracy: 0.9583 - val_loss: 0.1449 - val_accuracy: 1.0000\n",
      "Epoch 200/2500\n",
      "120/120 [==============================] - 0s 101us/step - loss: 0.1714 - accuracy: 0.9500 - val_loss: 0.1440 - val_accuracy: 0.9667\n",
      "Epoch 201/2500\n",
      "120/120 [==============================] - 0s 99us/step - loss: 0.1703 - accuracy: 0.9500 - val_loss: 0.1437 - val_accuracy: 1.0000\n",
      "Epoch 202/2500\n",
      "120/120 [==============================] - 0s 94us/step - loss: 0.1689 - accuracy: 0.9500 - val_loss: 0.1427 - val_accuracy: 1.0000\n",
      "Epoch 203/2500\n",
      "120/120 [==============================] - 0s 90us/step - loss: 0.1679 - accuracy: 0.9500 - val_loss: 0.1420 - val_accuracy: 0.9667\n",
      "Epoch 204/2500\n",
      "120/120 [==============================] - 0s 99us/step - loss: 0.1674 - accuracy: 0.9500 - val_loss: 0.1414 - val_accuracy: 1.0000\n",
      "Epoch 205/2500\n",
      "120/120 [==============================] - 0s 100us/step - loss: 0.1668 - accuracy: 0.9583 - val_loss: 0.1409 - val_accuracy: 1.0000\n",
      "Epoch 206/2500\n",
      "120/120 [==============================] - 0s 91us/step - loss: 0.1656 - accuracy: 0.9500 - val_loss: 0.1401 - val_accuracy: 1.0000\n",
      "Epoch 207/2500\n",
      "120/120 [==============================] - 0s 95us/step - loss: 0.1652 - accuracy: 0.9500 - val_loss: 0.1396 - val_accuracy: 1.0000\n",
      "Epoch 208/2500\n",
      "120/120 [==============================] - 0s 100us/step - loss: 0.1648 - accuracy: 0.9500 - val_loss: 0.1389 - val_accuracy: 1.0000\n",
      "Epoch 209/2500\n",
      "120/120 [==============================] - 0s 100us/step - loss: 0.1642 - accuracy: 0.9500 - val_loss: 0.1383 - val_accuracy: 1.0000\n",
      "Epoch 210/2500\n",
      "120/120 [==============================] - 0s 96us/step - loss: 0.1633 - accuracy: 0.9500 - val_loss: 0.1377 - val_accuracy: 1.0000\n",
      "Epoch 211/2500\n",
      "120/120 [==============================] - 0s 94us/step - loss: 0.1627 - accuracy: 0.9583 - val_loss: 0.1372 - val_accuracy: 1.0000\n",
      "Epoch 212/2500\n",
      "120/120 [==============================] - 0s 97us/step - loss: 0.1625 - accuracy: 0.9583 - val_loss: 0.1366 - val_accuracy: 1.0000\n",
      "Epoch 213/2500\n",
      "120/120 [==============================] - 0s 100us/step - loss: 0.1625 - accuracy: 0.9500 - val_loss: 0.1360 - val_accuracy: 1.0000\n",
      "Epoch 214/2500\n",
      "120/120 [==============================] - 0s 93us/step - loss: 0.1620 - accuracy: 0.9583 - val_loss: 0.1354 - val_accuracy: 1.0000\n",
      "Epoch 215/2500\n",
      "120/120 [==============================] - 0s 97us/step - loss: 0.1611 - accuracy: 0.9667 - val_loss: 0.1348 - val_accuracy: 1.0000\n",
      "Epoch 216/2500\n",
      "120/120 [==============================] - 0s 97us/step - loss: 0.1593 - accuracy: 0.9500 - val_loss: 0.1343 - val_accuracy: 1.0000\n",
      "Epoch 217/2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 102us/step - loss: 0.1597 - accuracy: 0.9500 - val_loss: 0.1340 - val_accuracy: 0.9667\n",
      "Epoch 218/2500\n",
      "120/120 [==============================] - 0s 95us/step - loss: 0.1593 - accuracy: 0.9500 - val_loss: 0.1330 - val_accuracy: 1.0000\n",
      "Epoch 219/2500\n",
      "120/120 [==============================] - 0s 96us/step - loss: 0.1583 - accuracy: 0.9500 - val_loss: 0.1325 - val_accuracy: 1.0000\n",
      "Epoch 220/2500\n",
      "120/120 [==============================] - 0s 93us/step - loss: 0.1596 - accuracy: 0.9500 - val_loss: 0.1320 - val_accuracy: 1.0000\n",
      "Epoch 221/2500\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.1579 - accuracy: 0.9667 - val_loss: 0.1316 - val_accuracy: 1.0000\n",
      "Epoch 222/2500\n",
      "120/120 [==============================] - 0s 98us/step - loss: 0.1569 - accuracy: 0.9583 - val_loss: 0.1309 - val_accuracy: 1.0000\n",
      "Epoch 223/2500\n",
      "120/120 [==============================] - 0s 93us/step - loss: 0.1562 - accuracy: 0.9583 - val_loss: 0.1303 - val_accuracy: 1.0000\n",
      "Epoch 224/2500\n",
      "120/120 [==============================] - 0s 88us/step - loss: 0.1571 - accuracy: 0.9583 - val_loss: 0.1299 - val_accuracy: 1.0000\n",
      "Epoch 225/2500\n",
      "120/120 [==============================] - 0s 97us/step - loss: 0.1559 - accuracy: 0.9500 - val_loss: 0.1297 - val_accuracy: 0.9667\n",
      "Epoch 226/2500\n",
      "120/120 [==============================] - 0s 101us/step - loss: 0.1545 - accuracy: 0.9500 - val_loss: 0.1287 - val_accuracy: 1.0000\n",
      "Epoch 227/2500\n",
      "120/120 [==============================] - 0s 99us/step - loss: 0.1545 - accuracy: 0.9500 - val_loss: 0.1282 - val_accuracy: 1.0000\n",
      "Epoch 228/2500\n",
      "120/120 [==============================] - 0s 105us/step - loss: 0.1533 - accuracy: 0.9500 - val_loss: 0.1276 - val_accuracy: 1.0000\n",
      "Epoch 229/2500\n",
      "120/120 [==============================] - 0s 118us/step - loss: 0.1536 - accuracy: 0.9583 - val_loss: 0.1273 - val_accuracy: 1.0000\n",
      "Epoch 230/2500\n",
      "120/120 [==============================] - 0s 124us/step - loss: 0.1524 - accuracy: 0.9667 - val_loss: 0.1266 - val_accuracy: 1.0000\n",
      "Epoch 231/2500\n",
      "120/120 [==============================] - 0s 121us/step - loss: 0.1523 - accuracy: 0.9500 - val_loss: 0.1262 - val_accuracy: 1.0000\n",
      "Epoch 232/2500\n",
      "120/120 [==============================] - 0s 119us/step - loss: 0.1515 - accuracy: 0.9500 - val_loss: 0.1256 - val_accuracy: 1.0000\n",
      "Epoch 233/2500\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.1538 - accuracy: 0.9583 - val_loss: 0.1253 - val_accuracy: 1.0000\n",
      "Epoch 234/2500\n",
      "120/120 [==============================] - 0s 123us/step - loss: 0.1508 - accuracy: 0.9583 - val_loss: 0.1247 - val_accuracy: 1.0000\n",
      "Epoch 235/2500\n",
      "120/120 [==============================] - 0s 139us/step - loss: 0.1504 - accuracy: 0.9500 - val_loss: 0.1242 - val_accuracy: 1.0000\n",
      "Epoch 236/2500\n",
      "120/120 [==============================] - 0s 112us/step - loss: 0.1504 - accuracy: 0.9500 - val_loss: 0.1237 - val_accuracy: 1.0000\n",
      "Epoch 237/2500\n",
      "120/120 [==============================] - 0s 129us/step - loss: 0.1493 - accuracy: 0.9500 - val_loss: 0.1232 - val_accuracy: 1.0000\n",
      "Epoch 238/2500\n",
      "120/120 [==============================] - 0s 142us/step - loss: 0.1489 - accuracy: 0.9500 - val_loss: 0.1228 - val_accuracy: 1.0000\n",
      "Epoch 239/2500\n",
      "120/120 [==============================] - 0s 132us/step - loss: 0.1489 - accuracy: 0.9667 - val_loss: 0.1222 - val_accuracy: 1.0000\n",
      "Epoch 240/2500\n",
      "120/120 [==============================] - 0s 124us/step - loss: 0.1484 - accuracy: 0.9667 - val_loss: 0.1218 - val_accuracy: 1.0000\n",
      "Epoch 241/2500\n",
      "120/120 [==============================] - 0s 129us/step - loss: 0.1481 - accuracy: 0.9500 - val_loss: 0.1213 - val_accuracy: 1.0000\n",
      "Epoch 242/2500\n",
      "120/120 [==============================] - 0s 128us/step - loss: 0.1472 - accuracy: 0.9500 - val_loss: 0.1208 - val_accuracy: 1.0000\n",
      "Epoch 243/2500\n",
      "120/120 [==============================] - 0s 135us/step - loss: 0.1478 - accuracy: 0.9500 - val_loss: 0.1208 - val_accuracy: 1.0000\n",
      "Epoch 244/2500\n",
      "120/120 [==============================] - 0s 135us/step - loss: 0.1468 - accuracy: 0.9500 - val_loss: 0.1203 - val_accuracy: 1.0000\n",
      "Epoch 245/2500\n",
      "120/120 [==============================] - 0s 126us/step - loss: 0.1455 - accuracy: 0.9583 - val_loss: 0.1194 - val_accuracy: 1.0000\n",
      "Epoch 246/2500\n",
      "120/120 [==============================] - 0s 128us/step - loss: 0.1481 - accuracy: 0.9667 - val_loss: 0.1195 - val_accuracy: 1.0000\n",
      "Epoch 247/2500\n",
      "120/120 [==============================] - 0s 129us/step - loss: 0.1455 - accuracy: 0.9667 - val_loss: 0.1187 - val_accuracy: 1.0000\n",
      "Epoch 248/2500\n",
      "120/120 [==============================] - 0s 153us/step - loss: 0.1443 - accuracy: 0.9500 - val_loss: 0.1184 - val_accuracy: 1.0000\n",
      "Epoch 249/2500\n",
      "120/120 [==============================] - 0s 152us/step - loss: 0.1449 - accuracy: 0.9500 - val_loss: 0.1196 - val_accuracy: 0.9667\n",
      "Epoch 250/2500\n",
      "120/120 [==============================] - 0s 142us/step - loss: 0.1446 - accuracy: 0.9500 - val_loss: 0.1177 - val_accuracy: 1.0000\n",
      "Epoch 251/2500\n",
      "120/120 [==============================] - 0s 142us/step - loss: 0.1433 - accuracy: 0.9500 - val_loss: 0.1172 - val_accuracy: 1.0000\n",
      "Epoch 252/2500\n",
      "120/120 [==============================] - 0s 131us/step - loss: 0.1432 - accuracy: 0.9500 - val_loss: 0.1176 - val_accuracy: 1.0000\n",
      "Epoch 253/2500\n",
      "120/120 [==============================] - 0s 127us/step - loss: 0.1429 - accuracy: 0.9500 - val_loss: 0.1170 - val_accuracy: 1.0000\n",
      "Epoch 254/2500\n",
      "120/120 [==============================] - 0s 130us/step - loss: 0.1419 - accuracy: 0.9500 - val_loss: 0.1160 - val_accuracy: 1.0000\n",
      "Epoch 255/2500\n",
      "120/120 [==============================] - 0s 138us/step - loss: 0.1416 - accuracy: 0.9500 - val_loss: 0.1154 - val_accuracy: 1.0000\n",
      "Epoch 256/2500\n",
      "120/120 [==============================] - 0s 129us/step - loss: 0.1417 - accuracy: 0.9500 - val_loss: 0.1156 - val_accuracy: 1.0000\n",
      "Epoch 257/2500\n",
      "120/120 [==============================] - 0s 133us/step - loss: 0.1409 - accuracy: 0.9500 - val_loss: 0.1153 - val_accuracy: 1.0000\n",
      "Epoch 258/2500\n",
      "120/120 [==============================] - 0s 129us/step - loss: 0.1405 - accuracy: 0.9500 - val_loss: 0.1152 - val_accuracy: 1.0000\n",
      "Epoch 259/2500\n",
      "120/120 [==============================] - 0s 128us/step - loss: 0.1403 - accuracy: 0.9500 - val_loss: 0.1153 - val_accuracy: 0.9667\n",
      "Epoch 260/2500\n",
      "120/120 [==============================] - 0s 150us/step - loss: 0.1395 - accuracy: 0.9500 - val_loss: 0.1137 - val_accuracy: 1.0000\n",
      "Epoch 261/2500\n",
      "120/120 [==============================] - 0s 131us/step - loss: 0.1400 - accuracy: 0.9500 - val_loss: 0.1128 - val_accuracy: 1.0000\n",
      "Epoch 262/2500\n",
      "120/120 [==============================] - 0s 133us/step - loss: 0.1394 - accuracy: 0.9583 - val_loss: 0.1124 - val_accuracy: 1.0000\n",
      "Epoch 263/2500\n",
      "120/120 [==============================] - 0s 129us/step - loss: 0.1385 - accuracy: 0.9583 - val_loss: 0.1123 - val_accuracy: 1.0000\n",
      "Epoch 264/2500\n",
      "120/120 [==============================] - 0s 116us/step - loss: 0.1383 - accuracy: 0.9500 - val_loss: 0.1120 - val_accuracy: 1.0000\n",
      "Epoch 265/2500\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.1390 - accuracy: 0.9500 - val_loss: 0.1140 - val_accuracy: 0.9667\n",
      "Epoch 266/2500\n",
      "120/120 [==============================] - 0s 118us/step - loss: 0.1375 - accuracy: 0.9500 - val_loss: 0.1122 - val_accuracy: 1.0000\n",
      "Epoch 267/2500\n",
      "120/120 [==============================] - 0s 128us/step - loss: 0.1376 - accuracy: 0.9500 - val_loss: 0.1107 - val_accuracy: 1.0000\n",
      "Epoch 268/2500\n",
      "120/120 [==============================] - 0s 121us/step - loss: 0.1371 - accuracy: 0.9500 - val_loss: 0.1111 - val_accuracy: 1.0000\n",
      "Epoch 269/2500\n",
      "120/120 [==============================] - 0s 119us/step - loss: 0.1372 - accuracy: 0.9500 - val_loss: 0.1116 - val_accuracy: 1.0000\n",
      "Epoch 270/2500\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.1366 - accuracy: 0.9500 - val_loss: 0.1095 - val_accuracy: 1.0000\n",
      "Epoch 271/2500\n",
      "120/120 [==============================] - 0s 123us/step - loss: 0.1358 - accuracy: 0.9583 - val_loss: 0.1091 - val_accuracy: 1.0000\n",
      "Epoch 272/2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 118us/step - loss: 0.1353 - accuracy: 0.9500 - val_loss: 0.1097 - val_accuracy: 1.0000\n",
      "Epoch 273/2500\n",
      "120/120 [==============================] - 0s 121us/step - loss: 0.1348 - accuracy: 0.9500 - val_loss: 0.1093 - val_accuracy: 1.0000\n",
      "Epoch 274/2500\n",
      "120/120 [==============================] - 0s 127us/step - loss: 0.1350 - accuracy: 0.9500 - val_loss: 0.1084 - val_accuracy: 1.0000\n",
      "Epoch 275/2500\n",
      "120/120 [==============================] - 0s 131us/step - loss: 0.1346 - accuracy: 0.9500 - val_loss: 0.1099 - val_accuracy: 1.0000\n",
      "Epoch 276/2500\n",
      "120/120 [==============================] - 0s 118us/step - loss: 0.1350 - accuracy: 0.9500 - val_loss: 0.1079 - val_accuracy: 1.0000\n",
      "Epoch 277/2500\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.1335 - accuracy: 0.9500 - val_loss: 0.1074 - val_accuracy: 1.0000\n",
      "Epoch 278/2500\n",
      "120/120 [==============================] - 0s 119us/step - loss: 0.1329 - accuracy: 0.9500 - val_loss: 0.1078 - val_accuracy: 1.0000\n",
      "Epoch 279/2500\n",
      "120/120 [==============================] - 0s 120us/step - loss: 0.1323 - accuracy: 0.9500 - val_loss: 0.1076 - val_accuracy: 1.0000\n",
      "Epoch 280/2500\n",
      "120/120 [==============================] - 0s 113us/step - loss: 0.1329 - accuracy: 0.9500 - val_loss: 0.1074 - val_accuracy: 1.0000\n",
      "Epoch 281/2500\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.1326 - accuracy: 0.9500 - val_loss: 0.1061 - val_accuracy: 1.0000\n",
      "Epoch 282/2500\n",
      "120/120 [==============================] - 0s 103us/step - loss: 0.1317 - accuracy: 0.9500 - val_loss: 0.1060 - val_accuracy: 1.0000\n",
      "Epoch 283/2500\n",
      "120/120 [==============================] - 0s 104us/step - loss: 0.1317 - accuracy: 0.9500 - val_loss: 0.1064 - val_accuracy: 1.0000\n",
      "Epoch 284/2500\n",
      "120/120 [==============================] - 0s 115us/step - loss: 0.1337 - accuracy: 0.9500 - val_loss: 0.1076 - val_accuracy: 0.9667\n",
      "Epoch 285/2500\n",
      "120/120 [==============================] - 0s 132us/step - loss: 0.1307 - accuracy: 0.9500 - val_loss: 0.1048 - val_accuracy: 1.0000\n",
      "Epoch 286/2500\n",
      "120/120 [==============================] - 0s 137us/step - loss: 0.1325 - accuracy: 0.9500 - val_loss: 0.1054 - val_accuracy: 1.0000\n",
      "Epoch 287/2500\n",
      "120/120 [==============================] - 0s 143us/step - loss: 0.1294 - accuracy: 0.9500 - val_loss: 0.1040 - val_accuracy: 1.0000\n",
      "Epoch 288/2500\n",
      "120/120 [==============================] - 0s 154us/step - loss: 0.1302 - accuracy: 0.9500 - val_loss: 0.1033 - val_accuracy: 1.0000\n",
      "Epoch 289/2500\n",
      "120/120 [==============================] - 0s 155us/step - loss: 0.1304 - accuracy: 0.9583 - val_loss: 0.1031 - val_accuracy: 1.0000\n",
      "Epoch 290/2500\n",
      "120/120 [==============================] - 0s 168us/step - loss: 0.1291 - accuracy: 0.9500 - val_loss: 0.1039 - val_accuracy: 1.0000\n",
      "Epoch 291/2500\n",
      "120/120 [==============================] - 0s 168us/step - loss: 0.1289 - accuracy: 0.9500 - val_loss: 0.1031 - val_accuracy: 1.0000\n",
      "Epoch 292/2500\n",
      "120/120 [==============================] - 0s 181us/step - loss: 0.1296 - accuracy: 0.9500 - val_loss: 0.1047 - val_accuracy: 1.0000\n",
      "Epoch 293/2500\n",
      "120/120 [==============================] - 0s 145us/step - loss: 0.1296 - accuracy: 0.9500 - val_loss: 0.1022 - val_accuracy: 1.0000\n",
      "Epoch 294/2500\n",
      "120/120 [==============================] - 0s 129us/step - loss: 0.1278 - accuracy: 0.9500 - val_loss: 0.1024 - val_accuracy: 1.0000\n",
      "Epoch 295/2500\n",
      "120/120 [==============================] - 0s 146us/step - loss: 0.1292 - accuracy: 0.9500 - val_loss: 0.1014 - val_accuracy: 1.0000\n",
      "Epoch 296/2500\n",
      "120/120 [==============================] - 0s 115us/step - loss: 0.1269 - accuracy: 0.9583 - val_loss: 0.1017 - val_accuracy: 1.0000\n",
      "Epoch 297/2500\n",
      "120/120 [==============================] - 0s 115us/step - loss: 0.1269 - accuracy: 0.9500 - val_loss: 0.1032 - val_accuracy: 1.0000\n",
      "Epoch 298/2500\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.1275 - accuracy: 0.9500 - val_loss: 0.1027 - val_accuracy: 1.0000\n",
      "Epoch 299/2500\n",
      "120/120 [==============================] - 0s 112us/step - loss: 0.1276 - accuracy: 0.9500 - val_loss: 0.1007 - val_accuracy: 1.0000\n",
      "Epoch 300/2500\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.1269 - accuracy: 0.9500 - val_loss: 0.1018 - val_accuracy: 1.0000\n",
      "Epoch 301/2500\n",
      "120/120 [==============================] - 0s 129us/step - loss: 0.1259 - accuracy: 0.9500 - val_loss: 0.1008 - val_accuracy: 1.0000\n",
      "Epoch 302/2500\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.1279 - accuracy: 0.9500 - val_loss: 0.0990 - val_accuracy: 1.0000\n",
      "Epoch 303/2500\n",
      "120/120 [==============================] - 0s 120us/step - loss: 0.1254 - accuracy: 0.9583 - val_loss: 0.0992 - val_accuracy: 1.0000\n",
      "Epoch 304/2500\n",
      "120/120 [==============================] - 0s 123us/step - loss: 0.1267 - accuracy: 0.9500 - val_loss: 0.1014 - val_accuracy: 1.0000\n",
      "Epoch 305/2500\n",
      "120/120 [==============================] - 0s 130us/step - loss: 0.1264 - accuracy: 0.9500 - val_loss: 0.0987 - val_accuracy: 1.0000\n",
      "Epoch 306/2500\n",
      "120/120 [==============================] - 0s 118us/step - loss: 0.1251 - accuracy: 0.9583 - val_loss: 0.0991 - val_accuracy: 1.0000\n",
      "Epoch 307/2500\n",
      "120/120 [==============================] - 0s 122us/step - loss: 0.1244 - accuracy: 0.9500 - val_loss: 0.1003 - val_accuracy: 1.0000\n",
      "Epoch 308/2500\n",
      "120/120 [==============================] - 0s 114us/step - loss: 0.1239 - accuracy: 0.9500 - val_loss: 0.0999 - val_accuracy: 1.0000\n",
      "Epoch 309/2500\n",
      "120/120 [==============================] - 0s 107us/step - loss: 0.1242 - accuracy: 0.9500 - val_loss: 0.0992 - val_accuracy: 1.0000\n",
      "Epoch 310/2500\n",
      "120/120 [==============================] - 0s 109us/step - loss: 0.1236 - accuracy: 0.9500 - val_loss: 0.0976 - val_accuracy: 1.0000\n",
      "Epoch 311/2500\n",
      "120/120 [==============================] - 0s 114us/step - loss: 0.1247 - accuracy: 0.9583 - val_loss: 0.0964 - val_accuracy: 1.0000\n",
      "Epoch 312/2500\n",
      "120/120 [==============================] - 0s 110us/step - loss: 0.1230 - accuracy: 0.9583 - val_loss: 0.0970 - val_accuracy: 1.0000\n",
      "Epoch 313/2500\n",
      "120/120 [==============================] - 0s 119us/step - loss: 0.1234 - accuracy: 0.9583 - val_loss: 0.0992 - val_accuracy: 1.0000\n",
      "Epoch 314/2500\n",
      "120/120 [==============================] - 0s 122us/step - loss: 0.1228 - accuracy: 0.9500 - val_loss: 0.0972 - val_accuracy: 1.0000\n",
      "Epoch 315/2500\n",
      "120/120 [==============================] - 0s 114us/step - loss: 0.1224 - accuracy: 0.9583 - val_loss: 0.0962 - val_accuracy: 1.0000\n",
      "Epoch 316/2500\n",
      "120/120 [==============================] - 0s 127us/step - loss: 0.1221 - accuracy: 0.9583 - val_loss: 0.0957 - val_accuracy: 1.0000\n",
      "Epoch 317/2500\n",
      "120/120 [==============================] - 0s 129us/step - loss: 0.1212 - accuracy: 0.9583 - val_loss: 0.0963 - val_accuracy: 1.0000\n",
      "Epoch 318/2500\n",
      "120/120 [==============================] - 0s 116us/step - loss: 0.1214 - accuracy: 0.9500 - val_loss: 0.0973 - val_accuracy: 1.0000\n",
      "Epoch 319/2500\n",
      "120/120 [==============================] - 0s 121us/step - loss: 0.1214 - accuracy: 0.9500 - val_loss: 0.0975 - val_accuracy: 1.0000\n",
      "Epoch 320/2500\n",
      "120/120 [==============================] - 0s 120us/step - loss: 0.1232 - accuracy: 0.9500 - val_loss: 0.0985 - val_accuracy: 1.0000\n",
      "Epoch 321/2500\n",
      "120/120 [==============================] - 0s 122us/step - loss: 0.1204 - accuracy: 0.9500 - val_loss: 0.0944 - val_accuracy: 1.0000\n",
      "Epoch 322/2500\n",
      "120/120 [==============================] - 0s 126us/step - loss: 0.1204 - accuracy: 0.9583 - val_loss: 0.0940 - val_accuracy: 1.0000\n",
      "Epoch 323/2500\n",
      "120/120 [==============================] - 0s 128us/step - loss: 0.1203 - accuracy: 0.9583 - val_loss: 0.0939 - val_accuracy: 1.0000\n",
      "Epoch 324/2500\n",
      "120/120 [==============================] - 0s 124us/step - loss: 0.1206 - accuracy: 0.9583 - val_loss: 0.0932 - val_accuracy: 1.0000\n",
      "Epoch 325/2500\n",
      "120/120 [==============================] - 0s 124us/step - loss: 0.1198 - accuracy: 0.9583 - val_loss: 0.0943 - val_accuracy: 1.0000\n",
      "Epoch 326/2500\n",
      "120/120 [==============================] - 0s 136us/step - loss: 0.1200 - accuracy: 0.9583 - val_loss: 0.0956 - val_accuracy: 1.0000\n",
      "Epoch 327/2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 137us/step - loss: 0.1211 - accuracy: 0.9500 - val_loss: 0.0934 - val_accuracy: 1.0000\n",
      "Epoch 328/2500\n",
      "120/120 [==============================] - 0s 124us/step - loss: 0.1196 - accuracy: 0.9500 - val_loss: 0.0946 - val_accuracy: 1.0000\n",
      "Epoch 329/2500\n",
      "120/120 [==============================] - 0s 133us/step - loss: 0.1202 - accuracy: 0.9500 - val_loss: 0.0956 - val_accuracy: 1.0000\n",
      "Epoch 330/2500\n",
      "120/120 [==============================] - 0s 122us/step - loss: 0.1187 - accuracy: 0.9500 - val_loss: 0.0935 - val_accuracy: 1.0000\n",
      "Epoch 331/2500\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.1180 - accuracy: 0.9583 - val_loss: 0.0918 - val_accuracy: 1.0000\n",
      "Epoch 332/2500\n",
      "120/120 [==============================] - 0s 126us/step - loss: 0.1183 - accuracy: 0.9583 - val_loss: 0.0917 - val_accuracy: 1.0000\n",
      "Epoch 333/2500\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.1179 - accuracy: 0.9583 - val_loss: 0.0928 - val_accuracy: 1.0000\n",
      "Epoch 334/2500\n",
      "120/120 [==============================] - 0s 116us/step - loss: 0.1179 - accuracy: 0.9583 - val_loss: 0.0916 - val_accuracy: 1.0000\n",
      "Epoch 335/2500\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.1173 - accuracy: 0.9583 - val_loss: 0.0915 - val_accuracy: 1.0000\n",
      "Epoch 336/2500\n",
      "120/120 [==============================] - 0s 112us/step - loss: 0.1175 - accuracy: 0.9583 - val_loss: 0.0912 - val_accuracy: 1.0000\n",
      "Epoch 337/2500\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.1167 - accuracy: 0.9583 - val_loss: 0.0920 - val_accuracy: 1.0000\n",
      "Epoch 338/2500\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.1167 - accuracy: 0.9500 - val_loss: 0.0917 - val_accuracy: 1.0000\n",
      "Epoch 339/2500\n",
      "120/120 [==============================] - 0s 102us/step - loss: 0.1181 - accuracy: 0.9500 - val_loss: 0.0939 - val_accuracy: 1.0000\n",
      "Epoch 340/2500\n",
      "120/120 [==============================] - 0s 109us/step - loss: 0.1168 - accuracy: 0.9500 - val_loss: 0.0901 - val_accuracy: 1.0000\n",
      "Epoch 341/2500\n",
      "120/120 [==============================] - 0s 105us/step - loss: 0.1158 - accuracy: 0.9583 - val_loss: 0.0896 - val_accuracy: 1.0000\n",
      "Epoch 342/2500\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.1160 - accuracy: 0.9583 - val_loss: 0.0901 - val_accuracy: 1.0000\n",
      "Epoch 343/2500\n",
      "120/120 [==============================] - 0s 94us/step - loss: 0.1160 - accuracy: 0.9583 - val_loss: 0.0906 - val_accuracy: 1.0000\n",
      "Epoch 344/2500\n",
      "120/120 [==============================] - 0s 95us/step - loss: 0.1152 - accuracy: 0.9583 - val_loss: 0.0898 - val_accuracy: 1.0000\n",
      "Epoch 345/2500\n",
      "120/120 [==============================] - 0s 100us/step - loss: 0.1156 - accuracy: 0.9583 - val_loss: 0.0885 - val_accuracy: 1.0000\n",
      "Epoch 346/2500\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.1163 - accuracy: 0.9583 - val_loss: 0.0882 - val_accuracy: 1.0000\n",
      "Epoch 347/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.1149 - accuracy: 0.9500 - val_loss: 0.0904 - val_accuracy: 1.0000\n",
      "Epoch 348/2500\n",
      "120/120 [==============================] - 0s 103us/step - loss: 0.1154 - accuracy: 0.9500 - val_loss: 0.0904 - val_accuracy: 1.0000\n",
      "Epoch 349/2500\n",
      "120/120 [==============================] - 0s 91us/step - loss: 0.1150 - accuracy: 0.9500 - val_loss: 0.0906 - val_accuracy: 1.0000\n",
      "Epoch 350/2500\n",
      "120/120 [==============================] - 0s 103us/step - loss: 0.1147 - accuracy: 0.9500 - val_loss: 0.0897 - val_accuracy: 1.0000\n",
      "Epoch 351/2500\n",
      "120/120 [==============================] - 0s 112us/step - loss: 0.1178 - accuracy: 0.9500 - val_loss: 0.0866 - val_accuracy: 1.0000\n",
      "Epoch 352/2500\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.1165 - accuracy: 0.9583 - val_loss: 0.0891 - val_accuracy: 1.0000\n",
      "Epoch 353/2500\n",
      "120/120 [==============================] - 0s 104us/step - loss: 0.1136 - accuracy: 0.9583 - val_loss: 0.0880 - val_accuracy: 1.0000\n",
      "Epoch 354/2500\n",
      "120/120 [==============================] - 0s 95us/step - loss: 0.1132 - accuracy: 0.9583 - val_loss: 0.0873 - val_accuracy: 1.0000\n",
      "Epoch 355/2500\n",
      "120/120 [==============================] - 0s 96us/step - loss: 0.1141 - accuracy: 0.9583 - val_loss: 0.0881 - val_accuracy: 1.0000\n",
      "Epoch 356/2500\n",
      "120/120 [==============================] - 0s 109us/step - loss: 0.1139 - accuracy: 0.9583 - val_loss: 0.0865 - val_accuracy: 1.0000\n",
      "Epoch 357/2500\n",
      "120/120 [==============================] - 0s 120us/step - loss: 0.1129 - accuracy: 0.9583 - val_loss: 0.0862 - val_accuracy: 1.0000\n",
      "Epoch 358/2500\n",
      "120/120 [==============================] - 0s 104us/step - loss: 0.1122 - accuracy: 0.9583 - val_loss: 0.0872 - val_accuracy: 1.0000\n",
      "Epoch 359/2500\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.1135 - accuracy: 0.9583 - val_loss: 0.0869 - val_accuracy: 1.0000\n",
      "Epoch 360/2500\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.1130 - accuracy: 0.9583 - val_loss: 0.0888 - val_accuracy: 1.0000\n",
      "Epoch 361/2500\n",
      "120/120 [==============================] - 0s 105us/step - loss: 0.1129 - accuracy: 0.9500 - val_loss: 0.0894 - val_accuracy: 1.0000\n",
      "Epoch 362/2500\n",
      "120/120 [==============================] - 0s 116us/step - loss: 0.1117 - accuracy: 0.9500 - val_loss: 0.0860 - val_accuracy: 1.0000\n",
      "Epoch 363/2500\n",
      "120/120 [==============================] - 0s 121us/step - loss: 0.1122 - accuracy: 0.9583 - val_loss: 0.0846 - val_accuracy: 1.0000\n",
      "Epoch 364/2500\n",
      "120/120 [==============================] - 0s 120us/step - loss: 0.1122 - accuracy: 0.9583 - val_loss: 0.0846 - val_accuracy: 1.0000\n",
      "Epoch 365/2500\n",
      "120/120 [==============================] - 0s 112us/step - loss: 0.1113 - accuracy: 0.9583 - val_loss: 0.0855 - val_accuracy: 1.0000\n",
      "Epoch 366/2500\n",
      "120/120 [==============================] - 0s 128us/step - loss: 0.1156 - accuracy: 0.9500 - val_loss: 0.0892 - val_accuracy: 1.0000\n",
      "Epoch 367/2500\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.1115 - accuracy: 0.9583 - val_loss: 0.0847 - val_accuracy: 1.0000\n",
      "Epoch 368/2500\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.1108 - accuracy: 0.9583 - val_loss: 0.0837 - val_accuracy: 1.0000\n",
      "Epoch 369/2500\n",
      "120/120 [==============================] - 0s 105us/step - loss: 0.1109 - accuracy: 0.9583 - val_loss: 0.0846 - val_accuracy: 1.0000\n",
      "Epoch 370/2500\n",
      "120/120 [==============================] - 0s 122us/step - loss: 0.1107 - accuracy: 0.9583 - val_loss: 0.0850 - val_accuracy: 1.0000\n",
      "Epoch 371/2500\n",
      "120/120 [==============================] - 0s 114us/step - loss: 0.1114 - accuracy: 0.9583 - val_loss: 0.0839 - val_accuracy: 1.0000\n",
      "Epoch 372/2500\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.1103 - accuracy: 0.9583 - val_loss: 0.0840 - val_accuracy: 1.0000\n",
      "Epoch 373/2500\n",
      "120/120 [==============================] - 0s 120us/step - loss: 0.1102 - accuracy: 0.9583 - val_loss: 0.0835 - val_accuracy: 1.0000\n",
      "Epoch 374/2500\n",
      "120/120 [==============================] - 0s 107us/step - loss: 0.1098 - accuracy: 0.9583 - val_loss: 0.0854 - val_accuracy: 1.0000\n",
      "Epoch 375/2500\n",
      "120/120 [==============================] - 0s 121us/step - loss: 0.1093 - accuracy: 0.9583 - val_loss: 0.0853 - val_accuracy: 1.0000\n",
      "Epoch 376/2500\n",
      "120/120 [==============================] - 0s 122us/step - loss: 0.1092 - accuracy: 0.9583 - val_loss: 0.0848 - val_accuracy: 1.0000\n",
      "Epoch 377/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.1102 - accuracy: 0.9583 - val_loss: 0.0821 - val_accuracy: 1.0000\n",
      "Epoch 378/2500\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.1089 - accuracy: 0.9583 - val_loss: 0.0830 - val_accuracy: 1.0000\n",
      "Epoch 379/2500\n",
      "120/120 [==============================] - 0s 121us/step - loss: 0.1089 - accuracy: 0.9583 - val_loss: 0.0837 - val_accuracy: 1.0000\n",
      "Epoch 380/2500\n",
      "120/120 [==============================] - 0s 118us/step - loss: 0.1088 - accuracy: 0.9583 - val_loss: 0.0826 - val_accuracy: 1.0000\n",
      "Epoch 381/2500\n",
      "120/120 [==============================] - 0s 122us/step - loss: 0.1092 - accuracy: 0.9583 - val_loss: 0.0828 - val_accuracy: 1.0000\n",
      "Epoch 382/2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 116us/step - loss: 0.1089 - accuracy: 0.9583 - val_loss: 0.0817 - val_accuracy: 1.0000\n",
      "Epoch 383/2500\n",
      "120/120 [==============================] - 0s 110us/step - loss: 0.1082 - accuracy: 0.9583 - val_loss: 0.0829 - val_accuracy: 1.0000\n",
      "Epoch 384/2500\n",
      "120/120 [==============================] - 0s 118us/step - loss: 0.1083 - accuracy: 0.9583 - val_loss: 0.0830 - val_accuracy: 1.0000\n",
      "Epoch 385/2500\n",
      "120/120 [==============================] - 0s 116us/step - loss: 0.1083 - accuracy: 0.9583 - val_loss: 0.0845 - val_accuracy: 1.0000\n",
      "Epoch 386/2500\n",
      "120/120 [==============================] - 0s 116us/step - loss: 0.1081 - accuracy: 0.9583 - val_loss: 0.0828 - val_accuracy: 1.0000\n",
      "Epoch 387/2500\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.1078 - accuracy: 0.9583 - val_loss: 0.0811 - val_accuracy: 1.0000\n",
      "Epoch 388/2500\n",
      "120/120 [==============================] - 0s 121us/step - loss: 0.1072 - accuracy: 0.9583 - val_loss: 0.0807 - val_accuracy: 1.0000\n",
      "Epoch 389/2500\n",
      "120/120 [==============================] - 0s 130us/step - loss: 0.1077 - accuracy: 0.9583 - val_loss: 0.0797 - val_accuracy: 1.0000\n",
      "Epoch 390/2500\n",
      "120/120 [==============================] - 0s 120us/step - loss: 0.1067 - accuracy: 0.9583 - val_loss: 0.0809 - val_accuracy: 1.0000\n",
      "Epoch 391/2500\n",
      "120/120 [==============================] - 0s 115us/step - loss: 0.1073 - accuracy: 0.9583 - val_loss: 0.0814 - val_accuracy: 1.0000\n",
      "Epoch 392/2500\n",
      "120/120 [==============================] - 0s 122us/step - loss: 0.1066 - accuracy: 0.9583 - val_loss: 0.0832 - val_accuracy: 1.0000\n",
      "Epoch 393/2500\n",
      "120/120 [==============================] - 0s 113us/step - loss: 0.1068 - accuracy: 0.9583 - val_loss: 0.0824 - val_accuracy: 1.0000\n",
      "Epoch 394/2500\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.1064 - accuracy: 0.9583 - val_loss: 0.0802 - val_accuracy: 1.0000\n",
      "Epoch 395/2500\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.1073 - accuracy: 0.9583 - val_loss: 0.0817 - val_accuracy: 1.0000\n",
      "Epoch 396/2500\n",
      "120/120 [==============================] - 0s 109us/step - loss: 0.1068 - accuracy: 0.9583 - val_loss: 0.0797 - val_accuracy: 1.0000\n",
      "Epoch 397/2500\n",
      "120/120 [==============================] - 0s 114us/step - loss: 0.1082 - accuracy: 0.9583 - val_loss: 0.0783 - val_accuracy: 1.0000\n",
      "Epoch 398/2500\n",
      "120/120 [==============================] - 0s 134us/step - loss: 0.1057 - accuracy: 0.9583 - val_loss: 0.0794 - val_accuracy: 1.0000\n",
      "Epoch 399/2500\n",
      "120/120 [==============================] - 0s 114us/step - loss: 0.1062 - accuracy: 0.9583 - val_loss: 0.0822 - val_accuracy: 1.0000\n",
      "Epoch 400/2500\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.1064 - accuracy: 0.9583 - val_loss: 0.0827 - val_accuracy: 1.0000\n",
      "Epoch 401/2500\n",
      "120/120 [==============================] - 0s 105us/step - loss: 0.1050 - accuracy: 0.9500 - val_loss: 0.0799 - val_accuracy: 1.0000\n",
      "Epoch 402/2500\n",
      "120/120 [==============================] - 0s 110us/step - loss: 0.1058 - accuracy: 0.9583 - val_loss: 0.0778 - val_accuracy: 1.0000\n",
      "Epoch 403/2500\n",
      "120/120 [==============================] - 0s 99us/step - loss: 0.1053 - accuracy: 0.9583 - val_loss: 0.0780 - val_accuracy: 1.0000\n",
      "Epoch 404/2500\n",
      "120/120 [==============================] - 0s 101us/step - loss: 0.1054 - accuracy: 0.9583 - val_loss: 0.0804 - val_accuracy: 1.0000\n",
      "Epoch 405/2500\n",
      "120/120 [==============================] - 0s 99us/step - loss: 0.1048 - accuracy: 0.9583 - val_loss: 0.0789 - val_accuracy: 1.0000\n",
      "Epoch 406/2500\n",
      "120/120 [==============================] - 0s 104us/step - loss: 0.1056 - accuracy: 0.9583 - val_loss: 0.0786 - val_accuracy: 1.0000\n",
      "Epoch 407/2500\n",
      "120/120 [==============================] - 0s 103us/step - loss: 0.1044 - accuracy: 0.9583 - val_loss: 0.0782 - val_accuracy: 1.0000\n",
      "Epoch 408/2500\n",
      "120/120 [==============================] - 0s 94us/step - loss: 0.1042 - accuracy: 0.9583 - val_loss: 0.0790 - val_accuracy: 1.0000\n",
      "Epoch 409/2500\n",
      "120/120 [==============================] - 0s 102us/step - loss: 0.1040 - accuracy: 0.9583 - val_loss: 0.0785 - val_accuracy: 1.0000\n",
      "Epoch 410/2500\n",
      "120/120 [==============================] - 0s 124us/step - loss: 0.1044 - accuracy: 0.9583 - val_loss: 0.0793 - val_accuracy: 1.0000\n",
      "Epoch 411/2500\n",
      "120/120 [==============================] - 0s 148us/step - loss: 0.1043 - accuracy: 0.9583 - val_loss: 0.0774 - val_accuracy: 1.0000\n",
      "Epoch 412/2500\n",
      "120/120 [==============================] - 0s 143us/step - loss: 0.1037 - accuracy: 0.9583 - val_loss: 0.0769 - val_accuracy: 1.0000\n",
      "Epoch 413/2500\n",
      "120/120 [==============================] - 0s 119us/step - loss: 0.1047 - accuracy: 0.9583 - val_loss: 0.0779 - val_accuracy: 1.0000\n",
      "Epoch 414/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.1034 - accuracy: 0.9583 - val_loss: 0.0767 - val_accuracy: 1.0000\n",
      "Epoch 415/2500\n",
      "120/120 [==============================] - 0s 100us/step - loss: 0.1031 - accuracy: 0.9583 - val_loss: 0.0770 - val_accuracy: 1.0000\n",
      "Epoch 416/2500\n",
      "120/120 [==============================] - 0s 103us/step - loss: 0.1029 - accuracy: 0.9583 - val_loss: 0.0777 - val_accuracy: 1.0000\n",
      "Epoch 417/2500\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.1028 - accuracy: 0.9583 - val_loss: 0.0773 - val_accuracy: 1.0000\n",
      "Epoch 418/2500\n",
      "120/120 [==============================] - 0s 105us/step - loss: 0.1028 - accuracy: 0.9583 - val_loss: 0.0773 - val_accuracy: 1.0000\n",
      "Epoch 419/2500\n",
      "120/120 [==============================] - 0s 110us/step - loss: 0.1027 - accuracy: 0.9583 - val_loss: 0.0772 - val_accuracy: 1.0000\n",
      "Epoch 420/2500\n",
      "120/120 [==============================] - 0s 116us/step - loss: 0.1027 - accuracy: 0.9583 - val_loss: 0.0773 - val_accuracy: 1.0000\n",
      "Epoch 421/2500\n",
      "120/120 [==============================] - 0s 103us/step - loss: 0.1024 - accuracy: 0.9583 - val_loss: 0.0773 - val_accuracy: 1.0000\n",
      "Epoch 422/2500\n",
      "120/120 [==============================] - 0s 131us/step - loss: 0.1028 - accuracy: 0.9583 - val_loss: 0.0772 - val_accuracy: 1.0000\n",
      "Epoch 423/2500\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.1080 - accuracy: 0.9583 - val_loss: 0.0731 - val_accuracy: 1.0000\n",
      "Epoch 424/2500\n",
      "120/120 [==============================] - 0s 109us/step - loss: 0.1023 - accuracy: 0.9583 - val_loss: 0.0740 - val_accuracy: 1.0000\n",
      "Epoch 425/2500\n",
      "120/120 [==============================] - 0s 113us/step - loss: 0.1010 - accuracy: 0.9583 - val_loss: 0.0761 - val_accuracy: 1.0000\n",
      "Epoch 426/2500\n",
      "120/120 [==============================] - 0s 104us/step - loss: 0.1021 - accuracy: 0.9583 - val_loss: 0.0802 - val_accuracy: 1.0000\n",
      "Epoch 427/2500\n",
      "120/120 [==============================] - 0s 105us/step - loss: 0.1023 - accuracy: 0.9500 - val_loss: 0.0777 - val_accuracy: 1.0000\n",
      "Epoch 428/2500\n",
      "120/120 [==============================] - 0s 115us/step - loss: 0.1017 - accuracy: 0.9583 - val_loss: 0.0754 - val_accuracy: 1.0000\n",
      "Epoch 429/2500\n",
      "120/120 [==============================] - 0s 112us/step - loss: 0.1014 - accuracy: 0.9583 - val_loss: 0.0759 - val_accuracy: 1.0000\n",
      "Epoch 430/2500\n",
      "120/120 [==============================] - 0s 105us/step - loss: 0.1013 - accuracy: 0.9583 - val_loss: 0.0745 - val_accuracy: 1.0000\n",
      "Epoch 431/2500\n",
      "120/120 [==============================] - 0s 101us/step - loss: 0.1030 - accuracy: 0.9583 - val_loss: 0.0731 - val_accuracy: 1.0000\n",
      "Epoch 432/2500\n",
      "120/120 [==============================] - 0s 112us/step - loss: 0.1012 - accuracy: 0.9583 - val_loss: 0.0765 - val_accuracy: 1.0000\n",
      "Epoch 433/2500\n",
      "120/120 [==============================] - 0s 105us/step - loss: 0.1043 - accuracy: 0.9583 - val_loss: 0.0734 - val_accuracy: 1.0000\n",
      "Epoch 434/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.1027 - accuracy: 0.9583 - val_loss: 0.0788 - val_accuracy: 1.0000\n",
      "Epoch 435/2500\n",
      "120/120 [==============================] - 0s 102us/step - loss: 0.1009 - accuracy: 0.9583 - val_loss: 0.0761 - val_accuracy: 1.0000\n",
      "Epoch 436/2500\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.1010 - accuracy: 0.9583 - val_loss: 0.0765 - val_accuracy: 1.0000\n",
      "Epoch 437/2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 109us/step - loss: 0.1008 - accuracy: 0.9583 - val_loss: 0.0753 - val_accuracy: 1.0000\n",
      "Epoch 438/2500\n",
      "120/120 [==============================] - 0s 112us/step - loss: 0.1004 - accuracy: 0.9583 - val_loss: 0.0736 - val_accuracy: 1.0000\n",
      "Epoch 439/2500\n",
      "120/120 [==============================] - 0s 129us/step - loss: 0.1024 - accuracy: 0.9583 - val_loss: 0.0717 - val_accuracy: 1.0000\n",
      "Epoch 440/2500\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0997 - accuracy: 0.9583 - val_loss: 0.0740 - val_accuracy: 1.0000\n",
      "Epoch 441/2500\n",
      "120/120 [==============================] - 0s 92us/step - loss: 0.0999 - accuracy: 0.9583 - val_loss: 0.0762 - val_accuracy: 1.0000\n",
      "Epoch 442/2500\n",
      "120/120 [==============================] - 0s 95us/step - loss: 0.1000 - accuracy: 0.9583 - val_loss: 0.0758 - val_accuracy: 1.0000\n",
      "Epoch 443/2500\n",
      "120/120 [==============================] - 0s 112us/step - loss: 0.1009 - accuracy: 0.9583 - val_loss: 0.0726 - val_accuracy: 1.0000\n",
      "Epoch 444/2500\n",
      "120/120 [==============================] - 0s 113us/step - loss: 0.0998 - accuracy: 0.9583 - val_loss: 0.0753 - val_accuracy: 1.0000\n",
      "Epoch 445/2500\n",
      "120/120 [==============================] - 0s 105us/step - loss: 0.1004 - accuracy: 0.9583 - val_loss: 0.0757 - val_accuracy: 1.0000\n",
      "Epoch 446/2500\n",
      "120/120 [==============================] - 0s 107us/step - loss: 0.1031 - accuracy: 0.9583 - val_loss: 0.0706 - val_accuracy: 1.0000\n",
      "Epoch 447/2500\n",
      "120/120 [==============================] - 0s 101us/step - loss: 0.0998 - accuracy: 0.9583 - val_loss: 0.0738 - val_accuracy: 1.0000\n",
      "Epoch 448/2500\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0993 - accuracy: 0.9583 - val_loss: 0.0724 - val_accuracy: 1.0000\n",
      "Epoch 449/2500\n",
      "120/120 [==============================] - 0s 109us/step - loss: 0.1008 - accuracy: 0.9583 - val_loss: 0.0758 - val_accuracy: 1.0000\n",
      "Epoch 450/2500\n",
      "120/120 [==============================] - 0s 105us/step - loss: 0.0995 - accuracy: 0.9583 - val_loss: 0.0726 - val_accuracy: 1.0000\n",
      "Epoch 451/2500\n",
      "120/120 [==============================] - 0s 105us/step - loss: 0.0987 - accuracy: 0.9583 - val_loss: 0.0718 - val_accuracy: 1.0000\n",
      "Epoch 452/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.0990 - accuracy: 0.9583 - val_loss: 0.0714 - val_accuracy: 1.0000\n",
      "Epoch 453/2500\n",
      "120/120 [==============================] - 0s 114us/step - loss: 0.0997 - accuracy: 0.9583 - val_loss: 0.0751 - val_accuracy: 1.0000\n",
      "Epoch 454/2500\n",
      "120/120 [==============================] - 0s 103us/step - loss: 0.0982 - accuracy: 0.9583 - val_loss: 0.0739 - val_accuracy: 1.0000\n",
      "Epoch 455/2500\n",
      "120/120 [==============================] - 0s 97us/step - loss: 0.0986 - accuracy: 0.9583 - val_loss: 0.0713 - val_accuracy: 1.0000\n",
      "Epoch 456/2500\n",
      "120/120 [==============================] - 0s 103us/step - loss: 0.0984 - accuracy: 0.9583 - val_loss: 0.0721 - val_accuracy: 1.0000\n",
      "Epoch 457/2500\n",
      "120/120 [==============================] - 0s 109us/step - loss: 0.0992 - accuracy: 0.9583 - val_loss: 0.0704 - val_accuracy: 1.0000\n",
      "Epoch 458/2500\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.0977 - accuracy: 0.9583 - val_loss: 0.0731 - val_accuracy: 1.0000\n",
      "Epoch 459/2500\n",
      "120/120 [==============================] - 0s 104us/step - loss: 0.0983 - accuracy: 0.9583 - val_loss: 0.0752 - val_accuracy: 1.0000\n",
      "Epoch 460/2500\n",
      "120/120 [==============================] - 0s 109us/step - loss: 0.0978 - accuracy: 0.9583 - val_loss: 0.0751 - val_accuracy: 1.0000\n",
      "Epoch 461/2500\n",
      "120/120 [==============================] - 0s 101us/step - loss: 0.0973 - accuracy: 0.9583 - val_loss: 0.0718 - val_accuracy: 1.0000\n",
      "Epoch 462/2500\n",
      "120/120 [==============================] - 0s 102us/step - loss: 0.1000 - accuracy: 0.9583 - val_loss: 0.0689 - val_accuracy: 1.0000\n",
      "Epoch 463/2500\n",
      "120/120 [==============================] - 0s 103us/step - loss: 0.0969 - accuracy: 0.9583 - val_loss: 0.0708 - val_accuracy: 1.0000\n",
      "Epoch 464/2500\n",
      "120/120 [==============================] - 0s 115us/step - loss: 0.0980 - accuracy: 0.9583 - val_loss: 0.0742 - val_accuracy: 1.0000\n",
      "Epoch 465/2500\n",
      "120/120 [==============================] - 0s 107us/step - loss: 0.0975 - accuracy: 0.9583 - val_loss: 0.0734 - val_accuracy: 1.0000\n",
      "Epoch 466/2500\n",
      "120/120 [==============================] - 0s 113us/step - loss: 0.0985 - accuracy: 0.9583 - val_loss: 0.0692 - val_accuracy: 1.0000\n",
      "Epoch 467/2500\n",
      "120/120 [==============================] - 0s 109us/step - loss: 0.0970 - accuracy: 0.9583 - val_loss: 0.0695 - val_accuracy: 1.0000\n",
      "Epoch 468/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.0966 - accuracy: 0.9583 - val_loss: 0.0707 - val_accuracy: 1.0000\n",
      "Epoch 469/2500\n",
      "120/120 [==============================] - 0s 105us/step - loss: 0.0974 - accuracy: 0.9583 - val_loss: 0.0748 - val_accuracy: 1.0000\n",
      "Epoch 470/2500\n",
      "120/120 [==============================] - 0s 134us/step - loss: 0.1013 - accuracy: 0.9583 - val_loss: 0.0687 - val_accuracy: 1.0000\n",
      "Epoch 471/2500\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.0965 - accuracy: 0.9583 - val_loss: 0.0712 - val_accuracy: 1.0000\n",
      "Epoch 472/2500\n",
      "120/120 [==============================] - 0s 105us/step - loss: 0.0962 - accuracy: 0.9583 - val_loss: 0.0713 - val_accuracy: 1.0000\n",
      "Epoch 473/2500\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0967 - accuracy: 0.9583 - val_loss: 0.0734 - val_accuracy: 1.0000\n",
      "Epoch 474/2500\n",
      "120/120 [==============================] - 0s 105us/step - loss: 0.0958 - accuracy: 0.9583 - val_loss: 0.0717 - val_accuracy: 1.0000\n",
      "Epoch 475/2500\n",
      "120/120 [==============================] - 0s 102us/step - loss: 0.0985 - accuracy: 0.9583 - val_loss: 0.0735 - val_accuracy: 1.0000\n",
      "Epoch 476/2500\n",
      "120/120 [==============================] - 0s 101us/step - loss: 0.0967 - accuracy: 0.9583 - val_loss: 0.0681 - val_accuracy: 1.0000\n",
      "Epoch 477/2500\n",
      "120/120 [==============================] - 0s 100us/step - loss: 0.0977 - accuracy: 0.9583 - val_loss: 0.0705 - val_accuracy: 1.0000\n",
      "Epoch 478/2500\n",
      "120/120 [==============================] - 0s 104us/step - loss: 0.0969 - accuracy: 0.9583 - val_loss: 0.0682 - val_accuracy: 1.0000\n",
      "Epoch 479/2500\n",
      "120/120 [==============================] - 0s 107us/step - loss: 0.0976 - accuracy: 0.9583 - val_loss: 0.0710 - val_accuracy: 1.0000\n",
      "Epoch 480/2500\n",
      "120/120 [==============================] - 0s 124us/step - loss: 0.0958 - accuracy: 0.9583 - val_loss: 0.0699 - val_accuracy: 1.0000\n",
      "Epoch 481/2500\n",
      "120/120 [==============================] - 0s 119us/step - loss: 0.0957 - accuracy: 0.9583 - val_loss: 0.0682 - val_accuracy: 1.0000\n",
      "Epoch 482/2500\n",
      "120/120 [==============================] - 0s 109us/step - loss: 0.0984 - accuracy: 0.9583 - val_loss: 0.0727 - val_accuracy: 1.0000\n",
      "Epoch 483/2500\n",
      "120/120 [==============================] - 0s 99us/step - loss: 0.0955 - accuracy: 0.9583 - val_loss: 0.0689 - val_accuracy: 1.0000\n",
      "Epoch 484/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.0953 - accuracy: 0.9583 - val_loss: 0.0700 - val_accuracy: 1.0000\n",
      "Epoch 485/2500\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0946 - accuracy: 0.9583 - val_loss: 0.0684 - val_accuracy: 1.0000\n",
      "Epoch 486/2500\n",
      "120/120 [==============================] - 0s 107us/step - loss: 0.0949 - accuracy: 0.9583 - val_loss: 0.0683 - val_accuracy: 1.0000\n",
      "Epoch 487/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.0954 - accuracy: 0.9583 - val_loss: 0.0670 - val_accuracy: 1.0000\n",
      "Epoch 488/2500\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0948 - accuracy: 0.9583 - val_loss: 0.0709 - val_accuracy: 1.0000\n",
      "Epoch 489/2500\n",
      "120/120 [==============================] - 0s 101us/step - loss: 0.0961 - accuracy: 0.9583 - val_loss: 0.0719 - val_accuracy: 1.0000\n",
      "Epoch 490/2500\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0953 - accuracy: 0.9583 - val_loss: 0.0683 - val_accuracy: 1.0000\n",
      "Epoch 491/2500\n",
      "120/120 [==============================] - 0s 116us/step - loss: 0.0947 - accuracy: 0.9583 - val_loss: 0.0695 - val_accuracy: 1.0000\n",
      "Epoch 492/2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 102us/step - loss: 0.0946 - accuracy: 0.9583 - val_loss: 0.0711 - val_accuracy: 1.0000\n",
      "Epoch 493/2500\n",
      "120/120 [==============================] - 0s 102us/step - loss: 0.0939 - accuracy: 0.9583 - val_loss: 0.0681 - val_accuracy: 1.0000\n",
      "Epoch 494/2500\n",
      "120/120 [==============================] - 0s 102us/step - loss: 0.0943 - accuracy: 0.9583 - val_loss: 0.0672 - val_accuracy: 1.0000\n",
      "Epoch 495/2500\n",
      "120/120 [==============================] - 0s 135us/step - loss: 0.0945 - accuracy: 0.9583 - val_loss: 0.0662 - val_accuracy: 1.0000\n",
      "Epoch 496/2500\n",
      "120/120 [==============================] - 0s 116us/step - loss: 0.0951 - accuracy: 0.9583 - val_loss: 0.0705 - val_accuracy: 1.0000\n",
      "Epoch 497/2500\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0936 - accuracy: 0.9583 - val_loss: 0.0711 - val_accuracy: 1.0000\n",
      "Epoch 498/2500\n",
      "120/120 [==============================] - 0s 118us/step - loss: 0.0945 - accuracy: 0.9583 - val_loss: 0.0695 - val_accuracy: 1.0000\n",
      "Epoch 499/2500\n",
      "120/120 [==============================] - 0s 88us/step - loss: 0.0937 - accuracy: 0.9583 - val_loss: 0.0667 - val_accuracy: 1.0000\n",
      "Epoch 500/2500\n",
      "120/120 [==============================] - 0s 99us/step - loss: 0.0934 - accuracy: 0.9583 - val_loss: 0.0667 - val_accuracy: 1.0000\n",
      "Epoch 501/2500\n",
      "120/120 [==============================] - 0s 102us/step - loss: 0.0938 - accuracy: 0.9583 - val_loss: 0.0658 - val_accuracy: 1.0000\n",
      "Epoch 502/2500\n",
      "120/120 [==============================] - 0s 109us/step - loss: 0.0945 - accuracy: 0.9583 - val_loss: 0.0703 - val_accuracy: 1.0000\n",
      "Epoch 503/2500\n",
      "120/120 [==============================] - 0s 109us/step - loss: 0.0929 - accuracy: 0.9583 - val_loss: 0.0695 - val_accuracy: 1.0000\n",
      "Epoch 504/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.0950 - accuracy: 0.9583 - val_loss: 0.0718 - val_accuracy: 1.0000\n",
      "Epoch 505/2500\n",
      "120/120 [==============================] - 0s 109us/step - loss: 0.0940 - accuracy: 0.9583 - val_loss: 0.0656 - val_accuracy: 1.0000\n",
      "Epoch 506/2500\n",
      "120/120 [==============================] - 0s 105us/step - loss: 0.0929 - accuracy: 0.9583 - val_loss: 0.0668 - val_accuracy: 1.0000\n",
      "Epoch 507/2500\n",
      "120/120 [==============================] - 0s 103us/step - loss: 0.0926 - accuracy: 0.9583 - val_loss: 0.0681 - val_accuracy: 1.0000\n",
      "Epoch 508/2500\n",
      "120/120 [==============================] - 0s 102us/step - loss: 0.0924 - accuracy: 0.9583 - val_loss: 0.0677 - val_accuracy: 1.0000\n",
      "Epoch 509/2500\n",
      "120/120 [==============================] - 0s 110us/step - loss: 0.0925 - accuracy: 0.9583 - val_loss: 0.0672 - val_accuracy: 1.0000\n",
      "Epoch 510/2500\n",
      "120/120 [==============================] - 0s 102us/step - loss: 0.0933 - accuracy: 0.9583 - val_loss: 0.0655 - val_accuracy: 1.0000\n",
      "Epoch 511/2500\n",
      "120/120 [==============================] - 0s 98us/step - loss: 0.0924 - accuracy: 0.9583 - val_loss: 0.0679 - val_accuracy: 1.0000\n",
      "Epoch 512/2500\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.0933 - accuracy: 0.9583 - val_loss: 0.0701 - val_accuracy: 1.0000\n",
      "Epoch 513/2500\n",
      "120/120 [==============================] - 0s 100us/step - loss: 0.0945 - accuracy: 0.9583 - val_loss: 0.0724 - val_accuracy: 0.9667\n",
      "Epoch 514/2500\n",
      "120/120 [==============================] - 0s 109us/step - loss: 0.0970 - accuracy: 0.9583 - val_loss: 0.0642 - val_accuracy: 1.0000\n",
      "Epoch 515/2500\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0923 - accuracy: 0.9583 - val_loss: 0.0646 - val_accuracy: 1.0000\n",
      "Epoch 516/2500\n",
      "120/120 [==============================] - 0s 103us/step - loss: 0.0920 - accuracy: 0.9583 - val_loss: 0.0673 - val_accuracy: 1.0000\n",
      "Epoch 517/2500\n",
      "120/120 [==============================] - 0s 92us/step - loss: 0.0930 - accuracy: 0.9583 - val_loss: 0.0666 - val_accuracy: 1.0000\n",
      "Epoch 518/2500\n",
      "120/120 [==============================] - 0s 97us/step - loss: 0.0928 - accuracy: 0.9583 - val_loss: 0.0657 - val_accuracy: 1.0000\n",
      "Epoch 519/2500\n",
      "120/120 [==============================] - 0s 104us/step - loss: 0.0916 - accuracy: 0.9583 - val_loss: 0.0677 - val_accuracy: 1.0000\n",
      "Epoch 520/2500\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.0915 - accuracy: 0.9583 - val_loss: 0.0685 - val_accuracy: 1.0000\n",
      "Epoch 521/2500\n",
      "120/120 [==============================] - 0s 97us/step - loss: 0.0919 - accuracy: 0.9583 - val_loss: 0.0703 - val_accuracy: 1.0000\n",
      "Epoch 522/2500\n",
      "120/120 [==============================] - 0s 97us/step - loss: 0.0923 - accuracy: 0.9583 - val_loss: 0.0660 - val_accuracy: 1.0000\n",
      "Epoch 523/2500\n",
      "120/120 [==============================] - 0s 103us/step - loss: 0.0918 - accuracy: 0.9583 - val_loss: 0.0676 - val_accuracy: 1.0000\n",
      "Epoch 524/2500\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0917 - accuracy: 0.9583 - val_loss: 0.0649 - val_accuracy: 1.0000\n",
      "Epoch 525/2500\n",
      "120/120 [==============================] - 0s 105us/step - loss: 0.0913 - accuracy: 0.9583 - val_loss: 0.0647 - val_accuracy: 1.0000\n",
      "Epoch 526/2500\n",
      "120/120 [==============================] - 0s 104us/step - loss: 0.0915 - accuracy: 0.9583 - val_loss: 0.0685 - val_accuracy: 1.0000\n",
      "Epoch 527/2500\n",
      "120/120 [==============================] - 0s 98us/step - loss: 0.0917 - accuracy: 0.9583 - val_loss: 0.0702 - val_accuracy: 0.9667\n",
      "Epoch 528/2500\n",
      "120/120 [==============================] - 0s 102us/step - loss: 0.0913 - accuracy: 0.9583 - val_loss: 0.0654 - val_accuracy: 1.0000\n",
      "Epoch 529/2500\n",
      "120/120 [==============================] - 0s 101us/step - loss: 0.0907 - accuracy: 0.9583 - val_loss: 0.0660 - val_accuracy: 1.0000\n",
      "Epoch 530/2500\n",
      "120/120 [==============================] - 0s 100us/step - loss: 0.0915 - accuracy: 0.9583 - val_loss: 0.0675 - val_accuracy: 1.0000\n",
      "Epoch 531/2500\n",
      "120/120 [==============================] - 0s 105us/step - loss: 0.0906 - accuracy: 0.9583 - val_loss: 0.0663 - val_accuracy: 1.0000\n",
      "Epoch 532/2500\n",
      "120/120 [==============================] - 0s 93us/step - loss: 0.0941 - accuracy: 0.9583 - val_loss: 0.0619 - val_accuracy: 1.0000\n",
      "Epoch 533/2500\n",
      "120/120 [==============================] - 0s 100us/step - loss: 0.0919 - accuracy: 0.9583 - val_loss: 0.0661 - val_accuracy: 1.0000\n",
      "Epoch 534/2500\n",
      "120/120 [==============================] - 0s 119us/step - loss: 0.0901 - accuracy: 0.9583 - val_loss: 0.0674 - val_accuracy: 1.0000\n",
      "Epoch 535/2500\n",
      "120/120 [==============================] - 0s 122us/step - loss: 0.0918 - accuracy: 0.9583 - val_loss: 0.0644 - val_accuracy: 1.0000\n",
      "Epoch 536/2500\n",
      "120/120 [==============================] - 0s 112us/step - loss: 0.0906 - accuracy: 0.9583 - val_loss: 0.0668 - val_accuracy: 1.0000\n",
      "Epoch 537/2500\n",
      "120/120 [==============================] - 0s 114us/step - loss: 0.0903 - accuracy: 0.9583 - val_loss: 0.0683 - val_accuracy: 1.0000\n",
      "Epoch 538/2500\n",
      "120/120 [==============================] - 0s 121us/step - loss: 0.0938 - accuracy: 0.9583 - val_loss: 0.0623 - val_accuracy: 1.0000\n",
      "Epoch 539/2500\n",
      "120/120 [==============================] - 0s 110us/step - loss: 0.0902 - accuracy: 0.9583 - val_loss: 0.0632 - val_accuracy: 1.0000\n",
      "Epoch 540/2500\n",
      "120/120 [==============================] - 0s 110us/step - loss: 0.0893 - accuracy: 0.9583 - val_loss: 0.0646 - val_accuracy: 1.0000\n",
      "Epoch 541/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.0897 - accuracy: 0.9583 - val_loss: 0.0690 - val_accuracy: 0.9667\n",
      "Epoch 542/2500\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0898 - accuracy: 0.9583 - val_loss: 0.0702 - val_accuracy: 0.9667\n",
      "Epoch 543/2500\n",
      "120/120 [==============================] - 0s 110us/step - loss: 0.0893 - accuracy: 0.9583 - val_loss: 0.0665 - val_accuracy: 1.0000\n",
      "Epoch 544/2500\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.0914 - accuracy: 0.9583 - val_loss: 0.0621 - val_accuracy: 1.0000\n",
      "Epoch 545/2500\n",
      "120/120 [==============================] - 0s 124us/step - loss: 0.0897 - accuracy: 0.9583 - val_loss: 0.0624 - val_accuracy: 1.0000\n",
      "Epoch 546/2500\n",
      "120/120 [==============================] - 0s 126us/step - loss: 0.0904 - accuracy: 0.9583 - val_loss: 0.0665 - val_accuracy: 1.0000\n",
      "Epoch 547/2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 114us/step - loss: 0.0917 - accuracy: 0.9583 - val_loss: 0.0702 - val_accuracy: 0.9667\n",
      "Epoch 548/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.0890 - accuracy: 0.9583 - val_loss: 0.0673 - val_accuracy: 1.0000\n",
      "Epoch 549/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.0916 - accuracy: 0.9583 - val_loss: 0.0608 - val_accuracy: 1.0000\n",
      "Epoch 550/2500\n",
      "120/120 [==============================] - 0s 120us/step - loss: 0.0890 - accuracy: 0.9583 - val_loss: 0.0626 - val_accuracy: 1.0000\n",
      "Epoch 551/2500\n",
      "120/120 [==============================] - 0s 132us/step - loss: 0.0895 - accuracy: 0.9583 - val_loss: 0.0640 - val_accuracy: 1.0000\n",
      "Epoch 552/2500\n",
      "120/120 [==============================] - 0s 110us/step - loss: 0.0887 - accuracy: 0.9583 - val_loss: 0.0643 - val_accuracy: 1.0000\n",
      "Epoch 553/2500\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0886 - accuracy: 0.9583 - val_loss: 0.0672 - val_accuracy: 1.0000\n",
      "Epoch 554/2500\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0892 - accuracy: 0.9583 - val_loss: 0.0672 - val_accuracy: 1.0000\n",
      "Epoch 555/2500\n",
      "120/120 [==============================] - 0s 101us/step - loss: 0.0883 - accuracy: 0.9583 - val_loss: 0.0650 - val_accuracy: 1.0000\n",
      "Epoch 556/2500\n",
      "120/120 [==============================] - 0s 103us/step - loss: 0.0888 - accuracy: 0.9583 - val_loss: 0.0634 - val_accuracy: 1.0000\n",
      "Epoch 557/2500\n",
      "120/120 [==============================] - 0s 110us/step - loss: 0.0900 - accuracy: 0.9583 - val_loss: 0.0611 - val_accuracy: 1.0000\n",
      "Epoch 558/2500\n",
      "120/120 [==============================] - 0s 112us/step - loss: 0.0887 - accuracy: 0.9583 - val_loss: 0.0624 - val_accuracy: 1.0000\n",
      "Epoch 559/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.0883 - accuracy: 0.9583 - val_loss: 0.0662 - val_accuracy: 1.0000\n",
      "Epoch 560/2500\n",
      "120/120 [==============================] - 0s 119us/step - loss: 0.0884 - accuracy: 0.9583 - val_loss: 0.0678 - val_accuracy: 0.9667\n",
      "Epoch 561/2500\n",
      "120/120 [==============================] - 0s 121us/step - loss: 0.0893 - accuracy: 0.9583 - val_loss: 0.0693 - val_accuracy: 0.9667\n",
      "Epoch 562/2500\n",
      "120/120 [==============================] - 0s 137us/step - loss: 0.0880 - accuracy: 0.9583 - val_loss: 0.0640 - val_accuracy: 1.0000\n",
      "Epoch 563/2500\n",
      "120/120 [==============================] - 0s 145us/step - loss: 0.0880 - accuracy: 0.9583 - val_loss: 0.0624 - val_accuracy: 1.0000\n",
      "Epoch 564/2500\n",
      "120/120 [==============================] - 0s 128us/step - loss: 0.0882 - accuracy: 0.9583 - val_loss: 0.0636 - val_accuracy: 1.0000\n",
      "Epoch 565/2500\n",
      "120/120 [==============================] - 0s 116us/step - loss: 0.0887 - accuracy: 0.9583 - val_loss: 0.0647 - val_accuracy: 1.0000\n",
      "Epoch 566/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.0913 - accuracy: 0.9583 - val_loss: 0.0689 - val_accuracy: 0.9667\n",
      "Epoch 567/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.0879 - accuracy: 0.9583 - val_loss: 0.0628 - val_accuracy: 1.0000\n",
      "Epoch 568/2500\n",
      "120/120 [==============================] - 0s 114us/step - loss: 0.0876 - accuracy: 0.9583 - val_loss: 0.0612 - val_accuracy: 1.0000\n",
      "Epoch 569/2500\n",
      "120/120 [==============================] - 0s 99us/step - loss: 0.0874 - accuracy: 0.9583 - val_loss: 0.0623 - val_accuracy: 1.0000\n",
      "Epoch 570/2500\n",
      "120/120 [==============================] - 0s 102us/step - loss: 0.0894 - accuracy: 0.9583 - val_loss: 0.0595 - val_accuracy: 1.0000\n",
      "Epoch 571/2500\n",
      "120/120 [==============================] - 0s 110us/step - loss: 0.0871 - accuracy: 0.9583 - val_loss: 0.0626 - val_accuracy: 1.0000\n",
      "Epoch 572/2500\n",
      "120/120 [==============================] - 0s 104us/step - loss: 0.0876 - accuracy: 0.9583 - val_loss: 0.0682 - val_accuracy: 0.9667\n",
      "Epoch 573/2500\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.0884 - accuracy: 0.9583 - val_loss: 0.0634 - val_accuracy: 1.0000\n",
      "Epoch 574/2500\n",
      "120/120 [==============================] - 0s 109us/step - loss: 0.0870 - accuracy: 0.9583 - val_loss: 0.0652 - val_accuracy: 1.0000\n",
      "Epoch 575/2500\n",
      "120/120 [==============================] - 0s 107us/step - loss: 0.0877 - accuracy: 0.9583 - val_loss: 0.0621 - val_accuracy: 1.0000\n",
      "Epoch 576/2500\n",
      "120/120 [==============================] - 0s 98us/step - loss: 0.0866 - accuracy: 0.9583 - val_loss: 0.0627 - val_accuracy: 1.0000\n",
      "Epoch 577/2500\n",
      "120/120 [==============================] - 0s 103us/step - loss: 0.0873 - accuracy: 0.9583 - val_loss: 0.0666 - val_accuracy: 0.9667\n",
      "Epoch 578/2500\n",
      "120/120 [==============================] - 0s 104us/step - loss: 0.0870 - accuracy: 0.9583 - val_loss: 0.0657 - val_accuracy: 0.9667\n",
      "Epoch 579/2500\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.0868 - accuracy: 0.9583 - val_loss: 0.0629 - val_accuracy: 1.0000\n",
      "Epoch 580/2500\n",
      "120/120 [==============================] - 0s 107us/step - loss: 0.0865 - accuracy: 0.9583 - val_loss: 0.0641 - val_accuracy: 1.0000\n",
      "Epoch 581/2500\n",
      "120/120 [==============================] - 0s 105us/step - loss: 0.0894 - accuracy: 0.9583 - val_loss: 0.0690 - val_accuracy: 0.9667\n",
      "Epoch 582/2500\n",
      "120/120 [==============================] - 0s 96us/step - loss: 0.0872 - accuracy: 0.9583 - val_loss: 0.0626 - val_accuracy: 1.0000\n",
      "Epoch 583/2500\n",
      "120/120 [==============================] - 0s 99us/step - loss: 0.0864 - accuracy: 0.9583 - val_loss: 0.0606 - val_accuracy: 1.0000\n",
      "Epoch 584/2500\n",
      "120/120 [==============================] - 0s 104us/step - loss: 0.0866 - accuracy: 0.9583 - val_loss: 0.0607 - val_accuracy: 1.0000\n",
      "Epoch 585/2500\n",
      "120/120 [==============================] - 0s 105us/step - loss: 0.0866 - accuracy: 0.9583 - val_loss: 0.0646 - val_accuracy: 1.0000\n",
      "Epoch 586/2500\n",
      "120/120 [==============================] - 0s 98us/step - loss: 0.0859 - accuracy: 0.9583 - val_loss: 0.0641 - val_accuracy: 1.0000\n",
      "Epoch 587/2500\n",
      "120/120 [==============================] - 0s 98us/step - loss: 0.0860 - accuracy: 0.9583 - val_loss: 0.0649 - val_accuracy: 0.9667\n",
      "Epoch 588/2500\n",
      "120/120 [==============================] - 0s 105us/step - loss: 0.0863 - accuracy: 0.9583 - val_loss: 0.0618 - val_accuracy: 1.0000\n",
      "Epoch 589/2500\n",
      "120/120 [==============================] - 0s 99us/step - loss: 0.0869 - accuracy: 0.9583 - val_loss: 0.0646 - val_accuracy: 1.0000\n",
      "Epoch 590/2500\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.0873 - accuracy: 0.9583 - val_loss: 0.0649 - val_accuracy: 0.9667\n",
      "Epoch 591/2500\n",
      "120/120 [==============================] - 0s 102us/step - loss: 0.0862 - accuracy: 0.9583 - val_loss: 0.0595 - val_accuracy: 1.0000\n",
      "Epoch 592/2500\n",
      "120/120 [==============================] - 0s 99us/step - loss: 0.0859 - accuracy: 0.9583 - val_loss: 0.0599 - val_accuracy: 1.0000\n",
      "Epoch 593/2500\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.0867 - accuracy: 0.9583 - val_loss: 0.0633 - val_accuracy: 1.0000\n",
      "Epoch 594/2500\n",
      "120/120 [==============================] - 0s 102us/step - loss: 0.0868 - accuracy: 0.9583 - val_loss: 0.0597 - val_accuracy: 1.0000\n",
      "Epoch 595/2500\n",
      "120/120 [==============================] - 0s 107us/step - loss: 0.0858 - accuracy: 0.9583 - val_loss: 0.0610 - val_accuracy: 1.0000\n",
      "Epoch 596/2500\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.0859 - accuracy: 0.9583 - val_loss: 0.0639 - val_accuracy: 1.0000\n",
      "Epoch 597/2500\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.0853 - accuracy: 0.9583 - val_loss: 0.0628 - val_accuracy: 1.0000\n",
      "Epoch 598/2500\n",
      "120/120 [==============================] - 0s 100us/step - loss: 0.0851 - accuracy: 0.9583 - val_loss: 0.0635 - val_accuracy: 1.0000\n",
      "Epoch 599/2500\n",
      "120/120 [==============================] - 0s 109us/step - loss: 0.0892 - accuracy: 0.9583 - val_loss: 0.0685 - val_accuracy: 0.9667\n",
      "Epoch 600/2500\n",
      "120/120 [==============================] - 0s 98us/step - loss: 0.0861 - accuracy: 0.9583 - val_loss: 0.0605 - val_accuracy: 1.0000\n",
      "Epoch 601/2500\n",
      "120/120 [==============================] - 0s 101us/step - loss: 0.0865 - accuracy: 0.9583 - val_loss: 0.0575 - val_accuracy: 1.0000\n",
      "Epoch 602/2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 111us/step - loss: 0.0854 - accuracy: 0.9583 - val_loss: 0.0599 - val_accuracy: 1.0000\n",
      "Epoch 603/2500\n",
      "120/120 [==============================] - 0s 105us/step - loss: 0.0844 - accuracy: 0.9583 - val_loss: 0.0611 - val_accuracy: 1.0000\n",
      "Epoch 604/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.0854 - accuracy: 0.9583 - val_loss: 0.0656 - val_accuracy: 0.9667\n",
      "Epoch 605/2500\n",
      "120/120 [==============================] - 0s 95us/step - loss: 0.0878 - accuracy: 0.9583 - val_loss: 0.0676 - val_accuracy: 0.9667\n",
      "Epoch 606/2500\n",
      "120/120 [==============================] - 0s 98us/step - loss: 0.0845 - accuracy: 0.9583 - val_loss: 0.0636 - val_accuracy: 0.9667\n",
      "Epoch 607/2500\n",
      "120/120 [==============================] - 0s 104us/step - loss: 0.0846 - accuracy: 0.9583 - val_loss: 0.0591 - val_accuracy: 1.0000\n",
      "Epoch 608/2500\n",
      "120/120 [==============================] - 0s 101us/step - loss: 0.0853 - accuracy: 0.9583 - val_loss: 0.0593 - val_accuracy: 1.0000\n",
      "Epoch 609/2500\n",
      "120/120 [==============================] - 0s 104us/step - loss: 0.0844 - accuracy: 0.9583 - val_loss: 0.0596 - val_accuracy: 1.0000\n",
      "Epoch 610/2500\n",
      "120/120 [==============================] - 0s 95us/step - loss: 0.0848 - accuracy: 0.9583 - val_loss: 0.0640 - val_accuracy: 0.9667\n",
      "Epoch 611/2500\n",
      "120/120 [==============================] - 0s 101us/step - loss: 0.0847 - accuracy: 0.9583 - val_loss: 0.0648 - val_accuracy: 0.9667\n",
      "Epoch 612/2500\n",
      "120/120 [==============================] - 0s 104us/step - loss: 0.0847 - accuracy: 0.9583 - val_loss: 0.0647 - val_accuracy: 0.9667\n",
      "Epoch 613/2500\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0850 - accuracy: 0.9583 - val_loss: 0.0586 - val_accuracy: 1.0000\n",
      "Epoch 614/2500\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0846 - accuracy: 0.9583 - val_loss: 0.0599 - val_accuracy: 1.0000\n",
      "Epoch 615/2500\n",
      "120/120 [==============================] - 0s 103us/step - loss: 0.0849 - accuracy: 0.9583 - val_loss: 0.0581 - val_accuracy: 1.0000\n",
      "Epoch 616/2500\n",
      "120/120 [==============================] - 0s 93us/step - loss: 0.0846 - accuracy: 0.9583 - val_loss: 0.0617 - val_accuracy: 1.0000\n",
      "Epoch 617/2500\n",
      "120/120 [==============================] - 0s 99us/step - loss: 0.0844 - accuracy: 0.9583 - val_loss: 0.0644 - val_accuracy: 0.9667\n",
      "Epoch 618/2500\n",
      "120/120 [==============================] - 0s 107us/step - loss: 0.0845 - accuracy: 0.9583 - val_loss: 0.0602 - val_accuracy: 1.0000\n",
      "Epoch 619/2500\n",
      "120/120 [==============================] - 0s 107us/step - loss: 0.0836 - accuracy: 0.9583 - val_loss: 0.0604 - val_accuracy: 1.0000\n",
      "Epoch 620/2500\n",
      "120/120 [==============================] - 0s 101us/step - loss: 0.0862 - accuracy: 0.9583 - val_loss: 0.0645 - val_accuracy: 0.9667\n",
      "Epoch 621/2500\n",
      "120/120 [==============================] - 0s 134us/step - loss: 0.0849 - accuracy: 0.9583 - val_loss: 0.0585 - val_accuracy: 1.0000\n",
      "Epoch 622/2500\n",
      "120/120 [==============================] - 0s 105us/step - loss: 0.0842 - accuracy: 0.9583 - val_loss: 0.0577 - val_accuracy: 1.0000\n",
      "Epoch 623/2500\n",
      "120/120 [==============================] - 0s 98us/step - loss: 0.0833 - accuracy: 0.9583 - val_loss: 0.0606 - val_accuracy: 1.0000\n",
      "Epoch 624/2500\n",
      "120/120 [==============================] - 0s 114us/step - loss: 0.0833 - accuracy: 0.9583 - val_loss: 0.0634 - val_accuracy: 0.9667\n",
      "Epoch 625/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.0863 - accuracy: 0.9583 - val_loss: 0.0674 - val_accuracy: 0.9667\n",
      "Epoch 626/2500\n",
      "120/120 [==============================] - 0s 116us/step - loss: 0.0837 - accuracy: 0.9583 - val_loss: 0.0605 - val_accuracy: 1.0000\n",
      "Epoch 627/2500\n",
      "120/120 [==============================] - 0s 119us/step - loss: 0.0835 - accuracy: 0.9583 - val_loss: 0.0581 - val_accuracy: 1.0000\n",
      "Epoch 628/2500\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0831 - accuracy: 0.9583 - val_loss: 0.0587 - val_accuracy: 1.0000\n",
      "Epoch 629/2500\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0835 - accuracy: 0.9583 - val_loss: 0.0603 - val_accuracy: 1.0000\n",
      "Epoch 630/2500\n",
      "120/120 [==============================] - 0s 112us/step - loss: 0.0847 - accuracy: 0.9583 - val_loss: 0.0583 - val_accuracy: 1.0000\n",
      "Epoch 631/2500\n",
      "120/120 [==============================] - 0s 109us/step - loss: 0.0830 - accuracy: 0.9583 - val_loss: 0.0627 - val_accuracy: 0.9667\n",
      "Epoch 632/2500\n",
      "120/120 [==============================] - 0s 105us/step - loss: 0.0832 - accuracy: 0.9583 - val_loss: 0.0625 - val_accuracy: 0.9667\n",
      "Epoch 633/2500\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0833 - accuracy: 0.9583 - val_loss: 0.0651 - val_accuracy: 0.9667\n",
      "Epoch 634/2500\n",
      "120/120 [==============================] - 0s 118us/step - loss: 0.0848 - accuracy: 0.9583 - val_loss: 0.0594 - val_accuracy: 1.0000\n",
      "Epoch 635/2500\n",
      "120/120 [==============================] - 0s 105us/step - loss: 0.0825 - accuracy: 0.9583 - val_loss: 0.0613 - val_accuracy: 0.9667\n",
      "Epoch 636/2500\n",
      "120/120 [==============================] - 0s 100us/step - loss: 0.0833 - accuracy: 0.9583 - val_loss: 0.0623 - val_accuracy: 0.9667\n",
      "Epoch 637/2500\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0827 - accuracy: 0.9583 - val_loss: 0.0617 - val_accuracy: 0.9667\n",
      "Epoch 638/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.0832 - accuracy: 0.9583 - val_loss: 0.0625 - val_accuracy: 0.9667\n",
      "Epoch 639/2500\n",
      "120/120 [==============================] - 0s 89us/step - loss: 0.0824 - accuracy: 0.9583 - val_loss: 0.0614 - val_accuracy: 0.9667\n",
      "Epoch 640/2500\n",
      "120/120 [==============================] - 0s 98us/step - loss: 0.0826 - accuracy: 0.9583 - val_loss: 0.0616 - val_accuracy: 0.9667\n",
      "Epoch 641/2500\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0829 - accuracy: 0.9583 - val_loss: 0.0579 - val_accuracy: 1.0000\n",
      "Epoch 642/2500\n",
      "120/120 [==============================] - 0s 112us/step - loss: 0.0828 - accuracy: 0.9583 - val_loss: 0.0604 - val_accuracy: 1.0000\n",
      "Epoch 643/2500\n",
      "120/120 [==============================] - 0s 105us/step - loss: 0.0832 - accuracy: 0.9583 - val_loss: 0.0617 - val_accuracy: 0.9667\n",
      "Epoch 644/2500\n",
      "120/120 [==============================] - 0s 109us/step - loss: 0.0822 - accuracy: 0.9583 - val_loss: 0.0586 - val_accuracy: 1.0000\n",
      "Epoch 645/2500\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.0821 - accuracy: 0.9583 - val_loss: 0.0577 - val_accuracy: 1.0000\n",
      "Epoch 646/2500\n",
      "120/120 [==============================] - 0s 110us/step - loss: 0.0828 - accuracy: 0.9583 - val_loss: 0.0568 - val_accuracy: 1.0000\n",
      "Epoch 647/2500\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0822 - accuracy: 0.9583 - val_loss: 0.0586 - val_accuracy: 1.0000\n",
      "Epoch 648/2500\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0819 - accuracy: 0.9583 - val_loss: 0.0590 - val_accuracy: 1.0000\n",
      "Epoch 649/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.0822 - accuracy: 0.9583 - val_loss: 0.0650 - val_accuracy: 0.9667\n",
      "Epoch 650/2500\n",
      "120/120 [==============================] - 0s 107us/step - loss: 0.0830 - accuracy: 0.9583 - val_loss: 0.0598 - val_accuracy: 1.0000\n",
      "Epoch 651/2500\n",
      "120/120 [==============================] - 0s 95us/step - loss: 0.0820 - accuracy: 0.9583 - val_loss: 0.0607 - val_accuracy: 0.9667\n",
      "Epoch 652/2500\n",
      "120/120 [==============================] - 0s 105us/step - loss: 0.0817 - accuracy: 0.9583 - val_loss: 0.0614 - val_accuracy: 0.9667\n",
      "Epoch 653/2500\n",
      "120/120 [==============================] - 0s 105us/step - loss: 0.0817 - accuracy: 0.9583 - val_loss: 0.0605 - val_accuracy: 0.9667\n",
      "Epoch 654/2500\n",
      "120/120 [==============================] - 0s 104us/step - loss: 0.0813 - accuracy: 0.9583 - val_loss: 0.0593 - val_accuracy: 1.0000\n",
      "Epoch 655/2500\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0822 - accuracy: 0.9583 - val_loss: 0.0571 - val_accuracy: 1.0000\n",
      "Epoch 656/2500\n",
      "120/120 [==============================] - 0s 104us/step - loss: 0.0832 - accuracy: 0.9583 - val_loss: 0.0580 - val_accuracy: 1.0000\n",
      "Epoch 657/2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 95us/step - loss: 0.0825 - accuracy: 0.9583 - val_loss: 0.0650 - val_accuracy: 0.9667\n",
      "Epoch 658/2500\n",
      "120/120 [==============================] - 0s 101us/step - loss: 0.0813 - accuracy: 0.9583 - val_loss: 0.0638 - val_accuracy: 0.9667\n",
      "Epoch 659/2500\n",
      "120/120 [==============================] - 0s 104us/step - loss: 0.0816 - accuracy: 0.9583 - val_loss: 0.0637 - val_accuracy: 0.9667\n",
      "Epoch 660/2500\n",
      "120/120 [==============================] - 0s 101us/step - loss: 0.0807 - accuracy: 0.9583 - val_loss: 0.0583 - val_accuracy: 1.0000\n",
      "Epoch 661/2500\n",
      "120/120 [==============================] - 0s 102us/step - loss: 0.0810 - accuracy: 0.9583 - val_loss: 0.0565 - val_accuracy: 1.0000\n",
      "Epoch 662/2500\n",
      "120/120 [==============================] - 0s 100us/step - loss: 0.0826 - accuracy: 0.9583 - val_loss: 0.0541 - val_accuracy: 1.0000\n",
      "Epoch 663/2500\n",
      "120/120 [==============================] - 0s 92us/step - loss: 0.0834 - accuracy: 0.9583 - val_loss: 0.0621 - val_accuracy: 0.9667\n",
      "Epoch 664/2500\n",
      "120/120 [==============================] - 0s 100us/step - loss: 0.0819 - accuracy: 0.9583 - val_loss: 0.0650 - val_accuracy: 0.9667\n",
      "Epoch 665/2500\n",
      "120/120 [==============================] - 0s 105us/step - loss: 0.0811 - accuracy: 0.9583 - val_loss: 0.0606 - val_accuracy: 0.9667\n",
      "Epoch 666/2500\n",
      "120/120 [==============================] - 0s 109us/step - loss: 0.0806 - accuracy: 0.9583 - val_loss: 0.0571 - val_accuracy: 1.0000\n",
      "Epoch 667/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.0807 - accuracy: 0.9583 - val_loss: 0.0569 - val_accuracy: 1.0000\n",
      "Epoch 668/2500\n",
      "120/120 [==============================] - 0s 94us/step - loss: 0.0809 - accuracy: 0.9583 - val_loss: 0.0577 - val_accuracy: 1.0000\n",
      "Epoch 669/2500\n",
      "120/120 [==============================] - 0s 98us/step - loss: 0.0808 - accuracy: 0.9583 - val_loss: 0.0584 - val_accuracy: 1.0000\n",
      "Epoch 670/2500\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.0826 - accuracy: 0.9583 - val_loss: 0.0549 - val_accuracy: 1.0000\n",
      "Epoch 671/2500\n",
      "120/120 [==============================] - 0s 104us/step - loss: 0.0805 - accuracy: 0.9583 - val_loss: 0.0603 - val_accuracy: 0.9667\n",
      "Epoch 672/2500\n",
      "120/120 [==============================] - 0s 104us/step - loss: 0.0815 - accuracy: 0.9583 - val_loss: 0.0651 - val_accuracy: 0.9667\n",
      "Epoch 673/2500\n",
      "120/120 [==============================] - 0s 93us/step - loss: 0.0807 - accuracy: 0.9583 - val_loss: 0.0594 - val_accuracy: 0.9667\n",
      "Epoch 674/2500\n",
      "120/120 [==============================] - 0s 141us/step - loss: 0.0812 - accuracy: 0.9583 - val_loss: 0.0561 - val_accuracy: 1.0000\n",
      "Epoch 675/2500\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0802 - accuracy: 0.9583 - val_loss: 0.0592 - val_accuracy: 0.9667\n",
      "Epoch 676/2500\n",
      "120/120 [==============================] - 0s 109us/step - loss: 0.0805 - accuracy: 0.9583 - val_loss: 0.0617 - val_accuracy: 0.9667\n",
      "Epoch 677/2500\n",
      "120/120 [==============================] - 0s 143us/step - loss: 0.0800 - accuracy: 0.9583 - val_loss: 0.0616 - val_accuracy: 0.9667\n",
      "Epoch 678/2500\n",
      "120/120 [==============================] - 0s 107us/step - loss: 0.0798 - accuracy: 0.9583 - val_loss: 0.0605 - val_accuracy: 0.9667\n",
      "Epoch 679/2500\n",
      "120/120 [==============================] - 0s 107us/step - loss: 0.0800 - accuracy: 0.9583 - val_loss: 0.0585 - val_accuracy: 0.9667\n",
      "Epoch 680/2500\n",
      "120/120 [==============================] - 0s 128us/step - loss: 0.0812 - accuracy: 0.9583 - val_loss: 0.0610 - val_accuracy: 0.9667\n",
      "Epoch 681/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.0811 - accuracy: 0.9583 - val_loss: 0.0559 - val_accuracy: 1.0000\n",
      "Epoch 682/2500\n",
      "120/120 [==============================] - 0s 104us/step - loss: 0.0808 - accuracy: 0.9583 - val_loss: 0.0544 - val_accuracy: 1.0000\n",
      "Epoch 683/2500\n",
      "120/120 [==============================] - 0s 133us/step - loss: 0.0815 - accuracy: 0.9583 - val_loss: 0.0613 - val_accuracy: 0.9667\n",
      "Epoch 684/2500\n",
      "120/120 [==============================] - 0s 131us/step - loss: 0.0802 - accuracy: 0.9583 - val_loss: 0.0585 - val_accuracy: 0.9667\n",
      "Epoch 685/2500\n",
      "120/120 [==============================] - 0s 113us/step - loss: 0.0797 - accuracy: 0.9583 - val_loss: 0.0604 - val_accuracy: 0.9667\n",
      "Epoch 686/2500\n",
      "120/120 [==============================] - 0s 132us/step - loss: 0.0800 - accuracy: 0.9583 - val_loss: 0.0589 - val_accuracy: 0.9667\n",
      "Epoch 687/2500\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0813 - accuracy: 0.9583 - val_loss: 0.0555 - val_accuracy: 1.0000\n",
      "Epoch 688/2500\n",
      "120/120 [==============================] - 0s 102us/step - loss: 0.0803 - accuracy: 0.9583 - val_loss: 0.0602 - val_accuracy: 0.9667\n",
      "Epoch 689/2500\n",
      "120/120 [==============================] - 0s 105us/step - loss: 0.0791 - accuracy: 0.9583 - val_loss: 0.0603 - val_accuracy: 0.9667\n",
      "Epoch 690/2500\n",
      "120/120 [==============================] - 0s 136us/step - loss: 0.0811 - accuracy: 0.9583 - val_loss: 0.0564 - val_accuracy: 1.0000\n",
      "Epoch 691/2500\n",
      "120/120 [==============================] - 0s 102us/step - loss: 0.0790 - accuracy: 0.9583 - val_loss: 0.0587 - val_accuracy: 0.9667\n",
      "Epoch 692/2500\n",
      "120/120 [==============================] - 0s 107us/step - loss: 0.0794 - accuracy: 0.9583 - val_loss: 0.0579 - val_accuracy: 0.9667\n",
      "Epoch 693/2500\n",
      "120/120 [==============================] - 0s 133us/step - loss: 0.0795 - accuracy: 0.9583 - val_loss: 0.0571 - val_accuracy: 1.0000\n",
      "Epoch 694/2500\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.0787 - accuracy: 0.9583 - val_loss: 0.0580 - val_accuracy: 0.9667\n",
      "Epoch 695/2500\n",
      "120/120 [==============================] - 0s 105us/step - loss: 0.0817 - accuracy: 0.9583 - val_loss: 0.0660 - val_accuracy: 0.9667\n",
      "Epoch 696/2500\n",
      "120/120 [==============================] - 0s 130us/step - loss: 0.0788 - accuracy: 0.9583 - val_loss: 0.0604 - val_accuracy: 0.9667\n",
      "Epoch 697/2500\n",
      "120/120 [==============================] - 0s 121us/step - loss: 0.0790 - accuracy: 0.9583 - val_loss: 0.0595 - val_accuracy: 0.9667\n",
      "Epoch 698/2500\n",
      "120/120 [==============================] - 0s 110us/step - loss: 0.0798 - accuracy: 0.9583 - val_loss: 0.0544 - val_accuracy: 1.0000\n",
      "Epoch 699/2500\n",
      "120/120 [==============================] - 0s 139us/step - loss: 0.0790 - accuracy: 0.9583 - val_loss: 0.0567 - val_accuracy: 1.0000\n",
      "Epoch 700/2500\n",
      "120/120 [==============================] - 0s 118us/step - loss: 0.0785 - accuracy: 0.9583 - val_loss: 0.0581 - val_accuracy: 0.9667\n",
      "Epoch 701/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.0800 - accuracy: 0.9583 - val_loss: 0.0614 - val_accuracy: 0.9667\n",
      "Epoch 702/2500\n",
      "120/120 [==============================] - 0s 118us/step - loss: 0.0788 - accuracy: 0.9583 - val_loss: 0.0570 - val_accuracy: 0.9667\n",
      "Epoch 703/2500\n",
      "120/120 [==============================] - 0s 120us/step - loss: 0.0787 - accuracy: 0.9583 - val_loss: 0.0569 - val_accuracy: 1.0000\n",
      "Epoch 704/2500\n",
      "120/120 [==============================] - 0s 131us/step - loss: 0.0784 - accuracy: 0.9583 - val_loss: 0.0578 - val_accuracy: 0.9667\n",
      "Epoch 705/2500\n",
      "120/120 [==============================] - 0s 144us/step - loss: 0.0792 - accuracy: 0.9583 - val_loss: 0.0554 - val_accuracy: 1.0000\n",
      "Epoch 706/2500\n",
      "120/120 [==============================] - 0s 126us/step - loss: 0.0798 - accuracy: 0.9583 - val_loss: 0.0618 - val_accuracy: 0.9667\n",
      "Epoch 707/2500\n",
      "120/120 [==============================] - 0s 129us/step - loss: 0.0795 - accuracy: 0.9583 - val_loss: 0.0618 - val_accuracy: 0.9667\n",
      "Epoch 708/2500\n",
      "120/120 [==============================] - 0s 129us/step - loss: 0.0781 - accuracy: 0.9583 - val_loss: 0.0568 - val_accuracy: 0.9667\n",
      "Epoch 709/2500\n",
      "120/120 [==============================] - 0s 124us/step - loss: 0.0784 - accuracy: 0.9583 - val_loss: 0.0551 - val_accuracy: 1.0000\n",
      "Epoch 710/2500\n",
      "120/120 [==============================] - 0s 118us/step - loss: 0.0796 - accuracy: 0.9583 - val_loss: 0.0542 - val_accuracy: 1.0000\n",
      "Epoch 711/2500\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.0786 - accuracy: 0.9583 - val_loss: 0.0589 - val_accuracy: 0.9667\n",
      "Epoch 712/2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 116us/step - loss: 0.0779 - accuracy: 0.9583 - val_loss: 0.0611 - val_accuracy: 0.9667\n",
      "Epoch 713/2500\n",
      "120/120 [==============================] - 0s 107us/step - loss: 0.0795 - accuracy: 0.9583 - val_loss: 0.0562 - val_accuracy: 1.0000\n",
      "Epoch 714/2500\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0778 - accuracy: 0.9583 - val_loss: 0.0591 - val_accuracy: 0.9667\n",
      "Epoch 715/2500\n",
      "120/120 [==============================] - 0s 127us/step - loss: 0.0776 - accuracy: 0.9583 - val_loss: 0.0594 - val_accuracy: 0.9667\n",
      "Epoch 716/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.0785 - accuracy: 0.9583 - val_loss: 0.0576 - val_accuracy: 0.9667\n",
      "Epoch 717/2500\n",
      "120/120 [==============================] - 0s 100us/step - loss: 0.0776 - accuracy: 0.9583 - val_loss: 0.0577 - val_accuracy: 0.9667\n",
      "Epoch 718/2500\n",
      "120/120 [==============================] - 0s 107us/step - loss: 0.0776 - accuracy: 0.9583 - val_loss: 0.0591 - val_accuracy: 0.9667\n",
      "Epoch 719/2500\n",
      "120/120 [==============================] - 0s 105us/step - loss: 0.0801 - accuracy: 0.9583 - val_loss: 0.0555 - val_accuracy: 1.0000\n",
      "Epoch 720/2500\n",
      "120/120 [==============================] - 0s 105us/step - loss: 0.0787 - accuracy: 0.9583 - val_loss: 0.0610 - val_accuracy: 0.9667\n",
      "Epoch 721/2500\n",
      "120/120 [==============================] - 0s 116us/step - loss: 0.0780 - accuracy: 0.9583 - val_loss: 0.0571 - val_accuracy: 0.9667\n",
      "Epoch 722/2500\n",
      "120/120 [==============================] - 0s 113us/step - loss: 0.0779 - accuracy: 0.9583 - val_loss: 0.0606 - val_accuracy: 0.9667\n",
      "Epoch 723/2500\n",
      "120/120 [==============================] - 0s 102us/step - loss: 0.0776 - accuracy: 0.9583 - val_loss: 0.0592 - val_accuracy: 0.9667\n",
      "Epoch 724/2500\n",
      "120/120 [==============================] - 0s 92us/step - loss: 0.0796 - accuracy: 0.9583 - val_loss: 0.0543 - val_accuracy: 1.0000\n",
      "Epoch 725/2500\n",
      "120/120 [==============================] - 0s 114us/step - loss: 0.0771 - accuracy: 0.9583 - val_loss: 0.0559 - val_accuracy: 0.9667\n",
      "Epoch 726/2500\n",
      "120/120 [==============================] - 0s 107us/step - loss: 0.0779 - accuracy: 0.9583 - val_loss: 0.0603 - val_accuracy: 0.9667\n",
      "Epoch 727/2500\n",
      "120/120 [==============================] - 0s 107us/step - loss: 0.0774 - accuracy: 0.9583 - val_loss: 0.0575 - val_accuracy: 0.9667\n",
      "Epoch 728/2500\n",
      "120/120 [==============================] - 0s 112us/step - loss: 0.0773 - accuracy: 0.9583 - val_loss: 0.0570 - val_accuracy: 0.9667\n",
      "Epoch 729/2500\n",
      "120/120 [==============================] - 0s 118us/step - loss: 0.0770 - accuracy: 0.9583 - val_loss: 0.0568 - val_accuracy: 0.9667\n",
      "Epoch 730/2500\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0771 - accuracy: 0.9583 - val_loss: 0.0569 - val_accuracy: 0.9667\n",
      "Epoch 731/2500\n",
      "120/120 [==============================] - 0s 137us/step - loss: 0.0770 - accuracy: 0.9583 - val_loss: 0.0595 - val_accuracy: 0.9667\n",
      "Epoch 732/2500\n",
      "120/120 [==============================] - 0s 136us/step - loss: 0.0778 - accuracy: 0.9583 - val_loss: 0.0597 - val_accuracy: 0.9667\n",
      "Epoch 733/2500\n",
      "120/120 [==============================] - 0s 119us/step - loss: 0.0774 - accuracy: 0.9583 - val_loss: 0.0561 - val_accuracy: 0.9667\n",
      "Epoch 734/2500\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.0764 - accuracy: 0.9583 - val_loss: 0.0577 - val_accuracy: 0.9667\n",
      "Epoch 735/2500\n",
      "120/120 [==============================] - 0s 145us/step - loss: 0.0785 - accuracy: 0.9583 - val_loss: 0.0631 - val_accuracy: 0.9667\n",
      "Epoch 736/2500\n",
      "120/120 [==============================] - 0s 115us/step - loss: 0.0769 - accuracy: 0.9583 - val_loss: 0.0571 - val_accuracy: 0.9667\n",
      "Epoch 737/2500\n",
      "120/120 [==============================] - 0s 114us/step - loss: 0.0766 - accuracy: 0.9583 - val_loss: 0.0563 - val_accuracy: 0.9667\n",
      "Epoch 738/2500\n",
      "120/120 [==============================] - 0s 123us/step - loss: 0.0767 - accuracy: 0.9583 - val_loss: 0.0552 - val_accuracy: 0.9667\n",
      "Epoch 739/2500\n",
      "120/120 [==============================] - 0s 107us/step - loss: 0.0767 - accuracy: 0.9583 - val_loss: 0.0559 - val_accuracy: 0.9667\n",
      "Epoch 740/2500\n",
      "120/120 [==============================] - 0s 110us/step - loss: 0.0806 - accuracy: 0.9583 - val_loss: 0.0511 - val_accuracy: 1.0000\n",
      "Epoch 741/2500\n",
      "120/120 [==============================] - 0s 120us/step - loss: 0.0760 - accuracy: 0.9583 - val_loss: 0.0567 - val_accuracy: 0.9667\n",
      "Epoch 742/2500\n",
      "120/120 [==============================] - 0s 103us/step - loss: 0.0761 - accuracy: 0.9583 - val_loss: 0.0631 - val_accuracy: 0.9667\n",
      "Epoch 743/2500\n",
      "120/120 [==============================] - 0s 100us/step - loss: 0.0772 - accuracy: 0.9583 - val_loss: 0.0632 - val_accuracy: 0.9667\n",
      "Epoch 744/2500\n",
      "120/120 [==============================] - 0s 119us/step - loss: 0.0763 - accuracy: 0.9583 - val_loss: 0.0589 - val_accuracy: 0.9667\n",
      "Epoch 745/2500\n",
      "120/120 [==============================] - 0s 114us/step - loss: 0.0761 - accuracy: 0.9583 - val_loss: 0.0584 - val_accuracy: 0.9667\n",
      "Epoch 746/2500\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.0767 - accuracy: 0.9583 - val_loss: 0.0580 - val_accuracy: 0.9667\n",
      "Epoch 747/2500\n",
      "120/120 [==============================] - 0s 110us/step - loss: 0.0761 - accuracy: 0.9583 - val_loss: 0.0553 - val_accuracy: 0.9667\n",
      "Epoch 748/2500\n",
      "120/120 [==============================] - 0s 113us/step - loss: 0.0765 - accuracy: 0.9583 - val_loss: 0.0537 - val_accuracy: 1.0000\n",
      "Epoch 749/2500\n",
      "120/120 [==============================] - 0s 97us/step - loss: 0.0761 - accuracy: 0.9583 - val_loss: 0.0548 - val_accuracy: 0.9667\n",
      "Epoch 750/2500\n",
      "120/120 [==============================] - 0s 96us/step - loss: 0.0773 - accuracy: 0.9583 - val_loss: 0.0597 - val_accuracy: 0.9667\n",
      "Epoch 751/2500\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0778 - accuracy: 0.9583 - val_loss: 0.0536 - val_accuracy: 1.0000\n",
      "Epoch 752/2500\n",
      "120/120 [==============================] - 0s 114us/step - loss: 0.0757 - accuracy: 0.9583 - val_loss: 0.0564 - val_accuracy: 0.9667\n",
      "Epoch 753/2500\n",
      "120/120 [==============================] - 0s 109us/step - loss: 0.0771 - accuracy: 0.9583 - val_loss: 0.0621 - val_accuracy: 0.9667\n",
      "Epoch 754/2500\n",
      "120/120 [==============================] - 0s 104us/step - loss: 0.0760 - accuracy: 0.9583 - val_loss: 0.0580 - val_accuracy: 0.9667\n",
      "Epoch 755/2500\n",
      "120/120 [==============================] - 0s 128us/step - loss: 0.0761 - accuracy: 0.9583 - val_loss: 0.0552 - val_accuracy: 0.9667\n",
      "Epoch 756/2500\n",
      "120/120 [==============================] - 0s 104us/step - loss: 0.0757 - accuracy: 0.9583 - val_loss: 0.0542 - val_accuracy: 0.9667\n",
      "Epoch 757/2500\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.0755 - accuracy: 0.9583 - val_loss: 0.0541 - val_accuracy: 0.9667\n",
      "Epoch 758/2500\n",
      "120/120 [==============================] - 0s 126us/step - loss: 0.0759 - accuracy: 0.9583 - val_loss: 0.0585 - val_accuracy: 0.9667\n",
      "Epoch 759/2500\n",
      "120/120 [==============================] - 0s 110us/step - loss: 0.0762 - accuracy: 0.9583 - val_loss: 0.0552 - val_accuracy: 0.9667\n",
      "Epoch 760/2500\n",
      "120/120 [==============================] - 0s 105us/step - loss: 0.0751 - accuracy: 0.9583 - val_loss: 0.0576 - val_accuracy: 0.9667\n",
      "Epoch 761/2500\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0760 - accuracy: 0.9583 - val_loss: 0.0572 - val_accuracy: 0.9667\n",
      "Epoch 762/2500\n",
      "120/120 [==============================] - 0s 115us/step - loss: 0.0755 - accuracy: 0.9583 - val_loss: 0.0559 - val_accuracy: 0.9667\n",
      "Epoch 763/2500\n",
      "120/120 [==============================] - 0s 112us/step - loss: 0.0764 - accuracy: 0.9583 - val_loss: 0.0611 - val_accuracy: 0.9667\n",
      "Epoch 764/2500\n",
      "120/120 [==============================] - 0s 127us/step - loss: 0.0763 - accuracy: 0.9583 - val_loss: 0.0606 - val_accuracy: 0.9667\n",
      "Epoch 765/2500\n",
      "120/120 [==============================] - 0s 133us/step - loss: 0.0755 - accuracy: 0.9583 - val_loss: 0.0541 - val_accuracy: 0.9667\n",
      "Epoch 766/2500\n",
      "120/120 [==============================] - 0s 121us/step - loss: 0.0755 - accuracy: 0.9583 - val_loss: 0.0522 - val_accuracy: 1.0000\n",
      "Epoch 767/2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 152us/step - loss: 0.0755 - accuracy: 0.9583 - val_loss: 0.0544 - val_accuracy: 0.9667\n",
      "Epoch 768/2500\n",
      "120/120 [==============================] - 0s 161us/step - loss: 0.0755 - accuracy: 0.9583 - val_loss: 0.0591 - val_accuracy: 0.9667\n",
      "Epoch 769/2500\n",
      "120/120 [==============================] - 0s 152us/step - loss: 0.0757 - accuracy: 0.9583 - val_loss: 0.0556 - val_accuracy: 0.9667\n",
      "Epoch 770/2500\n",
      "120/120 [==============================] - 0s 126us/step - loss: 0.0754 - accuracy: 0.9583 - val_loss: 0.0566 - val_accuracy: 0.9667\n",
      "Epoch 771/2500\n",
      "120/120 [==============================] - 0s 118us/step - loss: 0.0754 - accuracy: 0.9583 - val_loss: 0.0561 - val_accuracy: 0.9667\n",
      "Epoch 772/2500\n",
      "120/120 [==============================] - 0s 115us/step - loss: 0.0755 - accuracy: 0.9583 - val_loss: 0.0535 - val_accuracy: 0.9667\n",
      "Epoch 773/2500\n",
      "120/120 [==============================] - 0s 129us/step - loss: 0.0758 - accuracy: 0.9583 - val_loss: 0.0595 - val_accuracy: 0.9667\n",
      "Epoch 774/2500\n",
      "120/120 [==============================] - 0s 102us/step - loss: 0.0747 - accuracy: 0.9583 - val_loss: 0.0588 - val_accuracy: 0.9667\n",
      "Epoch 775/2500\n",
      "120/120 [==============================] - 0s 100us/step - loss: 0.0755 - accuracy: 0.9583 - val_loss: 0.0536 - val_accuracy: 0.9667\n",
      "Epoch 776/2500\n",
      "120/120 [==============================] - 0s 150us/step - loss: 0.0748 - accuracy: 0.9583 - val_loss: 0.0566 - val_accuracy: 0.9667\n",
      "Epoch 777/2500\n",
      "120/120 [==============================] - 0s 112us/step - loss: 0.0750 - accuracy: 0.9583 - val_loss: 0.0594 - val_accuracy: 0.9667\n",
      "Epoch 778/2500\n",
      "120/120 [==============================] - 0s 150us/step - loss: 0.0756 - accuracy: 0.9583 - val_loss: 0.0593 - val_accuracy: 0.9667\n",
      "Epoch 779/2500\n",
      "120/120 [==============================] - 0s 139us/step - loss: 0.0746 - accuracy: 0.9583 - val_loss: 0.0581 - val_accuracy: 0.9667\n",
      "Epoch 780/2500\n",
      "120/120 [==============================] - 0s 112us/step - loss: 0.0744 - accuracy: 0.9583 - val_loss: 0.0542 - val_accuracy: 0.9667\n",
      "Epoch 781/2500\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.0776 - accuracy: 0.9583 - val_loss: 0.0496 - val_accuracy: 1.0000\n",
      "Epoch 782/2500\n",
      "120/120 [==============================] - 0s 118us/step - loss: 0.0746 - accuracy: 0.9583 - val_loss: 0.0546 - val_accuracy: 0.9667\n",
      "Epoch 783/2500\n",
      "120/120 [==============================] - 0s 110us/step - loss: 0.0741 - accuracy: 0.9583 - val_loss: 0.0571 - val_accuracy: 0.9667\n",
      "Epoch 784/2500\n",
      "120/120 [==============================] - 0s 105us/step - loss: 0.0756 - accuracy: 0.9583 - val_loss: 0.0615 - val_accuracy: 0.9667\n",
      "Epoch 785/2500\n",
      "120/120 [==============================] - 0s 120us/step - loss: 0.0747 - accuracy: 0.9583 - val_loss: 0.0614 - val_accuracy: 0.9667\n",
      "Epoch 786/2500\n",
      "120/120 [==============================] - 0s 115us/step - loss: 0.0745 - accuracy: 0.9583 - val_loss: 0.0552 - val_accuracy: 0.9667\n",
      "Epoch 787/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.0738 - accuracy: 0.9583 - val_loss: 0.0538 - val_accuracy: 0.9667\n",
      "Epoch 788/2500\n",
      "120/120 [==============================] - 0s 101us/step - loss: 0.0746 - accuracy: 0.9583 - val_loss: 0.0546 - val_accuracy: 0.9667\n",
      "Epoch 789/2500\n",
      "120/120 [==============================] - 0s 109us/step - loss: 0.0744 - accuracy: 0.9583 - val_loss: 0.0565 - val_accuracy: 0.9667\n",
      "Epoch 790/2500\n",
      "120/120 [==============================] - 0s 112us/step - loss: 0.0740 - accuracy: 0.9583 - val_loss: 0.0543 - val_accuracy: 0.9667\n",
      "Epoch 791/2500\n",
      "120/120 [==============================] - 0s 105us/step - loss: 0.0740 - accuracy: 0.9583 - val_loss: 0.0536 - val_accuracy: 0.9667\n",
      "Epoch 792/2500\n",
      "120/120 [==============================] - 0s 119us/step - loss: 0.0739 - accuracy: 0.9583 - val_loss: 0.0538 - val_accuracy: 0.9667\n",
      "Epoch 793/2500\n",
      "120/120 [==============================] - 0s 110us/step - loss: 0.0744 - accuracy: 0.9583 - val_loss: 0.0569 - val_accuracy: 0.9667\n",
      "Epoch 794/2500\n",
      "120/120 [==============================] - 0s 112us/step - loss: 0.0748 - accuracy: 0.9583 - val_loss: 0.0557 - val_accuracy: 0.9667\n",
      "Epoch 795/2500\n",
      "120/120 [==============================] - 0s 140us/step - loss: 0.0749 - accuracy: 0.9583 - val_loss: 0.0524 - val_accuracy: 0.9667\n",
      "Epoch 796/2500\n",
      "120/120 [==============================] - 0s 120us/step - loss: 0.0741 - accuracy: 0.9583 - val_loss: 0.0540 - val_accuracy: 0.9667\n",
      "Epoch 797/2500\n",
      "120/120 [==============================] - 0s 114us/step - loss: 0.0737 - accuracy: 0.9583 - val_loss: 0.0591 - val_accuracy: 0.9667\n",
      "Epoch 798/2500\n",
      "120/120 [==============================] - 0s 124us/step - loss: 0.0747 - accuracy: 0.9583 - val_loss: 0.0594 - val_accuracy: 0.9667\n",
      "Epoch 799/2500\n",
      "120/120 [==============================] - 0s 121us/step - loss: 0.0736 - accuracy: 0.9583 - val_loss: 0.0576 - val_accuracy: 0.9667\n",
      "Epoch 800/2500\n",
      "120/120 [==============================] - 0s 115us/step - loss: 0.0759 - accuracy: 0.9583 - val_loss: 0.0617 - val_accuracy: 0.9667\n",
      "Epoch 801/2500\n",
      "120/120 [==============================] - 0s 119us/step - loss: 0.0735 - accuracy: 0.9583 - val_loss: 0.0579 - val_accuracy: 0.9667\n",
      "Epoch 802/2500\n",
      "120/120 [==============================] - 0s 113us/step - loss: 0.0796 - accuracy: 0.9583 - val_loss: 0.0481 - val_accuracy: 1.0000\n",
      "Epoch 803/2500\n",
      "120/120 [==============================] - 0s 105us/step - loss: 0.0750 - accuracy: 0.9667 - val_loss: 0.0502 - val_accuracy: 1.0000\n",
      "Epoch 804/2500\n",
      "120/120 [==============================] - 0s 123us/step - loss: 0.0737 - accuracy: 0.9583 - val_loss: 0.0579 - val_accuracy: 0.9667\n",
      "Epoch 805/2500\n",
      "120/120 [==============================] - 0s 124us/step - loss: 0.0730 - accuracy: 0.9583 - val_loss: 0.0604 - val_accuracy: 0.9667\n",
      "Epoch 806/2500\n",
      "120/120 [==============================] - 0s 124us/step - loss: 0.0743 - accuracy: 0.9583 - val_loss: 0.0560 - val_accuracy: 0.9667\n",
      "Epoch 807/2500\n",
      "120/120 [==============================] - 0s 128us/step - loss: 0.0733 - accuracy: 0.9583 - val_loss: 0.0550 - val_accuracy: 0.9667\n",
      "Epoch 808/2500\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.0736 - accuracy: 0.9583 - val_loss: 0.0542 - val_accuracy: 0.9667\n",
      "Epoch 809/2500\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0754 - accuracy: 0.9583 - val_loss: 0.0610 - val_accuracy: 0.9667\n",
      "Epoch 810/2500\n",
      "120/120 [==============================] - 0s 124us/step - loss: 0.0748 - accuracy: 0.9667 - val_loss: 0.0641 - val_accuracy: 0.9667\n",
      "Epoch 811/2500\n",
      "120/120 [==============================] - 0s 126us/step - loss: 0.0742 - accuracy: 0.9583 - val_loss: 0.0533 - val_accuracy: 0.9667\n",
      "Epoch 812/2500\n",
      "120/120 [==============================] - 0s 124us/step - loss: 0.0728 - accuracy: 0.9583 - val_loss: 0.0539 - val_accuracy: 0.9667\n",
      "Epoch 813/2500\n",
      "120/120 [==============================] - 0s 122us/step - loss: 0.0729 - accuracy: 0.9583 - val_loss: 0.0550 - val_accuracy: 0.9667\n",
      "Epoch 814/2500\n",
      "120/120 [==============================] - 0s 120us/step - loss: 0.0730 - accuracy: 0.9583 - val_loss: 0.0544 - val_accuracy: 0.9667\n",
      "Epoch 815/2500\n",
      "120/120 [==============================] - 0s 120us/step - loss: 0.0755 - accuracy: 0.9583 - val_loss: 0.0604 - val_accuracy: 0.9667\n",
      "Epoch 816/2500\n",
      "120/120 [==============================] - 0s 126us/step - loss: 0.0731 - accuracy: 0.9583 - val_loss: 0.0536 - val_accuracy: 0.9667\n",
      "Epoch 817/2500\n",
      "120/120 [==============================] - 0s 133us/step - loss: 0.0729 - accuracy: 0.9583 - val_loss: 0.0544 - val_accuracy: 0.9667\n",
      "Epoch 818/2500\n",
      "120/120 [==============================] - 0s 120us/step - loss: 0.0743 - accuracy: 0.9583 - val_loss: 0.0579 - val_accuracy: 0.9667\n",
      "Epoch 819/2500\n",
      "120/120 [==============================] - 0s 114us/step - loss: 0.0727 - accuracy: 0.9583 - val_loss: 0.0555 - val_accuracy: 0.9667\n",
      "Epoch 820/2500\n",
      "120/120 [==============================] - 0s 116us/step - loss: 0.0740 - accuracy: 0.9583 - val_loss: 0.0491 - val_accuracy: 1.0000\n",
      "Epoch 821/2500\n",
      "120/120 [==============================] - 0s 105us/step - loss: 0.0743 - accuracy: 0.9583 - val_loss: 0.0532 - val_accuracy: 0.9667\n",
      "Epoch 822/2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 104us/step - loss: 0.0733 - accuracy: 0.9583 - val_loss: 0.0557 - val_accuracy: 0.9667\n",
      "Epoch 823/2500\n",
      "120/120 [==============================] - 0s 112us/step - loss: 0.0745 - accuracy: 0.9583 - val_loss: 0.0508 - val_accuracy: 1.0000\n",
      "Epoch 824/2500\n",
      "120/120 [==============================] - 0s 113us/step - loss: 0.0734 - accuracy: 0.9667 - val_loss: 0.0574 - val_accuracy: 0.9667\n",
      "Epoch 825/2500\n",
      "120/120 [==============================] - 0s 97us/step - loss: 0.0730 - accuracy: 0.9583 - val_loss: 0.0557 - val_accuracy: 0.9667\n",
      "Epoch 826/2500\n",
      "120/120 [==============================] - 0s 126us/step - loss: 0.0740 - accuracy: 0.9583 - val_loss: 0.0543 - val_accuracy: 0.9667\n",
      "Epoch 827/2500\n",
      "120/120 [==============================] - 0s 146us/step - loss: 0.0743 - accuracy: 0.9583 - val_loss: 0.0624 - val_accuracy: 0.9667\n",
      "Epoch 828/2500\n",
      "120/120 [==============================] - 0s 105us/step - loss: 0.0723 - accuracy: 0.9583 - val_loss: 0.0598 - val_accuracy: 0.9667\n",
      "Epoch 829/2500\n",
      "120/120 [==============================] - 0s 124us/step - loss: 0.0745 - accuracy: 0.9583 - val_loss: 0.0512 - val_accuracy: 0.9667\n",
      "Epoch 830/2500\n",
      "120/120 [==============================] - 0s 141us/step - loss: 0.0726 - accuracy: 0.9583 - val_loss: 0.0557 - val_accuracy: 0.9667\n",
      "Epoch 831/2500\n",
      "120/120 [==============================] - 0s 129us/step - loss: 0.0722 - accuracy: 0.9583 - val_loss: 0.0580 - val_accuracy: 0.9667\n",
      "Epoch 832/2500\n",
      "120/120 [==============================] - 0s 136us/step - loss: 0.0722 - accuracy: 0.9583 - val_loss: 0.0564 - val_accuracy: 0.9667\n",
      "Epoch 833/2500\n",
      "120/120 [==============================] - 0s 121us/step - loss: 0.0722 - accuracy: 0.9583 - val_loss: 0.0555 - val_accuracy: 0.9667\n",
      "Epoch 834/2500\n",
      "120/120 [==============================] - 0s 109us/step - loss: 0.0728 - accuracy: 0.9583 - val_loss: 0.0549 - val_accuracy: 0.9667\n",
      "Epoch 835/2500\n",
      "120/120 [==============================] - 0s 103us/step - loss: 0.0727 - accuracy: 0.9583 - val_loss: 0.0584 - val_accuracy: 0.9667\n",
      "Epoch 836/2500\n",
      "120/120 [==============================] - 0s 119us/step - loss: 0.0731 - accuracy: 0.9583 - val_loss: 0.0530 - val_accuracy: 0.9667\n",
      "Epoch 837/2500\n",
      "120/120 [==============================] - 0s 113us/step - loss: 0.0722 - accuracy: 0.9583 - val_loss: 0.0555 - val_accuracy: 0.9667\n",
      "Epoch 838/2500\n",
      "120/120 [==============================] - 0s 118us/step - loss: 0.0722 - accuracy: 0.9583 - val_loss: 0.0590 - val_accuracy: 0.9667\n",
      "Epoch 839/2500\n",
      "120/120 [==============================] - 0s 131us/step - loss: 0.0716 - accuracy: 0.9583 - val_loss: 0.0574 - val_accuracy: 0.9667\n",
      "Epoch 840/2500\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0736 - accuracy: 0.9583 - val_loss: 0.0511 - val_accuracy: 0.9667\n",
      "Epoch 841/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.0723 - accuracy: 0.9583 - val_loss: 0.0557 - val_accuracy: 0.9667\n",
      "Epoch 842/2500\n",
      "120/120 [==============================] - 0s 120us/step - loss: 0.0720 - accuracy: 0.9583 - val_loss: 0.0576 - val_accuracy: 0.9667\n",
      "Epoch 843/2500\n",
      "120/120 [==============================] - 0s 121us/step - loss: 0.0742 - accuracy: 0.9583 - val_loss: 0.0516 - val_accuracy: 0.9667\n",
      "Epoch 844/2500\n",
      "120/120 [==============================] - 0s 114us/step - loss: 0.0712 - accuracy: 0.9583 - val_loss: 0.0550 - val_accuracy: 0.9667\n",
      "Epoch 845/2500\n",
      "120/120 [==============================] - 0s 116us/step - loss: 0.0723 - accuracy: 0.9583 - val_loss: 0.0540 - val_accuracy: 0.9667\n",
      "Epoch 846/2500\n",
      "120/120 [==============================] - 0s 118us/step - loss: 0.0740 - accuracy: 0.9583 - val_loss: 0.0617 - val_accuracy: 0.9667\n",
      "Epoch 847/2500\n",
      "120/120 [==============================] - 0s 116us/step - loss: 0.0740 - accuracy: 0.9583 - val_loss: 0.0537 - val_accuracy: 0.9667\n",
      "Epoch 848/2500\n",
      "120/120 [==============================] - 0s 109us/step - loss: 0.0728 - accuracy: 0.9583 - val_loss: 0.0578 - val_accuracy: 0.9667\n",
      "Epoch 849/2500\n",
      "120/120 [==============================] - 0s 112us/step - loss: 0.0718 - accuracy: 0.9583 - val_loss: 0.0564 - val_accuracy: 0.9667\n",
      "Epoch 850/2500\n",
      "120/120 [==============================] - 0s 107us/step - loss: 0.0718 - accuracy: 0.9583 - val_loss: 0.0519 - val_accuracy: 0.9667\n",
      "Epoch 851/2500\n",
      "120/120 [==============================] - 0s 113us/step - loss: 0.0715 - accuracy: 0.9583 - val_loss: 0.0549 - val_accuracy: 0.9667\n",
      "Epoch 852/2500\n",
      "120/120 [==============================] - 0s 132us/step - loss: 0.0718 - accuracy: 0.9583 - val_loss: 0.0536 - val_accuracy: 0.9667\n",
      "Epoch 853/2500\n",
      "120/120 [==============================] - 0s 116us/step - loss: 0.0715 - accuracy: 0.9667 - val_loss: 0.0517 - val_accuracy: 0.9667\n",
      "Epoch 854/2500\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.0723 - accuracy: 0.9583 - val_loss: 0.0530 - val_accuracy: 0.9667\n",
      "Epoch 855/2500\n",
      "120/120 [==============================] - 0s 127us/step - loss: 0.0711 - accuracy: 0.9583 - val_loss: 0.0560 - val_accuracy: 0.9667\n",
      "Epoch 856/2500\n",
      "120/120 [==============================] - 0s 126us/step - loss: 0.0737 - accuracy: 0.9583 - val_loss: 0.0502 - val_accuracy: 0.9667\n",
      "Epoch 857/2500\n",
      "120/120 [==============================] - 0s 140us/step - loss: 0.0728 - accuracy: 0.9583 - val_loss: 0.0592 - val_accuracy: 0.9667\n",
      "Epoch 858/2500\n",
      "120/120 [==============================] - 0s 155us/step - loss: 0.0715 - accuracy: 0.9583 - val_loss: 0.0590 - val_accuracy: 0.9667\n",
      "Epoch 859/2500\n",
      "120/120 [==============================] - 0s 131us/step - loss: 0.0714 - accuracy: 0.9583 - val_loss: 0.0549 - val_accuracy: 0.9667\n",
      "Epoch 860/2500\n",
      "120/120 [==============================] - 0s 186us/step - loss: 0.0723 - accuracy: 0.9583 - val_loss: 0.0533 - val_accuracy: 0.9667\n",
      "Epoch 861/2500\n",
      "120/120 [==============================] - 0s 148us/step - loss: 0.0708 - accuracy: 0.9583 - val_loss: 0.0554 - val_accuracy: 0.9667\n",
      "Epoch 862/2500\n",
      "120/120 [==============================] - 0s 148us/step - loss: 0.0709 - accuracy: 0.9583 - val_loss: 0.0568 - val_accuracy: 0.9667\n",
      "Epoch 863/2500\n",
      "120/120 [==============================] - 0s 141us/step - loss: 0.0709 - accuracy: 0.9583 - val_loss: 0.0572 - val_accuracy: 0.9667\n",
      "Epoch 864/2500\n",
      "120/120 [==============================] - 0s 123us/step - loss: 0.0730 - accuracy: 0.9667 - val_loss: 0.0515 - val_accuracy: 0.9667\n",
      "Epoch 865/2500\n",
      "120/120 [==============================] - 0s 119us/step - loss: 0.0718 - accuracy: 0.9667 - val_loss: 0.0500 - val_accuracy: 0.9667\n",
      "Epoch 866/2500\n",
      "120/120 [==============================] - 0s 126us/step - loss: 0.0725 - accuracy: 0.9583 - val_loss: 0.0586 - val_accuracy: 0.9667\n",
      "Epoch 867/2500\n",
      "120/120 [==============================] - 0s 114us/step - loss: 0.0716 - accuracy: 0.9667 - val_loss: 0.0599 - val_accuracy: 0.9667\n",
      "Epoch 868/2500\n",
      "120/120 [==============================] - 0s 112us/step - loss: 0.0704 - accuracy: 0.9667 - val_loss: 0.0565 - val_accuracy: 0.9667\n",
      "Epoch 869/2500\n",
      "120/120 [==============================] - 0s 121us/step - loss: 0.0741 - accuracy: 0.9667 - val_loss: 0.0479 - val_accuracy: 1.0000\n",
      "Epoch 870/2500\n",
      "120/120 [==============================] - 0s 105us/step - loss: 0.0709 - accuracy: 0.9667 - val_loss: 0.0498 - val_accuracy: 0.9667\n",
      "Epoch 871/2500\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0729 - accuracy: 0.9667 - val_loss: 0.0609 - val_accuracy: 0.9667\n",
      "Epoch 872/2500\n",
      "120/120 [==============================] - 0s 114us/step - loss: 0.0731 - accuracy: 0.9667 - val_loss: 0.0654 - val_accuracy: 0.9667\n",
      "Epoch 873/2500\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0723 - accuracy: 0.9583 - val_loss: 0.0519 - val_accuracy: 0.9667\n",
      "Epoch 874/2500\n",
      "120/120 [==============================] - 0s 103us/step - loss: 0.0710 - accuracy: 0.9667 - val_loss: 0.0519 - val_accuracy: 0.9667\n",
      "Epoch 875/2500\n",
      "120/120 [==============================] - 0s 113us/step - loss: 0.0708 - accuracy: 0.9667 - val_loss: 0.0546 - val_accuracy: 0.9667\n",
      "Epoch 876/2500\n",
      "120/120 [==============================] - 0s 122us/step - loss: 0.0713 - accuracy: 0.9583 - val_loss: 0.0581 - val_accuracy: 0.9667\n",
      "Epoch 877/2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 123us/step - loss: 0.0706 - accuracy: 0.9583 - val_loss: 0.0541 - val_accuracy: 0.9667\n",
      "Epoch 878/2500\n",
      "120/120 [==============================] - 0s 119us/step - loss: 0.0705 - accuracy: 0.9583 - val_loss: 0.0525 - val_accuracy: 0.9667\n",
      "Epoch 879/2500\n",
      "120/120 [==============================] - 0s 146us/step - loss: 0.0704 - accuracy: 0.9583 - val_loss: 0.0545 - val_accuracy: 0.9667\n",
      "Epoch 880/2500\n",
      "120/120 [==============================] - 0s 103us/step - loss: 0.0754 - accuracy: 0.9667 - val_loss: 0.0463 - val_accuracy: 1.0000\n",
      "Epoch 881/2500\n",
      "120/120 [==============================] - 0s 96us/step - loss: 0.0708 - accuracy: 0.9667 - val_loss: 0.0530 - val_accuracy: 0.9667\n",
      "Epoch 882/2500\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0716 - accuracy: 0.9583 - val_loss: 0.0613 - val_accuracy: 0.9667\n",
      "Epoch 883/2500\n",
      "120/120 [==============================] - 0s 112us/step - loss: 0.0735 - accuracy: 0.9667 - val_loss: 0.0620 - val_accuracy: 0.9667\n",
      "Epoch 884/2500\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0725 - accuracy: 0.9583 - val_loss: 0.0494 - val_accuracy: 0.9667\n",
      "Epoch 885/2500\n",
      "120/120 [==============================] - 0s 114us/step - loss: 0.0703 - accuracy: 0.9667 - val_loss: 0.0488 - val_accuracy: 1.0000\n",
      "Epoch 886/2500\n",
      "120/120 [==============================] - 0s 116us/step - loss: 0.0710 - accuracy: 0.9583 - val_loss: 0.0552 - val_accuracy: 0.9667\n",
      "Epoch 887/2500\n",
      "120/120 [==============================] - 0s 116us/step - loss: 0.0705 - accuracy: 0.9583 - val_loss: 0.0579 - val_accuracy: 0.9667\n",
      "Epoch 888/2500\n",
      "120/120 [==============================] - 0s 130us/step - loss: 0.0698 - accuracy: 0.9583 - val_loss: 0.0532 - val_accuracy: 0.9667\n",
      "Epoch 889/2500\n",
      "120/120 [==============================] - 0s 124us/step - loss: 0.0704 - accuracy: 0.9583 - val_loss: 0.0557 - val_accuracy: 0.9667\n",
      "Epoch 890/2500\n",
      "120/120 [==============================] - 0s 120us/step - loss: 0.0712 - accuracy: 0.9667 - val_loss: 0.0563 - val_accuracy: 0.9667\n",
      "Epoch 891/2500\n",
      "120/120 [==============================] - 0s 119us/step - loss: 0.0703 - accuracy: 0.9583 - val_loss: 0.0503 - val_accuracy: 0.9667\n",
      "Epoch 892/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.0705 - accuracy: 0.9583 - val_loss: 0.0534 - val_accuracy: 0.9667\n",
      "Epoch 893/2500\n",
      "120/120 [==============================] - 0s 104us/step - loss: 0.0699 - accuracy: 0.9667 - val_loss: 0.0516 - val_accuracy: 0.9667\n",
      "Epoch 894/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.0707 - accuracy: 0.9583 - val_loss: 0.0559 - val_accuracy: 0.9667\n",
      "Epoch 895/2500\n",
      "120/120 [==============================] - 0s 133us/step - loss: 0.0697 - accuracy: 0.9583 - val_loss: 0.0568 - val_accuracy: 0.9667\n",
      "Epoch 896/2500\n",
      "120/120 [==============================] - 0s 104us/step - loss: 0.0704 - accuracy: 0.9667 - val_loss: 0.0512 - val_accuracy: 0.9667\n",
      "Epoch 897/2500\n",
      "120/120 [==============================] - 0s 120us/step - loss: 0.0696 - accuracy: 0.9667 - val_loss: 0.0522 - val_accuracy: 0.9667\n",
      "Epoch 898/2500\n",
      "120/120 [==============================] - 0s 114us/step - loss: 0.0696 - accuracy: 0.9667 - val_loss: 0.0535 - val_accuracy: 0.9667\n",
      "Epoch 899/2500\n",
      "120/120 [==============================] - 0s 113us/step - loss: 0.0699 - accuracy: 0.9583 - val_loss: 0.0559 - val_accuracy: 0.9667\n",
      "Epoch 900/2500\n",
      "120/120 [==============================] - 0s 110us/step - loss: 0.0703 - accuracy: 0.9667 - val_loss: 0.0507 - val_accuracy: 0.9667\n",
      "Epoch 901/2500\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0695 - accuracy: 0.9667 - val_loss: 0.0518 - val_accuracy: 0.9667\n",
      "Epoch 902/2500\n",
      "120/120 [==============================] - 0s 118us/step - loss: 0.0691 - accuracy: 0.9667 - val_loss: 0.0538 - val_accuracy: 0.9667\n",
      "Epoch 903/2500\n",
      "120/120 [==============================] - 0s 112us/step - loss: 0.0705 - accuracy: 0.9583 - val_loss: 0.0607 - val_accuracy: 0.9667\n",
      "Epoch 904/2500\n",
      "120/120 [==============================] - 0s 130us/step - loss: 0.0709 - accuracy: 0.9667 - val_loss: 0.0581 - val_accuracy: 0.9667\n",
      "Epoch 905/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.0688 - accuracy: 0.9667 - val_loss: 0.0537 - val_accuracy: 0.9667\n",
      "Epoch 906/2500\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.0690 - accuracy: 0.9667 - val_loss: 0.0512 - val_accuracy: 0.9667\n",
      "Epoch 907/2500\n",
      "120/120 [==============================] - 0s 124us/step - loss: 0.0692 - accuracy: 0.9667 - val_loss: 0.0518 - val_accuracy: 0.9667\n",
      "Epoch 908/2500\n",
      "120/120 [==============================] - 0s 122us/step - loss: 0.0696 - accuracy: 0.9583 - val_loss: 0.0535 - val_accuracy: 0.9667\n",
      "Epoch 909/2500\n",
      "120/120 [==============================] - 0s 116us/step - loss: 0.0698 - accuracy: 0.9583 - val_loss: 0.0544 - val_accuracy: 0.9667\n",
      "Epoch 910/2500\n",
      "120/120 [==============================] - 0s 131us/step - loss: 0.0688 - accuracy: 0.9583 - val_loss: 0.0548 - val_accuracy: 0.9667\n",
      "Epoch 911/2500\n",
      "120/120 [==============================] - 0s 126us/step - loss: 0.0691 - accuracy: 0.9667 - val_loss: 0.0521 - val_accuracy: 0.9667\n",
      "Epoch 912/2500\n",
      "120/120 [==============================] - 0s 119us/step - loss: 0.0696 - accuracy: 0.9667 - val_loss: 0.0518 - val_accuracy: 0.9667\n",
      "Epoch 913/2500\n",
      "120/120 [==============================] - 0s 120us/step - loss: 0.0696 - accuracy: 0.9667 - val_loss: 0.0513 - val_accuracy: 0.9667\n",
      "Epoch 914/2500\n",
      "120/120 [==============================] - 0s 137us/step - loss: 0.0692 - accuracy: 0.9667 - val_loss: 0.0583 - val_accuracy: 0.9667\n",
      "Epoch 915/2500\n",
      "120/120 [==============================] - 0s 119us/step - loss: 0.0697 - accuracy: 0.9667 - val_loss: 0.0604 - val_accuracy: 0.9667\n",
      "Epoch 916/2500\n",
      "120/120 [==============================] - 0s 121us/step - loss: 0.0690 - accuracy: 0.9667 - val_loss: 0.0551 - val_accuracy: 0.9667\n",
      "Epoch 917/2500\n",
      "120/120 [==============================] - 0s 118us/step - loss: 0.0690 - accuracy: 0.9667 - val_loss: 0.0534 - val_accuracy: 0.9667\n",
      "Epoch 918/2500\n",
      "120/120 [==============================] - 0s 118us/step - loss: 0.0689 - accuracy: 0.9667 - val_loss: 0.0508 - val_accuracy: 0.9667\n",
      "Epoch 919/2500\n",
      "120/120 [==============================] - 0s 119us/step - loss: 0.0689 - accuracy: 0.9667 - val_loss: 0.0529 - val_accuracy: 0.9667\n",
      "Epoch 920/2500\n",
      "120/120 [==============================] - 0s 140us/step - loss: 0.0707 - accuracy: 0.9667 - val_loss: 0.0494 - val_accuracy: 0.9667\n",
      "Epoch 921/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.0683 - accuracy: 0.9667 - val_loss: 0.0556 - val_accuracy: 0.9667\n",
      "Epoch 922/2500\n",
      "120/120 [==============================] - 0s 118us/step - loss: 0.0692 - accuracy: 0.9667 - val_loss: 0.0603 - val_accuracy: 0.9667\n",
      "Epoch 923/2500\n",
      "120/120 [==============================] - 0s 110us/step - loss: 0.0690 - accuracy: 0.9667 - val_loss: 0.0552 - val_accuracy: 0.9667\n",
      "Epoch 924/2500\n",
      "120/120 [==============================] - 0s 118us/step - loss: 0.0685 - accuracy: 0.9667 - val_loss: 0.0546 - val_accuracy: 0.9667\n",
      "Epoch 925/2500\n",
      "120/120 [==============================] - 0s 123us/step - loss: 0.0694 - accuracy: 0.9583 - val_loss: 0.0500 - val_accuracy: 0.9667\n",
      "Epoch 926/2500\n",
      "120/120 [==============================] - 0s 159us/step - loss: 0.0684 - accuracy: 0.9667 - val_loss: 0.0532 - val_accuracy: 0.9667\n",
      "Epoch 927/2500\n",
      "120/120 [==============================] - 0s 135us/step - loss: 0.0691 - accuracy: 0.9667 - val_loss: 0.0524 - val_accuracy: 0.9667\n",
      "Epoch 928/2500\n",
      "120/120 [==============================] - 0s 136us/step - loss: 0.0681 - accuracy: 0.9667 - val_loss: 0.0546 - val_accuracy: 0.9667\n",
      "Epoch 929/2500\n",
      "120/120 [==============================] - 0s 129us/step - loss: 0.0693 - accuracy: 0.9667 - val_loss: 0.0585 - val_accuracy: 0.9667\n",
      "Epoch 930/2500\n",
      "120/120 [==============================] - 0s 115us/step - loss: 0.0688 - accuracy: 0.9667 - val_loss: 0.0539 - val_accuracy: 0.9667\n",
      "Epoch 931/2500\n",
      "120/120 [==============================] - 0s 116us/step - loss: 0.0689 - accuracy: 0.9667 - val_loss: 0.0510 - val_accuracy: 0.9667\n",
      "Epoch 932/2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 126us/step - loss: 0.0689 - accuracy: 0.9667 - val_loss: 0.0501 - val_accuracy: 0.9667\n",
      "Epoch 933/2500\n",
      "120/120 [==============================] - 0s 114us/step - loss: 0.0697 - accuracy: 0.9583 - val_loss: 0.0584 - val_accuracy: 0.9667\n",
      "Epoch 934/2500\n",
      "120/120 [==============================] - 0s 129us/step - loss: 0.0689 - accuracy: 0.9667 - val_loss: 0.0536 - val_accuracy: 0.9667\n",
      "Epoch 935/2500\n",
      "120/120 [==============================] - 0s 119us/step - loss: 0.0685 - accuracy: 0.9667 - val_loss: 0.0537 - val_accuracy: 0.9667\n",
      "Epoch 936/2500\n",
      "120/120 [==============================] - 0s 112us/step - loss: 0.0682 - accuracy: 0.9667 - val_loss: 0.0528 - val_accuracy: 0.9667\n",
      "Epoch 937/2500\n",
      "120/120 [==============================] - 0s 109us/step - loss: 0.0711 - accuracy: 0.9667 - val_loss: 0.0574 - val_accuracy: 0.9667\n",
      "Epoch 938/2500\n",
      "120/120 [==============================] - 0s 115us/step - loss: 0.0683 - accuracy: 0.9583 - val_loss: 0.0527 - val_accuracy: 0.9667\n",
      "Epoch 939/2500\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.0681 - accuracy: 0.9667 - val_loss: 0.0510 - val_accuracy: 0.9667\n",
      "Epoch 940/2500\n",
      "120/120 [==============================] - 0s 113us/step - loss: 0.0688 - accuracy: 0.9667 - val_loss: 0.0495 - val_accuracy: 0.9667\n",
      "Epoch 941/2500\n",
      "120/120 [==============================] - 0s 163us/step - loss: 0.0678 - accuracy: 0.9667 - val_loss: 0.0523 - val_accuracy: 0.9667\n",
      "Epoch 942/2500\n",
      "120/120 [==============================] - 0s 126us/step - loss: 0.0684 - accuracy: 0.9667 - val_loss: 0.0558 - val_accuracy: 0.9667\n",
      "Epoch 943/2500\n",
      "120/120 [==============================] - 0s 135us/step - loss: 0.0686 - accuracy: 0.9750 - val_loss: 0.0522 - val_accuracy: 0.9667\n",
      "Epoch 944/2500\n",
      "120/120 [==============================] - 0s 138us/step - loss: 0.0683 - accuracy: 0.9667 - val_loss: 0.0542 - val_accuracy: 0.9667\n",
      "Epoch 945/2500\n",
      "120/120 [==============================] - 0s 133us/step - loss: 0.0695 - accuracy: 0.9667 - val_loss: 0.0563 - val_accuracy: 0.9667\n",
      "Epoch 946/2500\n",
      "120/120 [==============================] - 0s 178us/step - loss: 0.0679 - accuracy: 0.9750 - val_loss: 0.0540 - val_accuracy: 0.9667\n",
      "Epoch 947/2500\n",
      "120/120 [==============================] - 0s 132us/step - loss: 0.0691 - accuracy: 0.9667 - val_loss: 0.0551 - val_accuracy: 0.9667\n",
      "Epoch 948/2500\n",
      "120/120 [==============================] - 0s 131us/step - loss: 0.0680 - accuracy: 0.9667 - val_loss: 0.0509 - val_accuracy: 0.9667\n",
      "Epoch 949/2500\n",
      "120/120 [==============================] - 0s 137us/step - loss: 0.0681 - accuracy: 0.9667 - val_loss: 0.0496 - val_accuracy: 0.9667\n",
      "Epoch 950/2500\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0679 - accuracy: 0.9667 - val_loss: 0.0497 - val_accuracy: 0.9667\n",
      "Epoch 951/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.0683 - accuracy: 0.9667 - val_loss: 0.0513 - val_accuracy: 0.9667\n",
      "Epoch 952/2500\n",
      "120/120 [==============================] - 0s 126us/step - loss: 0.0676 - accuracy: 0.9667 - val_loss: 0.0571 - val_accuracy: 0.9667\n",
      "Epoch 953/2500\n",
      "120/120 [==============================] - 0s 120us/step - loss: 0.0685 - accuracy: 0.9667 - val_loss: 0.0567 - val_accuracy: 0.9667\n",
      "Epoch 954/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.0680 - accuracy: 0.9750 - val_loss: 0.0526 - val_accuracy: 0.9667\n",
      "Epoch 955/2500\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0687 - accuracy: 0.9667 - val_loss: 0.0499 - val_accuracy: 0.9667\n",
      "Epoch 956/2500\n",
      "120/120 [==============================] - 0s 124us/step - loss: 0.0683 - accuracy: 0.9667 - val_loss: 0.0550 - val_accuracy: 0.9667\n",
      "Epoch 957/2500\n",
      "120/120 [==============================] - 0s 120us/step - loss: 0.0685 - accuracy: 0.9667 - val_loss: 0.0516 - val_accuracy: 0.9667\n",
      "Epoch 958/2500\n",
      "120/120 [==============================] - 0s 141us/step - loss: 0.0675 - accuracy: 0.9667 - val_loss: 0.0518 - val_accuracy: 0.9667\n",
      "Epoch 959/2500\n",
      "120/120 [==============================] - 0s 120us/step - loss: 0.0686 - accuracy: 0.9667 - val_loss: 0.0511 - val_accuracy: 0.9667\n",
      "Epoch 960/2500\n",
      "120/120 [==============================] - 0s 126us/step - loss: 0.0675 - accuracy: 0.9667 - val_loss: 0.0523 - val_accuracy: 0.9667\n",
      "Epoch 961/2500\n",
      "120/120 [==============================] - 0s 131us/step - loss: 0.0675 - accuracy: 0.9750 - val_loss: 0.0592 - val_accuracy: 0.9667\n",
      "Epoch 962/2500\n",
      "120/120 [==============================] - 0s 131us/step - loss: 0.0695 - accuracy: 0.9667 - val_loss: 0.0535 - val_accuracy: 0.9667\n",
      "Epoch 963/2500\n",
      "120/120 [==============================] - 0s 128us/step - loss: 0.0677 - accuracy: 0.9667 - val_loss: 0.0578 - val_accuracy: 0.9667\n",
      "Epoch 964/2500\n",
      "120/120 [==============================] - 0s 123us/step - loss: 0.0683 - accuracy: 0.9667 - val_loss: 0.0524 - val_accuracy: 0.9667\n",
      "Epoch 965/2500\n",
      "120/120 [==============================] - 0s 116us/step - loss: 0.0672 - accuracy: 0.9750 - val_loss: 0.0535 - val_accuracy: 0.9667\n",
      "Epoch 966/2500\n",
      "120/120 [==============================] - 0s 126us/step - loss: 0.0694 - accuracy: 0.9667 - val_loss: 0.0483 - val_accuracy: 0.9667\n",
      "Epoch 967/2500\n",
      "120/120 [==============================] - 0s 121us/step - loss: 0.0675 - accuracy: 0.9667 - val_loss: 0.0492 - val_accuracy: 0.9667\n",
      "Epoch 968/2500\n",
      "120/120 [==============================] - 0s 112us/step - loss: 0.0665 - accuracy: 0.9667 - val_loss: 0.0555 - val_accuracy: 0.9667\n",
      "Epoch 969/2500\n",
      "120/120 [==============================] - 0s 112us/step - loss: 0.0679 - accuracy: 0.9667 - val_loss: 0.0609 - val_accuracy: 0.9667\n",
      "Epoch 970/2500\n",
      "120/120 [==============================] - 0s 126us/step - loss: 0.0699 - accuracy: 0.9667 - val_loss: 0.0518 - val_accuracy: 0.9667\n",
      "Epoch 971/2500\n",
      "120/120 [==============================] - 0s 115us/step - loss: 0.0671 - accuracy: 0.9667 - val_loss: 0.0510 - val_accuracy: 0.9667\n",
      "Epoch 972/2500\n",
      "120/120 [==============================] - 0s 115us/step - loss: 0.0673 - accuracy: 0.9750 - val_loss: 0.0564 - val_accuracy: 0.9667\n",
      "Epoch 973/2500\n",
      "120/120 [==============================] - 0s 123us/step - loss: 0.0671 - accuracy: 0.9667 - val_loss: 0.0549 - val_accuracy: 0.9667\n",
      "Epoch 974/2500\n",
      "120/120 [==============================] - 0s 114us/step - loss: 0.0675 - accuracy: 0.9750 - val_loss: 0.0573 - val_accuracy: 0.9667\n",
      "Epoch 975/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.0691 - accuracy: 0.9583 - val_loss: 0.0502 - val_accuracy: 0.9667\n",
      "Epoch 976/2500\n",
      "120/120 [==============================] - 0s 109us/step - loss: 0.0672 - accuracy: 0.9667 - val_loss: 0.0498 - val_accuracy: 0.9667\n",
      "Epoch 977/2500\n",
      "120/120 [==============================] - 0s 107us/step - loss: 0.0685 - accuracy: 0.9667 - val_loss: 0.0557 - val_accuracy: 0.9667\n",
      "Epoch 978/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.0672 - accuracy: 0.9750 - val_loss: 0.0518 - val_accuracy: 0.9667\n",
      "Epoch 979/2500\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0667 - accuracy: 0.9750 - val_loss: 0.0531 - val_accuracy: 0.9667\n",
      "Epoch 980/2500\n",
      "120/120 [==============================] - 0s 131us/step - loss: 0.0677 - accuracy: 0.9667 - val_loss: 0.0555 - val_accuracy: 0.9667\n",
      "Epoch 981/2500\n",
      "120/120 [==============================] - 0s 98us/step - loss: 0.0666 - accuracy: 0.9750 - val_loss: 0.0527 - val_accuracy: 0.9667\n",
      "Epoch 982/2500\n",
      "120/120 [==============================] - 0s 271us/step - loss: 0.0673 - accuracy: 0.9667 - val_loss: 0.0508 - val_accuracy: 0.9667\n",
      "Epoch 983/2500\n",
      "120/120 [==============================] - 0s 160us/step - loss: 0.0688 - accuracy: 0.9667 - val_loss: 0.0571 - val_accuracy: 0.9667\n",
      "Epoch 984/2500\n",
      "120/120 [==============================] - 0s 144us/step - loss: 0.0681 - accuracy: 0.9667 - val_loss: 0.0504 - val_accuracy: 0.9667\n",
      "Epoch 985/2500\n",
      "120/120 [==============================] - 0s 132us/step - loss: 0.0677 - accuracy: 0.9667 - val_loss: 0.0478 - val_accuracy: 0.9667\n",
      "Epoch 986/2500\n",
      "120/120 [==============================] - 0s 105us/step - loss: 0.0665 - accuracy: 0.9667 - val_loss: 0.0525 - val_accuracy: 0.9667\n",
      "Epoch 987/2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 130us/step - loss: 0.0674 - accuracy: 0.9750 - val_loss: 0.0570 - val_accuracy: 0.9667\n",
      "Epoch 988/2500\n",
      "120/120 [==============================] - 0s 135us/step - loss: 0.0682 - accuracy: 0.9750 - val_loss: 0.0510 - val_accuracy: 0.9667\n",
      "Epoch 989/2500\n",
      "120/120 [==============================] - 0s 152us/step - loss: 0.0672 - accuracy: 0.9667 - val_loss: 0.0503 - val_accuracy: 0.9667\n",
      "Epoch 990/2500\n",
      "120/120 [==============================] - 0s 149us/step - loss: 0.0677 - accuracy: 0.9750 - val_loss: 0.0581 - val_accuracy: 0.9667\n",
      "Epoch 991/2500\n",
      "120/120 [==============================] - 0s 119us/step - loss: 0.0676 - accuracy: 0.9667 - val_loss: 0.0514 - val_accuracy: 0.9667\n",
      "Epoch 992/2500\n",
      "120/120 [==============================] - 0s 116us/step - loss: 0.0667 - accuracy: 0.9667 - val_loss: 0.0518 - val_accuracy: 0.9667\n",
      "Epoch 993/2500\n",
      "120/120 [==============================] - 0s 122us/step - loss: 0.0667 - accuracy: 0.9750 - val_loss: 0.0509 - val_accuracy: 0.9667\n",
      "Epoch 994/2500\n",
      "120/120 [==============================] - 0s 113us/step - loss: 0.0665 - accuracy: 0.9750 - val_loss: 0.0536 - val_accuracy: 0.9667\n",
      "Epoch 995/2500\n",
      "120/120 [==============================] - 0s 107us/step - loss: 0.0679 - accuracy: 0.9667 - val_loss: 0.0496 - val_accuracy: 0.9667\n",
      "Epoch 996/2500\n",
      "120/120 [==============================] - 0s 100us/step - loss: 0.0664 - accuracy: 0.9667 - val_loss: 0.0503 - val_accuracy: 0.9667\n",
      "Epoch 997/2500\n",
      "120/120 [==============================] - 0s 148us/step - loss: 0.0661 - accuracy: 0.9750 - val_loss: 0.0542 - val_accuracy: 0.9667\n",
      "Epoch 998/2500\n",
      "120/120 [==============================] - 0s 147us/step - loss: 0.0668 - accuracy: 0.9750 - val_loss: 0.0542 - val_accuracy: 0.9667\n",
      "Epoch 999/2500\n",
      "120/120 [==============================] - 0s 142us/step - loss: 0.0661 - accuracy: 0.9750 - val_loss: 0.0547 - val_accuracy: 0.9667\n",
      "Epoch 1000/2500\n",
      "120/120 [==============================] - 0s 113us/step - loss: 0.0665 - accuracy: 0.9750 - val_loss: 0.0570 - val_accuracy: 0.9667\n",
      "Epoch 1001/2500\n",
      "120/120 [==============================] - 0s 107us/step - loss: 0.0661 - accuracy: 0.9833 - val_loss: 0.0550 - val_accuracy: 0.9667\n",
      "Epoch 1002/2500\n",
      "120/120 [==============================] - 0s 99us/step - loss: 0.0675 - accuracy: 0.9667 - val_loss: 0.0510 - val_accuracy: 0.9667\n",
      "Epoch 1003/2500\n",
      "120/120 [==============================] - 0s 112us/step - loss: 0.0673 - accuracy: 0.9667 - val_loss: 0.0517 - val_accuracy: 0.9667\n",
      "Epoch 1004/2500\n",
      "120/120 [==============================] - 0s 102us/step - loss: 0.0681 - accuracy: 0.9750 - val_loss: 0.0586 - val_accuracy: 0.9667\n",
      "Epoch 1005/2500\n",
      "120/120 [==============================] - 0s 105us/step - loss: 0.0662 - accuracy: 0.9833 - val_loss: 0.0538 - val_accuracy: 0.9667\n",
      "Epoch 1006/2500\n",
      "120/120 [==============================] - 0s 177us/step - loss: 0.0671 - accuracy: 0.9667 - val_loss: 0.0514 - val_accuracy: 0.9667\n",
      "Epoch 1007/2500\n",
      "120/120 [==============================] - 0s 155us/step - loss: 0.0663 - accuracy: 0.9750 - val_loss: 0.0503 - val_accuracy: 0.9667\n",
      "Epoch 1008/2500\n",
      "120/120 [==============================] - 0s 126us/step - loss: 0.0659 - accuracy: 0.9750 - val_loss: 0.0529 - val_accuracy: 0.9667\n",
      "Epoch 1009/2500\n",
      "120/120 [==============================] - 0s 114us/step - loss: 0.0660 - accuracy: 0.9750 - val_loss: 0.0536 - val_accuracy: 0.9667\n",
      "Epoch 1010/2500\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0661 - accuracy: 0.9750 - val_loss: 0.0512 - val_accuracy: 0.9667\n",
      "Epoch 1011/2500\n",
      "120/120 [==============================] - 0s 112us/step - loss: 0.0671 - accuracy: 0.9667 - val_loss: 0.0485 - val_accuracy: 0.9667\n",
      "Epoch 1012/2500\n",
      "120/120 [==============================] - 0s 128us/step - loss: 0.0654 - accuracy: 0.9750 - val_loss: 0.0541 - val_accuracy: 0.9667\n",
      "Epoch 1013/2500\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0660 - accuracy: 0.9833 - val_loss: 0.0590 - val_accuracy: 0.9667\n",
      "Epoch 1014/2500\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.0660 - accuracy: 0.9750 - val_loss: 0.0575 - val_accuracy: 0.9667\n",
      "Epoch 1015/2500\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0691 - accuracy: 0.9750 - val_loss: 0.0473 - val_accuracy: 0.9667\n",
      "Epoch 1016/2500\n",
      "120/120 [==============================] - 0s 118us/step - loss: 0.0673 - accuracy: 0.9750 - val_loss: 0.0549 - val_accuracy: 0.9667\n",
      "Epoch 1017/2500\n",
      "120/120 [==============================] - 0s 107us/step - loss: 0.0669 - accuracy: 0.9750 - val_loss: 0.0518 - val_accuracy: 0.9667\n",
      "Epoch 1018/2500\n",
      "120/120 [==============================] - 0s 112us/step - loss: 0.0656 - accuracy: 0.9750 - val_loss: 0.0518 - val_accuracy: 0.9667\n",
      "Epoch 1019/2500\n",
      "120/120 [==============================] - 0s 104us/step - loss: 0.0668 - accuracy: 0.9667 - val_loss: 0.0494 - val_accuracy: 0.9667\n",
      "Epoch 1020/2500\n",
      "120/120 [==============================] - 0s 95us/step - loss: 0.0655 - accuracy: 0.9750 - val_loss: 0.0523 - val_accuracy: 0.9667\n",
      "Epoch 1021/2500\n",
      "120/120 [==============================] - 0s 101us/step - loss: 0.0680 - accuracy: 0.9750 - val_loss: 0.0620 - val_accuracy: 0.9667\n",
      "Epoch 1022/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.0672 - accuracy: 0.9833 - val_loss: 0.0522 - val_accuracy: 0.9667\n",
      "Epoch 1023/2500\n",
      "120/120 [==============================] - 0s 107us/step - loss: 0.0663 - accuracy: 0.9750 - val_loss: 0.0499 - val_accuracy: 0.9667\n",
      "Epoch 1024/2500\n",
      "120/120 [==============================] - 0s 116us/step - loss: 0.0660 - accuracy: 0.9750 - val_loss: 0.0528 - val_accuracy: 0.9667\n",
      "Epoch 1025/2500\n",
      "120/120 [==============================] - 0s 135us/step - loss: 0.0658 - accuracy: 0.9750 - val_loss: 0.0527 - val_accuracy: 0.9667\n",
      "Epoch 1026/2500\n",
      "120/120 [==============================] - 0s 112us/step - loss: 0.0654 - accuracy: 0.9750 - val_loss: 0.0545 - val_accuracy: 0.9667\n",
      "Epoch 1027/2500\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0668 - accuracy: 0.9750 - val_loss: 0.0504 - val_accuracy: 0.9667\n",
      "Epoch 1028/2500\n",
      "120/120 [==============================] - 0s 116us/step - loss: 0.0681 - accuracy: 0.9667 - val_loss: 0.0468 - val_accuracy: 0.9667\n",
      "Epoch 1029/2500\n",
      "120/120 [==============================] - 0s 109us/step - loss: 0.0648 - accuracy: 0.9750 - val_loss: 0.0526 - val_accuracy: 0.9667\n",
      "Epoch 1030/2500\n",
      "120/120 [==============================] - 0s 98us/step - loss: 0.0653 - accuracy: 0.9833 - val_loss: 0.0577 - val_accuracy: 0.9667\n",
      "Epoch 1031/2500\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0657 - accuracy: 0.9750 - val_loss: 0.0586 - val_accuracy: 0.9667\n",
      "Epoch 1032/2500\n",
      "120/120 [==============================] - 0s 113us/step - loss: 0.0661 - accuracy: 0.9833 - val_loss: 0.0556 - val_accuracy: 0.9667\n",
      "Epoch 1033/2500\n",
      "120/120 [==============================] - 0s 95us/step - loss: 0.0657 - accuracy: 0.9833 - val_loss: 0.0498 - val_accuracy: 0.9667\n",
      "Epoch 1034/2500\n",
      "120/120 [==============================] - 0s 98us/step - loss: 0.0657 - accuracy: 0.9833 - val_loss: 0.0518 - val_accuracy: 0.9667\n",
      "Epoch 1035/2500\n",
      "120/120 [==============================] - 0s 96us/step - loss: 0.0651 - accuracy: 0.9750 - val_loss: 0.0500 - val_accuracy: 0.9667\n",
      "Epoch 1036/2500\n",
      "120/120 [==============================] - 0s 102us/step - loss: 0.0653 - accuracy: 0.9750 - val_loss: 0.0523 - val_accuracy: 0.9667\n",
      "Epoch 1037/2500\n",
      "120/120 [==============================] - 0s 102us/step - loss: 0.0650 - accuracy: 0.9750 - val_loss: 0.0526 - val_accuracy: 0.9667\n",
      "Epoch 1038/2500\n",
      "120/120 [==============================] - 0s 107us/step - loss: 0.0661 - accuracy: 0.9750 - val_loss: 0.0489 - val_accuracy: 0.9667\n",
      "Epoch 1039/2500\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0663 - accuracy: 0.9667 - val_loss: 0.0495 - val_accuracy: 0.9667\n",
      "Epoch 1040/2500\n",
      "120/120 [==============================] - 0s 105us/step - loss: 0.0650 - accuracy: 0.9750 - val_loss: 0.0539 - val_accuracy: 0.9667\n",
      "Epoch 1041/2500\n",
      "120/120 [==============================] - 0s 90us/step - loss: 0.0649 - accuracy: 0.9833 - val_loss: 0.0555 - val_accuracy: 0.9667\n",
      "Epoch 1042/2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 108us/step - loss: 0.0649 - accuracy: 0.9833 - val_loss: 0.0548 - val_accuracy: 0.9667\n",
      "Epoch 1043/2500\n",
      "120/120 [==============================] - 0s 112us/step - loss: 0.0652 - accuracy: 0.9833 - val_loss: 0.0540 - val_accuracy: 0.9667\n",
      "Epoch 1044/2500\n",
      "120/120 [==============================] - 0s 102us/step - loss: 0.0656 - accuracy: 0.9750 - val_loss: 0.0542 - val_accuracy: 0.9667\n",
      "Epoch 1045/2500\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0668 - accuracy: 0.9750 - val_loss: 0.0502 - val_accuracy: 0.9667\n",
      "Epoch 1046/2500\n",
      "120/120 [==============================] - 0s 128us/step - loss: 0.0660 - accuracy: 0.9833 - val_loss: 0.0575 - val_accuracy: 0.9667\n",
      "Epoch 1047/2500\n",
      "120/120 [==============================] - 0s 98us/step - loss: 0.0659 - accuracy: 0.9750 - val_loss: 0.0598 - val_accuracy: 0.9667\n",
      "Epoch 1048/2500\n",
      "120/120 [==============================] - 0s 88us/step - loss: 0.0654 - accuracy: 0.9833 - val_loss: 0.0512 - val_accuracy: 0.9667\n",
      "Epoch 1049/2500\n",
      "120/120 [==============================] - 0s 89us/step - loss: 0.0652 - accuracy: 0.9750 - val_loss: 0.0491 - val_accuracy: 0.9667\n",
      "Epoch 1050/2500\n",
      "120/120 [==============================] - 0s 131us/step - loss: 0.0668 - accuracy: 0.9667 - val_loss: 0.0462 - val_accuracy: 0.9667\n",
      "Epoch 1051/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.0654 - accuracy: 0.9750 - val_loss: 0.0516 - val_accuracy: 0.9667\n",
      "Epoch 1052/2500\n",
      "120/120 [==============================] - 0s 105us/step - loss: 0.0662 - accuracy: 0.9833 - val_loss: 0.0580 - val_accuracy: 0.9667\n",
      "Epoch 1053/2500\n",
      "120/120 [==============================] - 0s 120us/step - loss: 0.0653 - accuracy: 0.9833 - val_loss: 0.0508 - val_accuracy: 0.9667\n",
      "Epoch 1054/2500\n",
      "120/120 [==============================] - 0s 122us/step - loss: 0.0646 - accuracy: 0.9833 - val_loss: 0.0517 - val_accuracy: 0.9667\n",
      "Epoch 1055/2500\n",
      "120/120 [==============================] - 0s 93us/step - loss: 0.0654 - accuracy: 0.9750 - val_loss: 0.0495 - val_accuracy: 0.9667\n",
      "Epoch 1056/2500\n",
      "120/120 [==============================] - 0s 97us/step - loss: 0.0657 - accuracy: 0.9833 - val_loss: 0.0528 - val_accuracy: 0.9667\n",
      "Epoch 1057/2500\n",
      "120/120 [==============================] - 0s 129us/step - loss: 0.0670 - accuracy: 0.9833 - val_loss: 0.0563 - val_accuracy: 0.9667\n",
      "Epoch 1058/2500\n",
      "120/120 [==============================] - 0s 92us/step - loss: 0.0648 - accuracy: 0.9833 - val_loss: 0.0519 - val_accuracy: 0.9667\n",
      "Epoch 1059/2500\n",
      "120/120 [==============================] - 0s 101us/step - loss: 0.0651 - accuracy: 0.9833 - val_loss: 0.0508 - val_accuracy: 0.9667\n",
      "Epoch 1060/2500\n",
      "120/120 [==============================] - 0s 131us/step - loss: 0.0669 - accuracy: 0.9833 - val_loss: 0.0560 - val_accuracy: 0.9667\n",
      "Epoch 1061/2500\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.0646 - accuracy: 0.9833 - val_loss: 0.0495 - val_accuracy: 0.9667\n",
      "Epoch 1062/2500\n",
      "120/120 [==============================] - 0s 105us/step - loss: 0.0646 - accuracy: 0.9833 - val_loss: 0.0496 - val_accuracy: 0.9667\n",
      "Epoch 1063/2500\n",
      "120/120 [==============================] - 0s 124us/step - loss: 0.0655 - accuracy: 0.9833 - val_loss: 0.0536 - val_accuracy: 0.9667\n",
      "Epoch 1064/2500\n",
      "120/120 [==============================] - 0s 134us/step - loss: 0.0647 - accuracy: 0.9833 - val_loss: 0.0514 - val_accuracy: 0.9667\n",
      "Epoch 1065/2500\n",
      "120/120 [==============================] - 0s 103us/step - loss: 0.0647 - accuracy: 0.9750 - val_loss: 0.0492 - val_accuracy: 0.9667\n",
      "Epoch 1066/2500\n",
      "120/120 [==============================] - 0s 99us/step - loss: 0.0654 - accuracy: 0.9833 - val_loss: 0.0502 - val_accuracy: 0.9667\n",
      "Epoch 1067/2500\n",
      "120/120 [==============================] - 0s 98us/step - loss: 0.0657 - accuracy: 0.9750 - val_loss: 0.0500 - val_accuracy: 0.9667\n",
      "Epoch 1068/2500\n",
      "120/120 [==============================] - 0s 95us/step - loss: 0.0656 - accuracy: 0.9833 - val_loss: 0.0533 - val_accuracy: 0.9667\n",
      "Epoch 1069/2500\n",
      "120/120 [==============================] - 0s 95us/step - loss: 0.0655 - accuracy: 0.9833 - val_loss: 0.0541 - val_accuracy: 0.9667\n",
      "Epoch 1070/2500\n",
      "120/120 [==============================] - 0s 96us/step - loss: 0.0646 - accuracy: 0.9833 - val_loss: 0.0546 - val_accuracy: 0.9667\n",
      "Epoch 1071/2500\n",
      "120/120 [==============================] - 0s 96us/step - loss: 0.0662 - accuracy: 0.9750 - val_loss: 0.0472 - val_accuracy: 0.9667\n",
      "Epoch 1072/2500\n",
      "120/120 [==============================] - 0s 99us/step - loss: 0.0665 - accuracy: 0.9750 - val_loss: 0.0563 - val_accuracy: 0.9667\n",
      "Epoch 1073/2500\n",
      "120/120 [==============================] - 0s 100us/step - loss: 0.0646 - accuracy: 0.9833 - val_loss: 0.0537 - val_accuracy: 0.9667\n",
      "Epoch 1074/2500\n",
      "120/120 [==============================] - 0s 133us/step - loss: 0.0656 - accuracy: 0.9750 - val_loss: 0.0480 - val_accuracy: 0.9667\n",
      "Epoch 1075/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.0646 - accuracy: 0.9833 - val_loss: 0.0517 - val_accuracy: 0.9667\n",
      "Epoch 1076/2500\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0653 - accuracy: 0.9833 - val_loss: 0.0499 - val_accuracy: 0.9667\n",
      "Epoch 1077/2500\n",
      "120/120 [==============================] - 0s 89us/step - loss: 0.0643 - accuracy: 0.9833 - val_loss: 0.0496 - val_accuracy: 0.9667\n",
      "Epoch 1078/2500\n",
      "120/120 [==============================] - 0s 95us/step - loss: 0.0666 - accuracy: 0.9667 - val_loss: 0.0460 - val_accuracy: 0.9667\n",
      "Epoch 1079/2500\n",
      "120/120 [==============================] - 0s 120us/step - loss: 0.0661 - accuracy: 0.9833 - val_loss: 0.0579 - val_accuracy: 0.9667\n",
      "Epoch 1080/2500\n",
      "120/120 [==============================] - 0s 107us/step - loss: 0.0649 - accuracy: 0.9833 - val_loss: 0.0534 - val_accuracy: 0.9667\n",
      "Epoch 1081/2500\n",
      "120/120 [==============================] - 0s 116us/step - loss: 0.0649 - accuracy: 0.9833 - val_loss: 0.0506 - val_accuracy: 0.9667\n",
      "Epoch 1082/2500\n",
      "120/120 [==============================] - 0s 118us/step - loss: 0.0645 - accuracy: 0.9833 - val_loss: 0.0538 - val_accuracy: 0.9667\n",
      "Epoch 1083/2500\n",
      "120/120 [==============================] - 0s 97us/step - loss: 0.0640 - accuracy: 0.9833 - val_loss: 0.0523 - val_accuracy: 0.9667\n",
      "Epoch 1084/2500\n",
      "120/120 [==============================] - 0s 96us/step - loss: 0.0674 - accuracy: 0.9667 - val_loss: 0.0480 - val_accuracy: 0.9667\n",
      "Epoch 1085/2500\n",
      "120/120 [==============================] - 0s 105us/step - loss: 0.0647 - accuracy: 0.9750 - val_loss: 0.0555 - val_accuracy: 0.9667\n",
      "Epoch 1086/2500\n",
      "120/120 [==============================] - 0s 112us/step - loss: 0.0665 - accuracy: 0.9750 - val_loss: 0.0618 - val_accuracy: 0.9667\n",
      "Epoch 1087/2500\n",
      "120/120 [==============================] - 0s 110us/step - loss: 0.0679 - accuracy: 0.9833 - val_loss: 0.0480 - val_accuracy: 0.9667\n",
      "Epoch 1088/2500\n",
      "120/120 [==============================] - 0s 120us/step - loss: 0.0642 - accuracy: 0.9750 - val_loss: 0.0496 - val_accuracy: 0.9667\n",
      "Epoch 1089/2500\n",
      "120/120 [==============================] - 0s 121us/step - loss: 0.0643 - accuracy: 0.9833 - val_loss: 0.0489 - val_accuracy: 0.9667\n",
      "Epoch 1090/2500\n",
      "120/120 [==============================] - 0s 99us/step - loss: 0.0658 - accuracy: 0.9833 - val_loss: 0.0556 - val_accuracy: 0.9667\n",
      "Epoch 1091/2500\n",
      "120/120 [==============================] - 0s 109us/step - loss: 0.0639 - accuracy: 0.9833 - val_loss: 0.0543 - val_accuracy: 0.9667\n",
      "Epoch 1092/2500\n",
      "120/120 [==============================] - 0s 119us/step - loss: 0.0645 - accuracy: 0.9833 - val_loss: 0.0501 - val_accuracy: 0.9667\n",
      "Epoch 1093/2500\n",
      "120/120 [==============================] - 0s 100us/step - loss: 0.0636 - accuracy: 0.9833 - val_loss: 0.0512 - val_accuracy: 0.9667\n",
      "Epoch 1094/2500\n",
      "120/120 [==============================] - 0s 104us/step - loss: 0.0642 - accuracy: 0.9833 - val_loss: 0.0540 - val_accuracy: 0.9667\n",
      "Epoch 1095/2500\n",
      "120/120 [==============================] - 0s 119us/step - loss: 0.0662 - accuracy: 0.9833 - val_loss: 0.0483 - val_accuracy: 0.9667\n",
      "Epoch 1096/2500\n",
      "120/120 [==============================] - 0s 115us/step - loss: 0.0640 - accuracy: 0.9833 - val_loss: 0.0498 - val_accuracy: 0.9667\n",
      "Epoch 1097/2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 95us/step - loss: 0.0633 - accuracy: 0.9833 - val_loss: 0.0534 - val_accuracy: 0.9667\n",
      "Epoch 1098/2500\n",
      "120/120 [==============================] - 0s 97us/step - loss: 0.0642 - accuracy: 0.9833 - val_loss: 0.0580 - val_accuracy: 0.9667\n",
      "Epoch 1099/2500\n",
      "120/120 [==============================] - 0s 101us/step - loss: 0.0646 - accuracy: 0.9833 - val_loss: 0.0507 - val_accuracy: 0.9667\n",
      "Epoch 1100/2500\n",
      "120/120 [==============================] - 0s 98us/step - loss: 0.0638 - accuracy: 0.9833 - val_loss: 0.0498 - val_accuracy: 0.9667\n",
      "Epoch 1101/2500\n",
      "120/120 [==============================] - 0s 99us/step - loss: 0.0640 - accuracy: 0.9833 - val_loss: 0.0526 - val_accuracy: 0.9667\n",
      "Epoch 1102/2500\n",
      "120/120 [==============================] - 0s 112us/step - loss: 0.0647 - accuracy: 0.9833 - val_loss: 0.0551 - val_accuracy: 0.9667\n",
      "Epoch 1103/2500\n",
      "120/120 [==============================] - 0s 93us/step - loss: 0.0633 - accuracy: 0.9833 - val_loss: 0.0518 - val_accuracy: 0.9667\n",
      "Epoch 1104/2500\n",
      "120/120 [==============================] - 0s 95us/step - loss: 0.0674 - accuracy: 0.9750 - val_loss: 0.0455 - val_accuracy: 0.9667\n",
      "Epoch 1105/2500\n",
      "120/120 [==============================] - 0s 98us/step - loss: 0.0637 - accuracy: 0.9833 - val_loss: 0.0531 - val_accuracy: 0.9667\n",
      "Epoch 1106/2500\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0637 - accuracy: 0.9833 - val_loss: 0.0517 - val_accuracy: 0.9667\n",
      "Epoch 1107/2500\n",
      "120/120 [==============================] - 0s 97us/step - loss: 0.0641 - accuracy: 0.9833 - val_loss: 0.0576 - val_accuracy: 0.9667\n",
      "Epoch 1108/2500\n",
      "120/120 [==============================] - 0s 85us/step - loss: 0.0642 - accuracy: 0.9833 - val_loss: 0.0520 - val_accuracy: 0.9667\n",
      "Epoch 1109/2500\n",
      "120/120 [==============================] - 0s 98us/step - loss: 0.0633 - accuracy: 0.9833 - val_loss: 0.0517 - val_accuracy: 0.9667\n",
      "Epoch 1110/2500\n",
      "120/120 [==============================] - 0s 113us/step - loss: 0.0648 - accuracy: 0.9833 - val_loss: 0.0482 - val_accuracy: 0.9667\n",
      "Epoch 1111/2500\n",
      "120/120 [==============================] - 0s 104us/step - loss: 0.0632 - accuracy: 0.9833 - val_loss: 0.0502 - val_accuracy: 0.9667\n",
      "Epoch 1112/2500\n",
      "120/120 [==============================] - 0s 97us/step - loss: 0.0678 - accuracy: 0.9750 - val_loss: 0.0620 - val_accuracy: 0.9667\n",
      "Epoch 1113/2500\n",
      "120/120 [==============================] - 0s 110us/step - loss: 0.0637 - accuracy: 0.9833 - val_loss: 0.0550 - val_accuracy: 0.9667\n",
      "Epoch 1114/2500\n",
      "120/120 [==============================] - 0s 102us/step - loss: 0.0630 - accuracy: 0.9833 - val_loss: 0.0494 - val_accuracy: 0.9667\n",
      "Epoch 1115/2500\n",
      "120/120 [==============================] - 0s 94us/step - loss: 0.0633 - accuracy: 0.9833 - val_loss: 0.0497 - val_accuracy: 0.9667\n",
      "Epoch 1116/2500\n",
      "120/120 [==============================] - 0s 99us/step - loss: 0.0639 - accuracy: 0.9833 - val_loss: 0.0478 - val_accuracy: 0.9667\n",
      "Epoch 1117/2500\n",
      "120/120 [==============================] - 0s 135us/step - loss: 0.0639 - accuracy: 0.9833 - val_loss: 0.0493 - val_accuracy: 0.9667\n",
      "Epoch 1118/2500\n",
      "120/120 [==============================] - 0s 100us/step - loss: 0.0636 - accuracy: 0.9833 - val_loss: 0.0508 - val_accuracy: 0.9667\n",
      "Epoch 1119/2500\n",
      "120/120 [==============================] - 0s 90us/step - loss: 0.0642 - accuracy: 0.9833 - val_loss: 0.0559 - val_accuracy: 0.9667\n",
      "Epoch 1120/2500\n",
      "120/120 [==============================] - 0s 115us/step - loss: 0.0636 - accuracy: 0.9833 - val_loss: 0.0534 - val_accuracy: 0.9667\n",
      "Epoch 1121/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.0634 - accuracy: 0.9833 - val_loss: 0.0527 - val_accuracy: 0.9667\n",
      "Epoch 1122/2500\n",
      "120/120 [==============================] - 0s 96us/step - loss: 0.0633 - accuracy: 0.9833 - val_loss: 0.0505 - val_accuracy: 0.9667\n",
      "Epoch 1123/2500\n",
      "120/120 [==============================] - 0s 101us/step - loss: 0.0648 - accuracy: 0.9750 - val_loss: 0.0458 - val_accuracy: 0.9667\n",
      "Epoch 1124/2500\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0633 - accuracy: 0.9833 - val_loss: 0.0474 - val_accuracy: 0.9667\n",
      "Epoch 1125/2500\n",
      "120/120 [==============================] - 0s 89us/step - loss: 0.0630 - accuracy: 0.9833 - val_loss: 0.0529 - val_accuracy: 0.9667\n",
      "Epoch 1126/2500\n",
      "120/120 [==============================] - 0s 95us/step - loss: 0.0642 - accuracy: 0.9833 - val_loss: 0.0513 - val_accuracy: 0.9667\n",
      "Epoch 1127/2500\n",
      "120/120 [==============================] - 0s 102us/step - loss: 0.0631 - accuracy: 0.9833 - val_loss: 0.0535 - val_accuracy: 0.9667\n",
      "Epoch 1128/2500\n",
      "120/120 [==============================] - 0s 102us/step - loss: 0.0641 - accuracy: 0.9833 - val_loss: 0.0572 - val_accuracy: 0.9667\n",
      "Epoch 1129/2500\n",
      "120/120 [==============================] - 0s 85us/step - loss: 0.0635 - accuracy: 0.9833 - val_loss: 0.0522 - val_accuracy: 0.9667\n",
      "Epoch 1130/2500\n",
      "120/120 [==============================] - 0s 98us/step - loss: 0.0637 - accuracy: 0.9833 - val_loss: 0.0488 - val_accuracy: 0.9667\n",
      "Epoch 1131/2500\n",
      "120/120 [==============================] - 0s 102us/step - loss: 0.0632 - accuracy: 0.9833 - val_loss: 0.0516 - val_accuracy: 0.9667\n",
      "Epoch 1132/2500\n",
      "120/120 [==============================] - 0s 109us/step - loss: 0.0630 - accuracy: 0.9833 - val_loss: 0.0502 - val_accuracy: 0.9667\n",
      "Epoch 1133/2500\n",
      "120/120 [==============================] - 0s 93us/step - loss: 0.0633 - accuracy: 0.9833 - val_loss: 0.0493 - val_accuracy: 0.9667\n",
      "Epoch 1134/2500\n",
      "120/120 [==============================] - 0s 100us/step - loss: 0.0647 - accuracy: 0.9833 - val_loss: 0.0573 - val_accuracy: 0.9667\n",
      "Epoch 1135/2500\n",
      "120/120 [==============================] - 0s 99us/step - loss: 0.0661 - accuracy: 0.9833 - val_loss: 0.0471 - val_accuracy: 0.9667\n",
      "Epoch 1136/2500\n",
      "120/120 [==============================] - 0s 104us/step - loss: 0.0636 - accuracy: 0.9833 - val_loss: 0.0501 - val_accuracy: 0.9667\n",
      "Epoch 1137/2500\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.0656 - accuracy: 0.9833 - val_loss: 0.0607 - val_accuracy: 0.9667\n",
      "Epoch 1138/2500\n",
      "120/120 [==============================] - 0s 99us/step - loss: 0.0633 - accuracy: 0.9833 - val_loss: 0.0529 - val_accuracy: 0.9667\n",
      "Epoch 1139/2500\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.0636 - accuracy: 0.9833 - val_loss: 0.0547 - val_accuracy: 0.9667\n",
      "Epoch 1140/2500\n",
      "120/120 [==============================] - 0s 110us/step - loss: 0.0629 - accuracy: 0.9833 - val_loss: 0.0482 - val_accuracy: 0.9667\n",
      "Epoch 1141/2500\n",
      "120/120 [==============================] - 0s 89us/step - loss: 0.0633 - accuracy: 0.9833 - val_loss: 0.0478 - val_accuracy: 0.9667\n",
      "Epoch 1142/2500\n",
      "120/120 [==============================] - 0s 97us/step - loss: 0.0629 - accuracy: 0.9833 - val_loss: 0.0481 - val_accuracy: 0.9667\n",
      "Epoch 1143/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.0633 - accuracy: 0.9833 - val_loss: 0.0514 - val_accuracy: 0.9667\n",
      "Epoch 1144/2500\n",
      "120/120 [==============================] - 0s 104us/step - loss: 0.0629 - accuracy: 0.9833 - val_loss: 0.0515 - val_accuracy: 0.9667\n",
      "Epoch 1145/2500\n",
      "120/120 [==============================] - 0s 99us/step - loss: 0.0627 - accuracy: 0.9833 - val_loss: 0.0545 - val_accuracy: 0.9667\n",
      "Epoch 1146/2500\n",
      "120/120 [==============================] - 0s 96us/step - loss: 0.0625 - accuracy: 0.9833 - val_loss: 0.0542 - val_accuracy: 0.9667\n",
      "Epoch 1147/2500\n",
      "120/120 [==============================] - 0s 112us/step - loss: 0.0629 - accuracy: 0.9833 - val_loss: 0.0540 - val_accuracy: 0.9667\n",
      "Epoch 1148/2500\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0644 - accuracy: 0.9833 - val_loss: 0.0488 - val_accuracy: 0.9667\n",
      "Epoch 1149/2500\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.0625 - accuracy: 0.9833 - val_loss: 0.0517 - val_accuracy: 0.9667\n",
      "Epoch 1150/2500\n",
      "120/120 [==============================] - 0s 115us/step - loss: 0.0628 - accuracy: 0.9833 - val_loss: 0.0497 - val_accuracy: 0.9667\n",
      "Epoch 1151/2500\n",
      "120/120 [==============================] - 0s 95us/step - loss: 0.0625 - accuracy: 0.9833 - val_loss: 0.0520 - val_accuracy: 0.9667\n",
      "Epoch 1152/2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 118us/step - loss: 0.0638 - accuracy: 0.9833 - val_loss: 0.0477 - val_accuracy: 0.9667\n",
      "Epoch 1153/2500\n",
      "120/120 [==============================] - 0s 147us/step - loss: 0.0627 - accuracy: 0.9833 - val_loss: 0.0513 - val_accuracy: 0.9667\n",
      "Epoch 1154/2500\n",
      "120/120 [==============================] - 0s 131us/step - loss: 0.0644 - accuracy: 0.9833 - val_loss: 0.0586 - val_accuracy: 0.9667\n",
      "Epoch 1155/2500\n",
      "120/120 [==============================] - 0s 124us/step - loss: 0.0636 - accuracy: 0.9833 - val_loss: 0.0565 - val_accuracy: 0.9667\n",
      "Epoch 1156/2500\n",
      "120/120 [==============================] - 0s 130us/step - loss: 0.0632 - accuracy: 0.9833 - val_loss: 0.0506 - val_accuracy: 0.9667\n",
      "Epoch 1157/2500\n",
      "120/120 [==============================] - 0s 127us/step - loss: 0.0633 - accuracy: 0.9833 - val_loss: 0.0470 - val_accuracy: 0.9667\n",
      "Epoch 1158/2500\n",
      "120/120 [==============================] - 0s 123us/step - loss: 0.0655 - accuracy: 0.9583 - val_loss: 0.0440 - val_accuracy: 0.9667\n",
      "Epoch 1159/2500\n",
      "120/120 [==============================] - 0s 131us/step - loss: 0.0621 - accuracy: 0.9833 - val_loss: 0.0514 - val_accuracy: 0.9667\n",
      "Epoch 1160/2500\n",
      "120/120 [==============================] - 0s 118us/step - loss: 0.0636 - accuracy: 0.9833 - val_loss: 0.0582 - val_accuracy: 0.9667\n",
      "Epoch 1161/2500\n",
      "120/120 [==============================] - 0s 110us/step - loss: 0.0625 - accuracy: 0.9833 - val_loss: 0.0550 - val_accuracy: 0.9667\n",
      "Epoch 1162/2500\n",
      "120/120 [==============================] - 0s 133us/step - loss: 0.0622 - accuracy: 0.9833 - val_loss: 0.0521 - val_accuracy: 0.9667\n",
      "Epoch 1163/2500\n",
      "120/120 [==============================] - 0s 123us/step - loss: 0.0631 - accuracy: 0.9833 - val_loss: 0.0547 - val_accuracy: 0.9667\n",
      "Epoch 1164/2500\n",
      "120/120 [==============================] - 0s 120us/step - loss: 0.0624 - accuracy: 0.9833 - val_loss: 0.0497 - val_accuracy: 0.9667\n",
      "Epoch 1165/2500\n",
      "120/120 [==============================] - 0s 135us/step - loss: 0.0621 - accuracy: 0.9833 - val_loss: 0.0476 - val_accuracy: 0.9667\n",
      "Epoch 1166/2500\n",
      "120/120 [==============================] - 0s 126us/step - loss: 0.0626 - accuracy: 0.9833 - val_loss: 0.0459 - val_accuracy: 0.9667\n",
      "Epoch 1167/2500\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0634 - accuracy: 0.9833 - val_loss: 0.0533 - val_accuracy: 0.9667\n",
      "Epoch 1168/2500\n",
      "120/120 [==============================] - 0s 126us/step - loss: 0.0621 - accuracy: 0.9833 - val_loss: 0.0514 - val_accuracy: 0.9667\n",
      "Epoch 1169/2500\n",
      "120/120 [==============================] - 0s 121us/step - loss: 0.0622 - accuracy: 0.9833 - val_loss: 0.0509 - val_accuracy: 0.9667\n",
      "Epoch 1170/2500\n",
      "120/120 [==============================] - 0s 104us/step - loss: 0.0630 - accuracy: 0.9833 - val_loss: 0.0507 - val_accuracy: 0.9667\n",
      "Epoch 1171/2500\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.0619 - accuracy: 0.9833 - val_loss: 0.0511 - val_accuracy: 0.9667\n",
      "Epoch 1172/2500\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.0643 - accuracy: 0.9833 - val_loss: 0.0547 - val_accuracy: 0.9667\n",
      "Epoch 1173/2500\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0622 - accuracy: 0.9833 - val_loss: 0.0534 - val_accuracy: 0.9667\n",
      "Epoch 1174/2500\n",
      "120/120 [==============================] - 0s 119us/step - loss: 0.0620 - accuracy: 0.9833 - val_loss: 0.0507 - val_accuracy: 0.9667\n",
      "Epoch 1175/2500\n",
      "120/120 [==============================] - 0s 120us/step - loss: 0.0629 - accuracy: 0.9833 - val_loss: 0.0457 - val_accuracy: 0.9667\n",
      "Epoch 1176/2500\n",
      "120/120 [==============================] - 0s 112us/step - loss: 0.0623 - accuracy: 0.9833 - val_loss: 0.0470 - val_accuracy: 0.9667\n",
      "Epoch 1177/2500\n",
      "120/120 [==============================] - 0s 118us/step - loss: 0.0620 - accuracy: 0.9833 - val_loss: 0.0506 - val_accuracy: 0.9667\n",
      "Epoch 1178/2500\n",
      "120/120 [==============================] - 0s 124us/step - loss: 0.0619 - accuracy: 0.9833 - val_loss: 0.0515 - val_accuracy: 0.9667\n",
      "Epoch 1179/2500\n",
      "120/120 [==============================] - 0s 107us/step - loss: 0.0619 - accuracy: 0.9833 - val_loss: 0.0550 - val_accuracy: 0.9667\n",
      "Epoch 1180/2500\n",
      "120/120 [==============================] - 0s 270us/step - loss: 0.0622 - accuracy: 0.9833 - val_loss: 0.0536 - val_accuracy: 0.9667\n",
      "Epoch 1181/2500\n",
      "120/120 [==============================] - 0s 203us/step - loss: 0.0622 - accuracy: 0.9833 - val_loss: 0.0510 - val_accuracy: 0.9667\n",
      "Epoch 1182/2500\n",
      "120/120 [==============================] - 0s 193us/step - loss: 0.0626 - accuracy: 0.9833 - val_loss: 0.0536 - val_accuracy: 0.9667\n",
      "Epoch 1183/2500\n",
      "120/120 [==============================] - 0s 163us/step - loss: 0.0635 - accuracy: 0.9833 - val_loss: 0.0549 - val_accuracy: 0.9667\n",
      "Epoch 1184/2500\n",
      "120/120 [==============================] - 0s 148us/step - loss: 0.0695 - accuracy: 0.9583 - val_loss: 0.0413 - val_accuracy: 1.0000\n",
      "Epoch 1185/2500\n",
      "120/120 [==============================] - 0s 143us/step - loss: 0.0627 - accuracy: 0.9750 - val_loss: 0.0484 - val_accuracy: 0.9667\n",
      "Epoch 1186/2500\n",
      "120/120 [==============================] - 0s 136us/step - loss: 0.0622 - accuracy: 0.9833 - val_loss: 0.0501 - val_accuracy: 0.9667\n",
      "Epoch 1187/2500\n",
      "120/120 [==============================] - 0s 146us/step - loss: 0.0632 - accuracy: 0.9833 - val_loss: 0.0576 - val_accuracy: 0.9667\n",
      "Epoch 1188/2500\n",
      "120/120 [==============================] - 0s 144us/step - loss: 0.0629 - accuracy: 0.9833 - val_loss: 0.0507 - val_accuracy: 0.9667\n",
      "Epoch 1189/2500\n",
      "120/120 [==============================] - 0s 150us/step - loss: 0.0628 - accuracy: 0.9833 - val_loss: 0.0532 - val_accuracy: 0.9667\n",
      "Epoch 1190/2500\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.0622 - accuracy: 0.9833 - val_loss: 0.0524 - val_accuracy: 0.9667\n",
      "Epoch 1191/2500\n",
      "120/120 [==============================] - 0s 149us/step - loss: 0.0618 - accuracy: 0.9833 - val_loss: 0.0501 - val_accuracy: 0.9667\n",
      "Epoch 1192/2500\n",
      "120/120 [==============================] - 0s 151us/step - loss: 0.0618 - accuracy: 0.9833 - val_loss: 0.0479 - val_accuracy: 0.9667\n",
      "Epoch 1193/2500\n",
      "120/120 [==============================] - 0s 157us/step - loss: 0.0619 - accuracy: 0.9833 - val_loss: 0.0483 - val_accuracy: 0.9667\n",
      "Epoch 1194/2500\n",
      "120/120 [==============================] - 0s 173us/step - loss: 0.0637 - accuracy: 0.9833 - val_loss: 0.0542 - val_accuracy: 0.9667\n",
      "Epoch 1195/2500\n",
      "120/120 [==============================] - 0s 131us/step - loss: 0.0625 - accuracy: 0.9833 - val_loss: 0.0550 - val_accuracy: 0.9667\n",
      "Epoch 1196/2500\n",
      "120/120 [==============================] - 0s 133us/step - loss: 0.0618 - accuracy: 0.9833 - val_loss: 0.0484 - val_accuracy: 0.9667\n",
      "Epoch 1197/2500\n",
      "120/120 [==============================] - 0s 149us/step - loss: 0.0624 - accuracy: 0.9833 - val_loss: 0.0451 - val_accuracy: 0.9667\n",
      "Epoch 1198/2500\n",
      "120/120 [==============================] - 0s 124us/step - loss: 0.0633 - accuracy: 0.9833 - val_loss: 0.0521 - val_accuracy: 0.9667\n",
      "Epoch 1199/2500\n",
      "120/120 [==============================] - 0s 133us/step - loss: 0.0623 - accuracy: 0.9833 - val_loss: 0.0555 - val_accuracy: 0.9667\n",
      "Epoch 1200/2500\n",
      "120/120 [==============================] - 0s 138us/step - loss: 0.0626 - accuracy: 0.9833 - val_loss: 0.0500 - val_accuracy: 0.9667\n",
      "Epoch 1201/2500\n",
      "120/120 [==============================] - 0s 130us/step - loss: 0.0624 - accuracy: 0.9833 - val_loss: 0.0482 - val_accuracy: 0.9667\n",
      "Epoch 1202/2500\n",
      "120/120 [==============================] - 0s 138us/step - loss: 0.0619 - accuracy: 0.9833 - val_loss: 0.0531 - val_accuracy: 0.9667\n",
      "Epoch 1203/2500\n",
      "120/120 [==============================] - 0s 159us/step - loss: 0.0636 - accuracy: 0.9833 - val_loss: 0.0560 - val_accuracy: 0.9667\n",
      "Epoch 1204/2500\n",
      "120/120 [==============================] - 0s 138us/step - loss: 0.0641 - accuracy: 0.9833 - val_loss: 0.0463 - val_accuracy: 0.9667\n",
      "Epoch 1205/2500\n",
      "120/120 [==============================] - 0s 132us/step - loss: 0.0636 - accuracy: 0.9750 - val_loss: 0.0427 - val_accuracy: 0.9667\n",
      "Epoch 1206/2500\n",
      "120/120 [==============================] - 0s 119us/step - loss: 0.0648 - accuracy: 0.9833 - val_loss: 0.0570 - val_accuracy: 0.9667\n",
      "Epoch 1207/2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 114us/step - loss: 0.0655 - accuracy: 0.9833 - val_loss: 0.0486 - val_accuracy: 0.9667\n",
      "Epoch 1208/2500\n",
      "120/120 [==============================] - 0s 127us/step - loss: 0.0621 - accuracy: 0.9833 - val_loss: 0.0517 - val_accuracy: 0.9667\n",
      "Epoch 1209/2500\n",
      "120/120 [==============================] - 0s 133us/step - loss: 0.0619 - accuracy: 0.9833 - val_loss: 0.0507 - val_accuracy: 0.9667\n",
      "Epoch 1210/2500\n",
      "120/120 [==============================] - 0s 109us/step - loss: 0.0640 - accuracy: 0.9833 - val_loss: 0.0591 - val_accuracy: 0.9667\n",
      "Epoch 1211/2500\n",
      "120/120 [==============================] - 0s 122us/step - loss: 0.0624 - accuracy: 0.9833 - val_loss: 0.0486 - val_accuracy: 0.9667\n",
      "Epoch 1212/2500\n",
      "120/120 [==============================] - 0s 112us/step - loss: 0.0624 - accuracy: 0.9833 - val_loss: 0.0510 - val_accuracy: 0.9667\n",
      "Epoch 1213/2500\n",
      "120/120 [==============================] - 0s 103us/step - loss: 0.0614 - accuracy: 0.9833 - val_loss: 0.0487 - val_accuracy: 0.9667\n",
      "Epoch 1214/2500\n",
      "120/120 [==============================] - 0s 114us/step - loss: 0.0627 - accuracy: 0.9833 - val_loss: 0.0538 - val_accuracy: 0.9667\n",
      "Epoch 1215/2500\n",
      "120/120 [==============================] - 0s 120us/step - loss: 0.0645 - accuracy: 0.9833 - val_loss: 0.0459 - val_accuracy: 0.9667\n",
      "Epoch 1216/2500\n",
      "120/120 [==============================] - 0s 109us/step - loss: 0.0629 - accuracy: 0.9833 - val_loss: 0.0533 - val_accuracy: 0.9667\n",
      "Epoch 1217/2500\n",
      "120/120 [==============================] - 0s 110us/step - loss: 0.0625 - accuracy: 0.9833 - val_loss: 0.0554 - val_accuracy: 0.9667\n",
      "Epoch 1218/2500\n",
      "120/120 [==============================] - 0s 114us/step - loss: 0.0620 - accuracy: 0.9833 - val_loss: 0.0464 - val_accuracy: 0.9667\n",
      "Epoch 1219/2500\n",
      "120/120 [==============================] - 0s 101us/step - loss: 0.0622 - accuracy: 0.9833 - val_loss: 0.0489 - val_accuracy: 0.9667\n",
      "Epoch 1220/2500\n",
      "120/120 [==============================] - 0s 104us/step - loss: 0.0646 - accuracy: 0.9750 - val_loss: 0.0451 - val_accuracy: 0.9667\n",
      "Epoch 1221/2500\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0615 - accuracy: 0.9833 - val_loss: 0.0529 - val_accuracy: 0.9667\n",
      "Epoch 1222/2500\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0615 - accuracy: 0.9833 - val_loss: 0.0547 - val_accuracy: 0.9667\n",
      "Epoch 1223/2500\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0616 - accuracy: 0.9833 - val_loss: 0.0509 - val_accuracy: 0.9667\n",
      "Epoch 1224/2500\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.0622 - accuracy: 0.9833 - val_loss: 0.0539 - val_accuracy: 0.9667\n",
      "Epoch 1225/2500\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0613 - accuracy: 0.9833 - val_loss: 0.0496 - val_accuracy: 0.9667\n",
      "Epoch 1226/2500\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.0615 - accuracy: 0.9833 - val_loss: 0.0518 - val_accuracy: 0.9667\n",
      "Epoch 1227/2500\n",
      "120/120 [==============================] - 0s 100us/step - loss: 0.0626 - accuracy: 0.9833 - val_loss: 0.0453 - val_accuracy: 0.9667\n",
      "Epoch 1228/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.0613 - accuracy: 0.9833 - val_loss: 0.0494 - val_accuracy: 0.9667\n",
      "Epoch 1229/2500\n",
      "120/120 [==============================] - 0s 115us/step - loss: 0.0618 - accuracy: 0.9833 - val_loss: 0.0532 - val_accuracy: 0.9667\n",
      "Epoch 1230/2500\n",
      "120/120 [==============================] - 0s 101us/step - loss: 0.0611 - accuracy: 0.9833 - val_loss: 0.0539 - val_accuracy: 0.9667\n",
      "Epoch 1231/2500\n",
      "120/120 [==============================] - 0s 99us/step - loss: 0.0635 - accuracy: 0.9833 - val_loss: 0.0473 - val_accuracy: 0.9667\n",
      "Epoch 1232/2500\n",
      "120/120 [==============================] - 0s 96us/step - loss: 0.0617 - accuracy: 0.9833 - val_loss: 0.0502 - val_accuracy: 0.9667\n",
      "Epoch 1233/2500\n",
      "120/120 [==============================] - 0s 94us/step - loss: 0.0614 - accuracy: 0.9833 - val_loss: 0.0530 - val_accuracy: 0.9667\n",
      "Epoch 1234/2500\n",
      "120/120 [==============================] - 0s 99us/step - loss: 0.0617 - accuracy: 0.9833 - val_loss: 0.0567 - val_accuracy: 0.9667\n",
      "Epoch 1235/2500\n",
      "120/120 [==============================] - 0s 101us/step - loss: 0.0615 - accuracy: 0.9833 - val_loss: 0.0496 - val_accuracy: 0.9667\n",
      "Epoch 1236/2500\n",
      "120/120 [==============================] - 0s 113us/step - loss: 0.0621 - accuracy: 0.9833 - val_loss: 0.0455 - val_accuracy: 0.9667\n",
      "Epoch 1237/2500\n",
      "120/120 [==============================] - 0s 99us/step - loss: 0.0619 - accuracy: 0.9833 - val_loss: 0.0518 - val_accuracy: 0.9667\n",
      "Epoch 1238/2500\n",
      "120/120 [==============================] - 0s 91us/step - loss: 0.0614 - accuracy: 0.9833 - val_loss: 0.0498 - val_accuracy: 0.9667\n",
      "Epoch 1239/2500\n",
      "120/120 [==============================] - 0s 104us/step - loss: 0.0611 - accuracy: 0.9833 - val_loss: 0.0507 - val_accuracy: 0.9667\n",
      "Epoch 1240/2500\n",
      "120/120 [==============================] - 0s 103us/step - loss: 0.0617 - accuracy: 0.9833 - val_loss: 0.0558 - val_accuracy: 0.9667\n",
      "Epoch 1241/2500\n",
      "120/120 [==============================] - 0s 103us/step - loss: 0.0619 - accuracy: 0.9833 - val_loss: 0.0574 - val_accuracy: 0.9667\n",
      "Epoch 1242/2500\n",
      "120/120 [==============================] - 0s 90us/step - loss: 0.0619 - accuracy: 0.9833 - val_loss: 0.0465 - val_accuracy: 0.9667\n",
      "Epoch 1243/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.0615 - accuracy: 0.9833 - val_loss: 0.0482 - val_accuracy: 0.9667\n",
      "Epoch 1244/2500\n",
      "120/120 [==============================] - 0s 128us/step - loss: 0.0610 - accuracy: 0.9833 - val_loss: 0.0479 - val_accuracy: 0.9667\n",
      "Epoch 1245/2500\n",
      "120/120 [==============================] - 0s 101us/step - loss: 0.0621 - accuracy: 0.9833 - val_loss: 0.0456 - val_accuracy: 0.9667\n",
      "Epoch 1246/2500\n",
      "120/120 [==============================] - 0s 102us/step - loss: 0.0603 - accuracy: 0.9833 - val_loss: 0.0490 - val_accuracy: 0.9667\n",
      "Epoch 1247/2500\n",
      "120/120 [==============================] - 0s 124us/step - loss: 0.0619 - accuracy: 0.9833 - val_loss: 0.0565 - val_accuracy: 0.9667\n",
      "Epoch 1248/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.0619 - accuracy: 0.9833 - val_loss: 0.0594 - val_accuracy: 0.9667\n",
      "Epoch 1249/2500\n",
      "120/120 [==============================] - 0s 99us/step - loss: 0.0615 - accuracy: 0.9833 - val_loss: 0.0484 - val_accuracy: 0.9667\n",
      "Epoch 1250/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.0606 - accuracy: 0.9833 - val_loss: 0.0475 - val_accuracy: 0.9667\n",
      "Epoch 1251/2500\n",
      "120/120 [==============================] - 0s 112us/step - loss: 0.0608 - accuracy: 0.9833 - val_loss: 0.0483 - val_accuracy: 0.9667\n",
      "Epoch 1252/2500\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0608 - accuracy: 0.9833 - val_loss: 0.0495 - val_accuracy: 0.9667\n",
      "Epoch 1253/2500\n",
      "120/120 [==============================] - 0s 121us/step - loss: 0.0606 - accuracy: 0.9833 - val_loss: 0.0501 - val_accuracy: 0.9667\n",
      "Epoch 1254/2500\n",
      "120/120 [==============================] - 0s 107us/step - loss: 0.0615 - accuracy: 0.9833 - val_loss: 0.0531 - val_accuracy: 0.9667\n",
      "Epoch 1255/2500\n",
      "120/120 [==============================] - 0s 103us/step - loss: 0.0613 - accuracy: 0.9833 - val_loss: 0.0477 - val_accuracy: 0.9667\n",
      "Epoch 1256/2500\n",
      "120/120 [==============================] - 0s 112us/step - loss: 0.0618 - accuracy: 0.9750 - val_loss: 0.0458 - val_accuracy: 0.9667\n",
      "Epoch 1257/2500\n",
      "120/120 [==============================] - 0s 114us/step - loss: 0.0605 - accuracy: 0.9833 - val_loss: 0.0495 - val_accuracy: 0.9667\n",
      "Epoch 1258/2500\n",
      "120/120 [==============================] - 0s 102us/step - loss: 0.0647 - accuracy: 0.9833 - val_loss: 0.0615 - val_accuracy: 0.9667\n",
      "Epoch 1259/2500\n",
      "120/120 [==============================] - 0s 107us/step - loss: 0.0610 - accuracy: 0.9833 - val_loss: 0.0512 - val_accuracy: 0.9667\n",
      "Epoch 1260/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.0603 - accuracy: 0.9833 - val_loss: 0.0482 - val_accuracy: 0.9667\n",
      "Epoch 1261/2500\n",
      "120/120 [==============================] - 0s 110us/step - loss: 0.0606 - accuracy: 0.9833 - val_loss: 0.0468 - val_accuracy: 0.9667\n",
      "Epoch 1262/2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 105us/step - loss: 0.0605 - accuracy: 0.9833 - val_loss: 0.0472 - val_accuracy: 0.9667\n",
      "Epoch 1263/2500\n",
      "120/120 [==============================] - 0s 116us/step - loss: 0.0611 - accuracy: 0.9833 - val_loss: 0.0490 - val_accuracy: 0.9667\n",
      "Epoch 1264/2500\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0626 - accuracy: 0.9833 - val_loss: 0.0573 - val_accuracy: 0.9667\n",
      "Epoch 1265/2500\n",
      "120/120 [==============================] - 0s 112us/step - loss: 0.0614 - accuracy: 0.9833 - val_loss: 0.0499 - val_accuracy: 0.9667\n",
      "Epoch 1266/2500\n",
      "120/120 [==============================] - 0s 112us/step - loss: 0.0612 - accuracy: 0.9833 - val_loss: 0.0505 - val_accuracy: 0.9667\n",
      "Epoch 1267/2500\n",
      "120/120 [==============================] - 0s 115us/step - loss: 0.0609 - accuracy: 0.9833 - val_loss: 0.0524 - val_accuracy: 0.9667\n",
      "Epoch 1268/2500\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.0605 - accuracy: 0.9833 - val_loss: 0.0509 - val_accuracy: 0.9667\n",
      "Epoch 1269/2500\n",
      "120/120 [==============================] - 0s 98us/step - loss: 0.0615 - accuracy: 0.9833 - val_loss: 0.0460 - val_accuracy: 0.9667\n",
      "Epoch 1270/2500\n",
      "120/120 [==============================] - 0s 102us/step - loss: 0.0622 - accuracy: 0.9833 - val_loss: 0.0495 - val_accuracy: 0.9667\n",
      "Epoch 1271/2500\n",
      "120/120 [==============================] - 0s 112us/step - loss: 0.0608 - accuracy: 0.9833 - val_loss: 0.0518 - val_accuracy: 0.9667\n",
      "Epoch 1272/2500\n",
      "120/120 [==============================] - 0s 115us/step - loss: 0.0608 - accuracy: 0.9833 - val_loss: 0.0530 - val_accuracy: 0.9667\n",
      "Epoch 1273/2500\n",
      "120/120 [==============================] - 0s 123us/step - loss: 0.0606 - accuracy: 0.9833 - val_loss: 0.0506 - val_accuracy: 0.9667\n",
      "Epoch 1274/2500\n",
      "120/120 [==============================] - 0s 114us/step - loss: 0.0603 - accuracy: 0.9833 - val_loss: 0.0479 - val_accuracy: 0.9667\n",
      "Epoch 1275/2500\n",
      "120/120 [==============================] - 0s 128us/step - loss: 0.0606 - accuracy: 0.9833 - val_loss: 0.0485 - val_accuracy: 0.9667\n",
      "Epoch 1276/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.0610 - accuracy: 0.9833 - val_loss: 0.0459 - val_accuracy: 0.9667\n",
      "Epoch 1277/2500\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0610 - accuracy: 0.9833 - val_loss: 0.0500 - val_accuracy: 0.9667\n",
      "Epoch 1278/2500\n",
      "120/120 [==============================] - 0s 102us/step - loss: 0.0605 - accuracy: 0.9833 - val_loss: 0.0480 - val_accuracy: 0.9667\n",
      "Epoch 1279/2500\n",
      "120/120 [==============================] - 0s 100us/step - loss: 0.0622 - accuracy: 0.9750 - val_loss: 0.0438 - val_accuracy: 0.9667\n",
      "Epoch 1280/2500\n",
      "120/120 [==============================] - 0s 112us/step - loss: 0.0620 - accuracy: 0.9833 - val_loss: 0.0552 - val_accuracy: 0.9667\n",
      "Epoch 1281/2500\n",
      "120/120 [==============================] - 0s 109us/step - loss: 0.0618 - accuracy: 0.9833 - val_loss: 0.0559 - val_accuracy: 0.9667\n",
      "Epoch 1282/2500\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.0606 - accuracy: 0.9833 - val_loss: 0.0514 - val_accuracy: 0.9667\n",
      "Epoch 1283/2500\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.0623 - accuracy: 0.9833 - val_loss: 0.0444 - val_accuracy: 0.9667\n",
      "Epoch 1284/2500\n",
      "120/120 [==============================] - 0s 118us/step - loss: 0.0608 - accuracy: 0.9833 - val_loss: 0.0468 - val_accuracy: 0.9667\n",
      "Epoch 1285/2500\n",
      "120/120 [==============================] - 0s 113us/step - loss: 0.0619 - accuracy: 0.9833 - val_loss: 0.0535 - val_accuracy: 0.9667\n",
      "Epoch 1286/2500\n",
      "120/120 [==============================] - 0s 98us/step - loss: 0.0604 - accuracy: 0.9833 - val_loss: 0.0555 - val_accuracy: 0.9667\n",
      "Epoch 1287/2500\n",
      "120/120 [==============================] - 0s 134us/step - loss: 0.0606 - accuracy: 0.9833 - val_loss: 0.0537 - val_accuracy: 0.9667\n",
      "Epoch 1288/2500\n",
      "120/120 [==============================] - 0s 109us/step - loss: 0.0600 - accuracy: 0.9833 - val_loss: 0.0477 - val_accuracy: 0.9667\n",
      "Epoch 1289/2500\n",
      "120/120 [==============================] - 0s 104us/step - loss: 0.0632 - accuracy: 0.9833 - val_loss: 0.0530 - val_accuracy: 0.9667\n",
      "Epoch 1290/2500\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.0618 - accuracy: 0.9833 - val_loss: 0.0438 - val_accuracy: 0.9667\n",
      "Epoch 1291/2500\n",
      "120/120 [==============================] - 0s 114us/step - loss: 0.0608 - accuracy: 0.9750 - val_loss: 0.0476 - val_accuracy: 0.9667\n",
      "Epoch 1292/2500\n",
      "120/120 [==============================] - 0s 110us/step - loss: 0.0613 - accuracy: 0.9833 - val_loss: 0.0505 - val_accuracy: 0.9667\n",
      "Epoch 1293/2500\n",
      "120/120 [==============================] - 0s 104us/step - loss: 0.0620 - accuracy: 0.9833 - val_loss: 0.0456 - val_accuracy: 0.9667\n",
      "Epoch 1294/2500\n",
      "120/120 [==============================] - 0s 120us/step - loss: 0.0612 - accuracy: 0.9833 - val_loss: 0.0530 - val_accuracy: 0.9667\n",
      "Epoch 1295/2500\n",
      "120/120 [==============================] - 0s 103us/step - loss: 0.0637 - accuracy: 0.9833 - val_loss: 0.0603 - val_accuracy: 0.9667\n",
      "Epoch 1296/2500\n",
      "120/120 [==============================] - 0s 102us/step - loss: 0.0601 - accuracy: 0.9833 - val_loss: 0.0467 - val_accuracy: 0.9667\n",
      "Epoch 1297/2500\n",
      "120/120 [==============================] - 0s 103us/step - loss: 0.0617 - accuracy: 0.9833 - val_loss: 0.0470 - val_accuracy: 0.9667\n",
      "Epoch 1298/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.0607 - accuracy: 0.9750 - val_loss: 0.0423 - val_accuracy: 0.9667\n",
      "Epoch 1299/2500\n",
      "120/120 [==============================] - 0s 109us/step - loss: 0.0607 - accuracy: 0.9750 - val_loss: 0.0440 - val_accuracy: 0.9667\n",
      "Epoch 1300/2500\n",
      "120/120 [==============================] - 0s 101us/step - loss: 0.0613 - accuracy: 0.9833 - val_loss: 0.0530 - val_accuracy: 0.9667\n",
      "Epoch 1301/2500\n",
      "120/120 [==============================] - 0s 113us/step - loss: 0.0602 - accuracy: 0.9833 - val_loss: 0.0512 - val_accuracy: 0.9667\n",
      "Epoch 1302/2500\n",
      "120/120 [==============================] - 0s 113us/step - loss: 0.0611 - accuracy: 0.9833 - val_loss: 0.0550 - val_accuracy: 0.9667\n",
      "Epoch 1303/2500\n",
      "120/120 [==============================] - 0s 109us/step - loss: 0.0604 - accuracy: 0.9833 - val_loss: 0.0510 - val_accuracy: 0.9667\n",
      "Epoch 1304/2500\n",
      "120/120 [==============================] - 0s 107us/step - loss: 0.0607 - accuracy: 0.9833 - val_loss: 0.0469 - val_accuracy: 0.9667\n",
      "Epoch 1305/2500\n",
      "120/120 [==============================] - 0s 110us/step - loss: 0.0625 - accuracy: 0.9833 - val_loss: 0.0540 - val_accuracy: 0.9667\n",
      "Epoch 1306/2500\n",
      "120/120 [==============================] - 0s 118us/step - loss: 0.0602 - accuracy: 0.9833 - val_loss: 0.0514 - val_accuracy: 0.9667\n",
      "Epoch 1307/2500\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.0601 - accuracy: 0.9833 - val_loss: 0.0463 - val_accuracy: 0.9667\n",
      "Epoch 1308/2500\n",
      "120/120 [==============================] - 0s 161us/step - loss: 0.0621 - accuracy: 0.9750 - val_loss: 0.0419 - val_accuracy: 0.9667\n",
      "Epoch 1309/2500\n",
      "120/120 [==============================] - 0s 141us/step - loss: 0.0611 - accuracy: 0.9750 - val_loss: 0.0493 - val_accuracy: 0.9667\n",
      "Epoch 1310/2500\n",
      "120/120 [==============================] - 0s 123us/step - loss: 0.0598 - accuracy: 0.9833 - val_loss: 0.0506 - val_accuracy: 0.9667\n",
      "Epoch 1311/2500\n",
      "120/120 [==============================] - 0s 116us/step - loss: 0.0605 - accuracy: 0.9833 - val_loss: 0.0482 - val_accuracy: 0.9667\n",
      "Epoch 1312/2500\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0596 - accuracy: 0.9833 - val_loss: 0.0515 - val_accuracy: 0.9667\n",
      "Epoch 1313/2500\n",
      "120/120 [==============================] - 0s 126us/step - loss: 0.0608 - accuracy: 0.9833 - val_loss: 0.0557 - val_accuracy: 0.9667\n",
      "Epoch 1314/2500\n",
      "120/120 [==============================] - 0s 142us/step - loss: 0.0610 - accuracy: 0.9833 - val_loss: 0.0570 - val_accuracy: 0.9667\n",
      "Epoch 1315/2500\n",
      "120/120 [==============================] - 0s 115us/step - loss: 0.0597 - accuracy: 0.9833 - val_loss: 0.0498 - val_accuracy: 0.9667\n",
      "Epoch 1316/2500\n",
      "120/120 [==============================] - 0s 147us/step - loss: 0.0597 - accuracy: 0.9833 - val_loss: 0.0440 - val_accuracy: 0.9667\n",
      "Epoch 1317/2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 124us/step - loss: 0.0609 - accuracy: 0.9833 - val_loss: 0.0455 - val_accuracy: 0.9667\n",
      "Epoch 1318/2500\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0606 - accuracy: 0.9750 - val_loss: 0.0449 - val_accuracy: 0.9667\n",
      "Epoch 1319/2500\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0624 - accuracy: 0.9833 - val_loss: 0.0552 - val_accuracy: 0.9667\n",
      "Epoch 1320/2500\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0600 - accuracy: 0.9833 - val_loss: 0.0538 - val_accuracy: 0.9667\n",
      "Epoch 1321/2500\n",
      "120/120 [==============================] - 0s 115us/step - loss: 0.0606 - accuracy: 0.9833 - val_loss: 0.0469 - val_accuracy: 0.9667\n",
      "Epoch 1322/2500\n",
      "120/120 [==============================] - 0s 112us/step - loss: 0.0612 - accuracy: 0.9750 - val_loss: 0.0439 - val_accuracy: 0.9667\n",
      "Epoch 1323/2500\n",
      "120/120 [==============================] - 0s 122us/step - loss: 0.0596 - accuracy: 0.9833 - val_loss: 0.0480 - val_accuracy: 0.9667\n",
      "Epoch 1324/2500\n",
      "120/120 [==============================] - 0s 101us/step - loss: 0.0605 - accuracy: 0.9833 - val_loss: 0.0493 - val_accuracy: 0.9667\n",
      "Epoch 1325/2500\n",
      "120/120 [==============================] - 0s 107us/step - loss: 0.0599 - accuracy: 0.9833 - val_loss: 0.0542 - val_accuracy: 0.9667\n",
      "Epoch 1326/2500\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.0606 - accuracy: 0.9833 - val_loss: 0.0499 - val_accuracy: 0.9667\n",
      "Epoch 1327/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.0603 - accuracy: 0.9833 - val_loss: 0.0490 - val_accuracy: 0.9667\n",
      "Epoch 1328/2500\n",
      "120/120 [==============================] - 0s 112us/step - loss: 0.0620 - accuracy: 0.9833 - val_loss: 0.0569 - val_accuracy: 0.9667\n",
      "Epoch 1329/2500\n",
      "120/120 [==============================] - 0s 126us/step - loss: 0.0615 - accuracy: 0.9833 - val_loss: 0.0468 - val_accuracy: 0.9667\n",
      "Epoch 1330/2500\n",
      "120/120 [==============================] - 0s 113us/step - loss: 0.0601 - accuracy: 0.9833 - val_loss: 0.0484 - val_accuracy: 0.9667\n",
      "Epoch 1331/2500\n",
      "120/120 [==============================] - 0s 113us/step - loss: 0.0598 - accuracy: 0.9833 - val_loss: 0.0505 - val_accuracy: 0.9667\n",
      "Epoch 1332/2500\n",
      "120/120 [==============================] - 0s 113us/step - loss: 0.0604 - accuracy: 0.9833 - val_loss: 0.0460 - val_accuracy: 0.9667\n",
      "Epoch 1333/2500\n",
      "120/120 [==============================] - 0s 121us/step - loss: 0.0602 - accuracy: 0.9833 - val_loss: 0.0479 - val_accuracy: 0.9667\n",
      "Epoch 1334/2500\n",
      "120/120 [==============================] - 0s 119us/step - loss: 0.0596 - accuracy: 0.9833 - val_loss: 0.0514 - val_accuracy: 0.9667\n",
      "Epoch 1335/2500\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0620 - accuracy: 0.9833 - val_loss: 0.0461 - val_accuracy: 0.9667\n",
      "Epoch 1336/2500\n",
      "120/120 [==============================] - 0s 132us/step - loss: 0.0607 - accuracy: 0.9833 - val_loss: 0.0551 - val_accuracy: 0.9667\n",
      "Epoch 1337/2500\n",
      "120/120 [==============================] - 0s 121us/step - loss: 0.0600 - accuracy: 0.9833 - val_loss: 0.0531 - val_accuracy: 0.9667\n",
      "Epoch 1338/2500\n",
      "120/120 [==============================] - 0s 126us/step - loss: 0.0600 - accuracy: 0.9833 - val_loss: 0.0543 - val_accuracy: 0.9667\n",
      "Epoch 1339/2500\n",
      "120/120 [==============================] - 0s 131us/step - loss: 0.0601 - accuracy: 0.9833 - val_loss: 0.0487 - val_accuracy: 0.9667\n",
      "Epoch 1340/2500\n",
      "120/120 [==============================] - 0s 114us/step - loss: 0.0596 - accuracy: 0.9833 - val_loss: 0.0477 - val_accuracy: 0.9667\n",
      "Epoch 1341/2500\n",
      "120/120 [==============================] - 0s 110us/step - loss: 0.0609 - accuracy: 0.9833 - val_loss: 0.0426 - val_accuracy: 0.9667\n",
      "Epoch 1342/2500\n",
      "120/120 [==============================] - 0s 121us/step - loss: 0.0600 - accuracy: 0.9833 - val_loss: 0.0452 - val_accuracy: 0.9667\n",
      "Epoch 1343/2500\n",
      "120/120 [==============================] - 0s 99us/step - loss: 0.0600 - accuracy: 0.9833 - val_loss: 0.0531 - val_accuracy: 0.9667\n",
      "Epoch 1344/2500\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.0606 - accuracy: 0.9833 - val_loss: 0.0560 - val_accuracy: 0.9667\n",
      "Epoch 1345/2500\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0594 - accuracy: 0.9833 - val_loss: 0.0498 - val_accuracy: 0.9667\n",
      "Epoch 1346/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.0608 - accuracy: 0.9833 - val_loss: 0.0512 - val_accuracy: 0.9667\n",
      "Epoch 1347/2500\n",
      "120/120 [==============================] - 0s 107us/step - loss: 0.0618 - accuracy: 0.9750 - val_loss: 0.0422 - val_accuracy: 0.9667\n",
      "Epoch 1348/2500\n",
      "120/120 [==============================] - 0s 127us/step - loss: 0.0597 - accuracy: 0.9750 - val_loss: 0.0452 - val_accuracy: 0.9667\n",
      "Epoch 1349/2500\n",
      "120/120 [==============================] - 0s 126us/step - loss: 0.0604 - accuracy: 0.9833 - val_loss: 0.0535 - val_accuracy: 0.9667\n",
      "Epoch 1350/2500\n",
      "120/120 [==============================] - 0s 112us/step - loss: 0.0607 - accuracy: 0.9833 - val_loss: 0.0540 - val_accuracy: 0.9667\n",
      "Epoch 1351/2500\n",
      "120/120 [==============================] - 0s 107us/step - loss: 0.0604 - accuracy: 0.9833 - val_loss: 0.0479 - val_accuracy: 0.9667\n",
      "Epoch 1352/2500\n",
      "120/120 [==============================] - 0s 152us/step - loss: 0.0592 - accuracy: 0.9833 - val_loss: 0.0467 - val_accuracy: 0.9667\n",
      "Epoch 1353/2500\n",
      "120/120 [==============================] - 0s 138us/step - loss: 0.0636 - accuracy: 0.9833 - val_loss: 0.0567 - val_accuracy: 0.9667\n",
      "Epoch 1354/2500\n",
      "120/120 [==============================] - 0s 142us/step - loss: 0.0597 - accuracy: 0.9833 - val_loss: 0.0464 - val_accuracy: 0.9667\n",
      "Epoch 1355/2500\n",
      "120/120 [==============================] - 0s 129us/step - loss: 0.0606 - accuracy: 0.9750 - val_loss: 0.0442 - val_accuracy: 0.9667\n",
      "Epoch 1356/2500\n",
      "120/120 [==============================] - 0s 120us/step - loss: 0.0597 - accuracy: 0.9833 - val_loss: 0.0456 - val_accuracy: 0.9667\n",
      "Epoch 1357/2500\n",
      "120/120 [==============================] - 0s 126us/step - loss: 0.0600 - accuracy: 0.9833 - val_loss: 0.0471 - val_accuracy: 0.9667\n",
      "Epoch 1358/2500\n",
      "120/120 [==============================] - 0s 135us/step - loss: 0.0597 - accuracy: 0.9833 - val_loss: 0.0553 - val_accuracy: 0.9667\n",
      "Epoch 1359/2500\n",
      "120/120 [==============================] - 0s 114us/step - loss: 0.0605 - accuracy: 0.9833 - val_loss: 0.0500 - val_accuracy: 0.9667\n",
      "Epoch 1360/2500\n",
      "120/120 [==============================] - 0s 120us/step - loss: 0.0621 - accuracy: 0.9833 - val_loss: 0.0570 - val_accuracy: 0.9667\n",
      "Epoch 1361/2500\n",
      "120/120 [==============================] - 0s 123us/step - loss: 0.0596 - accuracy: 0.9833 - val_loss: 0.0499 - val_accuracy: 0.9667\n",
      "Epoch 1362/2500\n",
      "120/120 [==============================] - 0s 115us/step - loss: 0.0611 - accuracy: 0.9833 - val_loss: 0.0537 - val_accuracy: 0.9667\n",
      "Epoch 1363/2500\n",
      "120/120 [==============================] - 0s 107us/step - loss: 0.0599 - accuracy: 0.9833 - val_loss: 0.0436 - val_accuracy: 0.9667\n",
      "Epoch 1364/2500\n",
      "120/120 [==============================] - 0s 135us/step - loss: 0.0608 - accuracy: 0.9750 - val_loss: 0.0412 - val_accuracy: 0.9667\n",
      "Epoch 1365/2500\n",
      "120/120 [==============================] - 0s 107us/step - loss: 0.0599 - accuracy: 0.9750 - val_loss: 0.0442 - val_accuracy: 0.9667\n",
      "Epoch 1366/2500\n",
      "120/120 [==============================] - 0s 105us/step - loss: 0.0605 - accuracy: 0.9833 - val_loss: 0.0547 - val_accuracy: 0.9667\n",
      "Epoch 1367/2500\n",
      "120/120 [==============================] - 0s 124us/step - loss: 0.0594 - accuracy: 0.9833 - val_loss: 0.0535 - val_accuracy: 0.9667\n",
      "Epoch 1368/2500\n",
      "120/120 [==============================] - 0s 107us/step - loss: 0.0600 - accuracy: 0.9833 - val_loss: 0.0471 - val_accuracy: 0.9667\n",
      "Epoch 1369/2500\n",
      "120/120 [==============================] - 0s 100us/step - loss: 0.0594 - accuracy: 0.9833 - val_loss: 0.0472 - val_accuracy: 0.9667\n",
      "Epoch 1370/2500\n",
      "120/120 [==============================] - 0s 112us/step - loss: 0.0592 - accuracy: 0.9833 - val_loss: 0.0488 - val_accuracy: 0.9667\n",
      "Epoch 1371/2500\n",
      "120/120 [==============================] - 0s 124us/step - loss: 0.0597 - accuracy: 0.9833 - val_loss: 0.0527 - val_accuracy: 0.9667\n",
      "Epoch 1372/2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 116us/step - loss: 0.0603 - accuracy: 0.9833 - val_loss: 0.0475 - val_accuracy: 0.9667\n",
      "Epoch 1373/2500\n",
      "120/120 [==============================] - 0s 120us/step - loss: 0.0597 - accuracy: 0.9833 - val_loss: 0.0495 - val_accuracy: 0.9667\n",
      "Epoch 1374/2500\n",
      "120/120 [==============================] - 0s 131us/step - loss: 0.0593 - accuracy: 0.9833 - val_loss: 0.0496 - val_accuracy: 0.9667\n",
      "Epoch 1375/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.0610 - accuracy: 0.9833 - val_loss: 0.0446 - val_accuracy: 0.9667\n",
      "Epoch 1376/2500\n",
      "120/120 [==============================] - 0s 115us/step - loss: 0.0595 - accuracy: 0.9833 - val_loss: 0.0486 - val_accuracy: 0.9667\n",
      "Epoch 1377/2500\n",
      "120/120 [==============================] - 0s 128us/step - loss: 0.0602 - accuracy: 0.9833 - val_loss: 0.0470 - val_accuracy: 0.9667\n",
      "Epoch 1378/2500\n",
      "120/120 [==============================] - 0s 119us/step - loss: 0.0594 - accuracy: 0.9833 - val_loss: 0.0517 - val_accuracy: 0.9667\n",
      "Epoch 1379/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.0594 - accuracy: 0.9833 - val_loss: 0.0530 - val_accuracy: 0.9667\n",
      "Epoch 1380/2500\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.0280 - accuracy: 1.00 - 0s 121us/step - loss: 0.0625 - accuracy: 0.9833 - val_loss: 0.0427 - val_accuracy: 0.9667\n",
      "Epoch 1381/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.0591 - accuracy: 0.9750 - val_loss: 0.0470 - val_accuracy: 0.9667\n",
      "Epoch 1382/2500\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0596 - accuracy: 0.9833 - val_loss: 0.0508 - val_accuracy: 0.9667\n",
      "Epoch 1383/2500\n",
      "120/120 [==============================] - 0s 118us/step - loss: 0.0592 - accuracy: 0.9833 - val_loss: 0.0500 - val_accuracy: 0.9667\n",
      "Epoch 1384/2500\n",
      "120/120 [==============================] - 0s 109us/step - loss: 0.0592 - accuracy: 0.9833 - val_loss: 0.0487 - val_accuracy: 0.9667\n",
      "Epoch 1385/2500\n",
      "120/120 [==============================] - 0s 102us/step - loss: 0.0599 - accuracy: 0.9833 - val_loss: 0.0471 - val_accuracy: 0.9667\n",
      "Epoch 1386/2500\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0600 - accuracy: 0.9833 - val_loss: 0.0557 - val_accuracy: 0.9667\n",
      "Epoch 1387/2500\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.0592 - accuracy: 0.9833 - val_loss: 0.0510 - val_accuracy: 0.9667\n",
      "Epoch 1388/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.0589 - accuracy: 0.9833 - val_loss: 0.0496 - val_accuracy: 0.9667\n",
      "Epoch 1389/2500\n",
      "120/120 [==============================] - 0s 116us/step - loss: 0.0591 - accuracy: 0.9833 - val_loss: 0.0499 - val_accuracy: 0.9667\n",
      "Epoch 1390/2500\n",
      "120/120 [==============================] - 0s 135us/step - loss: 0.0597 - accuracy: 0.9833 - val_loss: 0.0449 - val_accuracy: 0.9667\n",
      "Epoch 1391/2500\n",
      "120/120 [==============================] - 0s 119us/step - loss: 0.0588 - accuracy: 0.9833 - val_loss: 0.0481 - val_accuracy: 0.9667\n",
      "Epoch 1392/2500\n",
      "120/120 [==============================] - 0s 116us/step - loss: 0.0592 - accuracy: 0.9833 - val_loss: 0.0493 - val_accuracy: 0.9667\n",
      "Epoch 1393/2500\n",
      "120/120 [==============================] - 0s 127us/step - loss: 0.0615 - accuracy: 0.9833 - val_loss: 0.0541 - val_accuracy: 0.9667\n",
      "Epoch 1394/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.0587 - accuracy: 0.9833 - val_loss: 0.0508 - val_accuracy: 0.9667\n",
      "Epoch 1395/2500\n",
      "120/120 [==============================] - 0s 95us/step - loss: 0.0639 - accuracy: 0.9750 - val_loss: 0.0404 - val_accuracy: 0.9667\n",
      "Epoch 1396/2500\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.0595 - accuracy: 0.9750 - val_loss: 0.0467 - val_accuracy: 0.9667\n",
      "Epoch 1397/2500\n",
      "120/120 [==============================] - 0s 114us/step - loss: 0.0617 - accuracy: 0.9833 - val_loss: 0.0563 - val_accuracy: 0.9667\n",
      "Epoch 1398/2500\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0597 - accuracy: 0.9833 - val_loss: 0.0566 - val_accuracy: 0.9667\n",
      "Epoch 1399/2500\n",
      "120/120 [==============================] - 0s 109us/step - loss: 0.0622 - accuracy: 0.9750 - val_loss: 0.0434 - val_accuracy: 0.9667\n",
      "Epoch 1400/2500\n",
      "120/120 [==============================] - 0s 126us/step - loss: 0.0598 - accuracy: 0.9750 - val_loss: 0.0422 - val_accuracy: 0.9667\n",
      "Epoch 1401/2500\n",
      "120/120 [==============================] - 0s 131us/step - loss: 0.0610 - accuracy: 0.9750 - val_loss: 0.0531 - val_accuracy: 0.9667\n",
      "Epoch 1402/2500\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.0592 - accuracy: 0.9833 - val_loss: 0.0525 - val_accuracy: 0.9667\n",
      "Epoch 1403/2500\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.0592 - accuracy: 0.9833 - val_loss: 0.0480 - val_accuracy: 0.9667\n",
      "Epoch 1404/2500\n",
      "120/120 [==============================] - 0s 126us/step - loss: 0.0600 - accuracy: 0.9833 - val_loss: 0.0528 - val_accuracy: 0.9667\n",
      "Epoch 1405/2500\n",
      "120/120 [==============================] - 0s 118us/step - loss: 0.0597 - accuracy: 0.9833 - val_loss: 0.0536 - val_accuracy: 0.9667\n",
      "Epoch 1406/2500\n",
      "120/120 [==============================] - 0s 129us/step - loss: 0.0597 - accuracy: 0.9833 - val_loss: 0.0449 - val_accuracy: 0.9667\n",
      "Epoch 1407/2500\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0590 - accuracy: 0.9833 - val_loss: 0.0449 - val_accuracy: 0.9667\n",
      "Epoch 1408/2500\n",
      "120/120 [==============================] - 0s 118us/step - loss: 0.0586 - accuracy: 0.9833 - val_loss: 0.0473 - val_accuracy: 0.9667\n",
      "Epoch 1409/2500\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.0591 - accuracy: 0.9833 - val_loss: 0.0478 - val_accuracy: 0.9667\n",
      "Epoch 1410/2500\n",
      "120/120 [==============================] - 0s 112us/step - loss: 0.0605 - accuracy: 0.9833 - val_loss: 0.0559 - val_accuracy: 0.9667\n",
      "Epoch 1411/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.0594 - accuracy: 0.9833 - val_loss: 0.0535 - val_accuracy: 0.9667\n",
      "Epoch 1412/2500\n",
      "120/120 [==============================] - 0s 114us/step - loss: 0.0583 - accuracy: 0.9833 - val_loss: 0.0501 - val_accuracy: 0.9667\n",
      "Epoch 1413/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.0583 - accuracy: 0.9833 - val_loss: 0.0444 - val_accuracy: 0.9667\n",
      "Epoch 1414/2500\n",
      "120/120 [==============================] - 0s 115us/step - loss: 0.0600 - accuracy: 0.9833 - val_loss: 0.0490 - val_accuracy: 0.9667\n",
      "Epoch 1415/2500\n",
      "120/120 [==============================] - 0s 130us/step - loss: 0.0587 - accuracy: 0.9833 - val_loss: 0.0449 - val_accuracy: 0.9667\n",
      "Epoch 1416/2500\n",
      "120/120 [==============================] - 0s 127us/step - loss: 0.0596 - accuracy: 0.9833 - val_loss: 0.0477 - val_accuracy: 0.9667\n",
      "Epoch 1417/2500\n",
      "120/120 [==============================] - 0s 114us/step - loss: 0.0623 - accuracy: 0.9833 - val_loss: 0.0561 - val_accuracy: 0.9667\n",
      "Epoch 1418/2500\n",
      "120/120 [==============================] - 0s 122us/step - loss: 0.0588 - accuracy: 0.9833 - val_loss: 0.0459 - val_accuracy: 0.9667\n",
      "Epoch 1419/2500\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.0592 - accuracy: 0.9833 - val_loss: 0.0453 - val_accuracy: 0.9667\n",
      "Epoch 1420/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.0587 - accuracy: 0.9750 - val_loss: 0.0448 - val_accuracy: 0.9667\n",
      "Epoch 1421/2500\n",
      "120/120 [==============================] - 0s 120us/step - loss: 0.0608 - accuracy: 0.9833 - val_loss: 0.0497 - val_accuracy: 0.9667\n",
      "Epoch 1422/2500\n",
      "120/120 [==============================] - 0s 145us/step - loss: 0.0622 - accuracy: 0.9750 - val_loss: 0.0424 - val_accuracy: 0.9667\n",
      "Epoch 1423/2500\n",
      "120/120 [==============================] - 0s 118us/step - loss: 0.0597 - accuracy: 0.9833 - val_loss: 0.0516 - val_accuracy: 0.9667\n",
      "Epoch 1424/2500\n",
      "120/120 [==============================] - 0s 102us/step - loss: 0.0588 - accuracy: 0.9833 - val_loss: 0.0490 - val_accuracy: 0.9667\n",
      "Epoch 1425/2500\n",
      "120/120 [==============================] - 0s 114us/step - loss: 0.0597 - accuracy: 0.9833 - val_loss: 0.0463 - val_accuracy: 0.9667\n",
      "Epoch 1426/2500\n",
      "120/120 [==============================] - 0s 115us/step - loss: 0.0587 - accuracy: 0.9833 - val_loss: 0.0475 - val_accuracy: 0.9667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1427/2500\n",
      "120/120 [==============================] - 0s 119us/step - loss: 0.0586 - accuracy: 0.9833 - val_loss: 0.0489 - val_accuracy: 0.9667\n",
      "Epoch 1428/2500\n",
      "120/120 [==============================] - 0s 119us/step - loss: 0.0583 - accuracy: 0.9833 - val_loss: 0.0517 - val_accuracy: 0.9667\n",
      "Epoch 1429/2500\n",
      "120/120 [==============================] - 0s 116us/step - loss: 0.0590 - accuracy: 0.9833 - val_loss: 0.0524 - val_accuracy: 0.9667\n",
      "Epoch 1430/2500\n",
      "120/120 [==============================] - 0s 113us/step - loss: 0.0593 - accuracy: 0.9833 - val_loss: 0.0560 - val_accuracy: 0.9667\n",
      "Epoch 1431/2500\n",
      "120/120 [==============================] - 0s 124us/step - loss: 0.0595 - accuracy: 0.9833 - val_loss: 0.0551 - val_accuracy: 0.9667\n",
      "Epoch 1432/2500\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0591 - accuracy: 0.9833 - val_loss: 0.0465 - val_accuracy: 0.9667\n",
      "Epoch 1433/2500\n",
      "120/120 [==============================] - 0s 105us/step - loss: 0.0596 - accuracy: 0.9833 - val_loss: 0.0507 - val_accuracy: 0.9667\n",
      "Epoch 1434/2500\n",
      "120/120 [==============================] - 0s 134us/step - loss: 0.0613 - accuracy: 0.9750 - val_loss: 0.0427 - val_accuracy: 0.9667\n",
      "Epoch 1435/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.0583 - accuracy: 0.9750 - val_loss: 0.0456 - val_accuracy: 0.9667\n",
      "Epoch 1436/2500\n",
      "120/120 [==============================] - 0s 110us/step - loss: 0.0583 - accuracy: 0.9833 - val_loss: 0.0497 - val_accuracy: 0.9667\n",
      "Epoch 1437/2500\n",
      "120/120 [==============================] - 0s 123us/step - loss: 0.0605 - accuracy: 0.9833 - val_loss: 0.0470 - val_accuracy: 0.9667\n",
      "Epoch 1438/2500\n",
      "120/120 [==============================] - 0s 112us/step - loss: 0.0586 - accuracy: 0.9833 - val_loss: 0.0496 - val_accuracy: 0.9667\n",
      "Epoch 1439/2500\n",
      "120/120 [==============================] - 0s 109us/step - loss: 0.0594 - accuracy: 0.9833 - val_loss: 0.0564 - val_accuracy: 0.9667\n",
      "Epoch 1440/2500\n",
      "120/120 [==============================] - 0s 124us/step - loss: 0.0611 - accuracy: 0.9833 - val_loss: 0.0472 - val_accuracy: 0.9667\n",
      "Epoch 1441/2500\n",
      "120/120 [==============================] - 0s 119us/step - loss: 0.0584 - accuracy: 0.9833 - val_loss: 0.0489 - val_accuracy: 0.9667\n",
      "Epoch 1442/2500\n",
      "120/120 [==============================] - 0s 114us/step - loss: 0.0587 - accuracy: 0.9833 - val_loss: 0.0502 - val_accuracy: 0.9667\n",
      "Epoch 1443/2500\n",
      "120/120 [==============================] - 0s 112us/step - loss: 0.0595 - accuracy: 0.9833 - val_loss: 0.0453 - val_accuracy: 0.9667\n",
      "Epoch 1444/2500\n",
      "120/120 [==============================] - 0s 120us/step - loss: 0.0612 - accuracy: 0.9750 - val_loss: 0.0417 - val_accuracy: 0.9667\n",
      "Epoch 1445/2500\n",
      "120/120 [==============================] - 0s 112us/step - loss: 0.0589 - accuracy: 0.9750 - val_loss: 0.0458 - val_accuracy: 0.9667\n",
      "Epoch 1446/2500\n",
      "120/120 [==============================] - 0s 107us/step - loss: 0.0590 - accuracy: 0.9833 - val_loss: 0.0583 - val_accuracy: 0.9667\n",
      "Epoch 1447/2500\n",
      "120/120 [==============================] - 0s 134us/step - loss: 0.0611 - accuracy: 0.9833 - val_loss: 0.0609 - val_accuracy: 0.9667\n",
      "Epoch 1448/2500\n",
      "120/120 [==============================] - 0s 116us/step - loss: 0.0591 - accuracy: 0.9833 - val_loss: 0.0561 - val_accuracy: 0.9667\n",
      "Epoch 1449/2500\n",
      "120/120 [==============================] - 0s 112us/step - loss: 0.0575 - accuracy: 0.9833 - val_loss: 0.0470 - val_accuracy: 0.9667\n",
      "Epoch 1450/2500\n",
      "120/120 [==============================] - 0s 128us/step - loss: 0.0590 - accuracy: 0.9833 - val_loss: 0.0413 - val_accuracy: 0.9667\n",
      "Epoch 1451/2500\n",
      "120/120 [==============================] - 0s 118us/step - loss: 0.0592 - accuracy: 0.9833 - val_loss: 0.0453 - val_accuracy: 0.9667\n",
      "Epoch 1452/2500\n",
      "120/120 [==============================] - 0s 167us/step - loss: 0.0597 - accuracy: 0.9750 - val_loss: 0.0439 - val_accuracy: 0.9667\n",
      "Epoch 1453/2500\n",
      "120/120 [==============================] - 0s 160us/step - loss: 0.0582 - accuracy: 0.9833 - val_loss: 0.0463 - val_accuracy: 0.9667\n",
      "Epoch 1454/2500\n",
      "120/120 [==============================] - 0s 124us/step - loss: 0.0588 - accuracy: 0.9833 - val_loss: 0.0475 - val_accuracy: 0.9667\n",
      "Epoch 1455/2500\n",
      "120/120 [==============================] - 0s 121us/step - loss: 0.0578 - accuracy: 0.9833 - val_loss: 0.0530 - val_accuracy: 0.9667\n",
      "Epoch 1456/2500\n",
      "120/120 [==============================] - 0s 121us/step - loss: 0.0583 - accuracy: 0.9833 - val_loss: 0.0547 - val_accuracy: 0.9667\n",
      "Epoch 1457/2500\n",
      "120/120 [==============================] - 0s 123us/step - loss: 0.0588 - accuracy: 0.9833 - val_loss: 0.0510 - val_accuracy: 0.9667\n",
      "Epoch 1458/2500\n",
      "120/120 [==============================] - 0s 119us/step - loss: 0.0584 - accuracy: 0.9833 - val_loss: 0.0520 - val_accuracy: 0.9667\n",
      "Epoch 1459/2500\n",
      "120/120 [==============================] - 0s 140us/step - loss: 0.0584 - accuracy: 0.9833 - val_loss: 0.0469 - val_accuracy: 0.9667\n",
      "Epoch 1460/2500\n",
      "120/120 [==============================] - 0s 118us/step - loss: 0.0593 - accuracy: 0.9750 - val_loss: 0.0437 - val_accuracy: 0.9667\n",
      "Epoch 1461/2500\n",
      "120/120 [==============================] - 0s 115us/step - loss: 0.0606 - accuracy: 0.9833 - val_loss: 0.0529 - val_accuracy: 0.9667\n",
      "Epoch 1462/2500\n",
      "120/120 [==============================] - 0s 109us/step - loss: 0.0583 - accuracy: 0.9833 - val_loss: 0.0492 - val_accuracy: 0.9667\n",
      "Epoch 1463/2500\n",
      "120/120 [==============================] - 0s 124us/step - loss: 0.0594 - accuracy: 0.9833 - val_loss: 0.0436 - val_accuracy: 0.9667\n",
      "Epoch 1464/2500\n",
      "120/120 [==============================] - 0s 116us/step - loss: 0.0612 - accuracy: 0.9833 - val_loss: 0.0549 - val_accuracy: 0.9667\n",
      "Epoch 1465/2500\n",
      "120/120 [==============================] - 0s 146us/step - loss: 0.0588 - accuracy: 0.9833 - val_loss: 0.0536 - val_accuracy: 0.9667\n",
      "Epoch 1466/2500\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0589 - accuracy: 0.9833 - val_loss: 0.0438 - val_accuracy: 0.9667\n",
      "Epoch 1467/2500\n",
      "120/120 [==============================] - 0s 109us/step - loss: 0.0601 - accuracy: 0.9750 - val_loss: 0.0412 - val_accuracy: 0.9667\n",
      "Epoch 1468/2500\n",
      "120/120 [==============================] - 0s 119us/step - loss: 0.0592 - accuracy: 0.9750 - val_loss: 0.0443 - val_accuracy: 0.9667\n",
      "Epoch 1469/2500\n",
      "120/120 [==============================] - 0s 115us/step - loss: 0.0587 - accuracy: 0.9833 - val_loss: 0.0542 - val_accuracy: 0.9667\n",
      "Epoch 1470/2500\n",
      "120/120 [==============================] - 0s 113us/step - loss: 0.0581 - accuracy: 0.9833 - val_loss: 0.0527 - val_accuracy: 0.9667\n",
      "Epoch 1471/2500\n",
      "120/120 [==============================] - 0s 124us/step - loss: 0.0584 - accuracy: 0.9833 - val_loss: 0.0524 - val_accuracy: 0.9667\n",
      "Epoch 1472/2500\n",
      "120/120 [==============================] - 0s 123us/step - loss: 0.0579 - accuracy: 0.9833 - val_loss: 0.0485 - val_accuracy: 0.9667\n",
      "Epoch 1473/2500\n",
      "120/120 [==============================] - 0s 109us/step - loss: 0.0591 - accuracy: 0.9833 - val_loss: 0.0504 - val_accuracy: 0.9667\n",
      "Epoch 1474/2500\n",
      "120/120 [==============================] - 0s 114us/step - loss: 0.0606 - accuracy: 0.9833 - val_loss: 0.0411 - val_accuracy: 0.9667\n",
      "Epoch 1475/2500\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0584 - accuracy: 0.9750 - val_loss: 0.0438 - val_accuracy: 0.9667\n",
      "Epoch 1476/2500\n",
      "120/120 [==============================] - 0s 113us/step - loss: 0.0579 - accuracy: 0.9833 - val_loss: 0.0476 - val_accuracy: 0.9667\n",
      "Epoch 1477/2500\n",
      "120/120 [==============================] - 0s 121us/step - loss: 0.0584 - accuracy: 0.9833 - val_loss: 0.0485 - val_accuracy: 0.9667\n",
      "Epoch 1478/2500\n",
      "120/120 [==============================] - 0s 127us/step - loss: 0.0611 - accuracy: 0.9833 - val_loss: 0.0594 - val_accuracy: 0.9667\n",
      "Epoch 1479/2500\n",
      "120/120 [==============================] - 0s 116us/step - loss: 0.0583 - accuracy: 0.9833 - val_loss: 0.0511 - val_accuracy: 0.9667\n",
      "Epoch 1480/2500\n",
      "120/120 [==============================] - 0s 109us/step - loss: 0.0584 - accuracy: 0.9833 - val_loss: 0.0457 - val_accuracy: 0.9667\n",
      "Epoch 1481/2500\n",
      "120/120 [==============================] - 0s 114us/step - loss: 0.0582 - accuracy: 0.9833 - val_loss: 0.0459 - val_accuracy: 0.9667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1482/2500\n",
      "120/120 [==============================] - 0s 124us/step - loss: 0.0592 - accuracy: 0.9833 - val_loss: 0.0481 - val_accuracy: 0.9667\n",
      "Epoch 1483/2500\n",
      "120/120 [==============================] - 0s 121us/step - loss: 0.0577 - accuracy: 0.9833 - val_loss: 0.0456 - val_accuracy: 0.9667\n",
      "Epoch 1484/2500\n",
      "120/120 [==============================] - 0s 116us/step - loss: 0.0585 - accuracy: 0.9833 - val_loss: 0.0495 - val_accuracy: 0.9667\n",
      "Epoch 1485/2500\n",
      "120/120 [==============================] - 0s 103us/step - loss: 0.0579 - accuracy: 0.9833 - val_loss: 0.0461 - val_accuracy: 0.9667\n",
      "Epoch 1486/2500\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0594 - accuracy: 0.9750 - val_loss: 0.0423 - val_accuracy: 0.9667\n",
      "Epoch 1487/2500\n",
      "120/120 [==============================] - 0s 115us/step - loss: 0.0584 - accuracy: 0.9750 - val_loss: 0.0464 - val_accuracy: 0.9667\n",
      "Epoch 1488/2500\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.0579 - accuracy: 0.9833 - val_loss: 0.0496 - val_accuracy: 0.9667\n",
      "Epoch 1489/2500\n",
      "120/120 [==============================] - 0s 109us/step - loss: 0.0585 - accuracy: 0.9833 - val_loss: 0.0484 - val_accuracy: 0.9667\n",
      "Epoch 1490/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.0577 - accuracy: 0.9833 - val_loss: 0.0521 - val_accuracy: 0.9667\n",
      "Epoch 1491/2500\n",
      "120/120 [==============================] - 0s 122us/step - loss: 0.0586 - accuracy: 0.9833 - val_loss: 0.0480 - val_accuracy: 0.9667\n",
      "Epoch 1492/2500\n",
      "120/120 [==============================] - 0s 120us/step - loss: 0.0626 - accuracy: 0.9833 - val_loss: 0.0581 - val_accuracy: 0.9667\n",
      "Epoch 1493/2500\n",
      "120/120 [==============================] - 0s 121us/step - loss: 0.0617 - accuracy: 0.9750 - val_loss: 0.0427 - val_accuracy: 0.9667\n",
      "Epoch 1494/2500\n",
      "120/120 [==============================] - 0s 134us/step - loss: 0.0600 - accuracy: 0.9750 - val_loss: 0.0498 - val_accuracy: 0.9667\n",
      "Epoch 1495/2500\n",
      "120/120 [==============================] - 0s 122us/step - loss: 0.0582 - accuracy: 0.9833 - val_loss: 0.0496 - val_accuracy: 0.9667\n",
      "Epoch 1496/2500\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.0589 - accuracy: 0.9833 - val_loss: 0.0527 - val_accuracy: 0.9667\n",
      "Epoch 1497/2500\n",
      "120/120 [==============================] - 0s 132us/step - loss: 0.0603 - accuracy: 0.9833 - val_loss: 0.0428 - val_accuracy: 0.9667\n",
      "Epoch 1498/2500\n",
      "120/120 [==============================] - 0s 113us/step - loss: 0.0585 - accuracy: 0.9750 - val_loss: 0.0434 - val_accuracy: 0.9667\n",
      "Epoch 1499/2500\n",
      "120/120 [==============================] - 0s 115us/step - loss: 0.0588 - accuracy: 0.9833 - val_loss: 0.0478 - val_accuracy: 0.9667\n",
      "Epoch 1500/2500\n",
      "120/120 [==============================] - 0s 113us/step - loss: 0.0575 - accuracy: 0.9833 - val_loss: 0.0490 - val_accuracy: 0.9667\n",
      "Epoch 1501/2500\n",
      "120/120 [==============================] - 0s 105us/step - loss: 0.0580 - accuracy: 0.9833 - val_loss: 0.0512 - val_accuracy: 0.9667\n",
      "Epoch 1502/2500\n",
      "120/120 [==============================] - 0s 101us/step - loss: 0.0585 - accuracy: 0.9833 - val_loss: 0.0505 - val_accuracy: 0.9667\n",
      "Epoch 1503/2500\n",
      "120/120 [==============================] - 0s 118us/step - loss: 0.0591 - accuracy: 0.9833 - val_loss: 0.0517 - val_accuracy: 0.9667\n",
      "Epoch 1504/2500\n",
      "120/120 [==============================] - 0s 114us/step - loss: 0.0616 - accuracy: 0.9750 - val_loss: 0.0398 - val_accuracy: 0.9667\n",
      "Epoch 1505/2500\n",
      "120/120 [==============================] - 0s 107us/step - loss: 0.0587 - accuracy: 0.9750 - val_loss: 0.0452 - val_accuracy: 0.9667\n",
      "Epoch 1506/2500\n",
      "120/120 [==============================] - 0s 122us/step - loss: 0.0575 - accuracy: 0.9833 - val_loss: 0.0478 - val_accuracy: 0.9667\n",
      "Epoch 1507/2500\n",
      "120/120 [==============================] - 0s 116us/step - loss: 0.0575 - accuracy: 0.9833 - val_loss: 0.0479 - val_accuracy: 0.9667\n",
      "Epoch 1508/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.0586 - accuracy: 0.9833 - val_loss: 0.0480 - val_accuracy: 0.9667\n",
      "Epoch 1509/2500\n",
      "120/120 [==============================] - 0s 116us/step - loss: 0.0586 - accuracy: 0.9833 - val_loss: 0.0530 - val_accuracy: 0.9667\n",
      "Epoch 1510/2500\n",
      "120/120 [==============================] - 0s 120us/step - loss: 0.0586 - accuracy: 0.9833 - val_loss: 0.0540 - val_accuracy: 0.9667\n",
      "Epoch 1511/2500\n",
      "120/120 [==============================] - 0s 110us/step - loss: 0.0586 - accuracy: 0.9833 - val_loss: 0.0446 - val_accuracy: 0.9667\n",
      "Epoch 1512/2500\n",
      "120/120 [==============================] - 0s 114us/step - loss: 0.0583 - accuracy: 0.9833 - val_loss: 0.0443 - val_accuracy: 0.9667\n",
      "Epoch 1513/2500\n",
      "120/120 [==============================] - 0s 119us/step - loss: 0.0582 - accuracy: 0.9833 - val_loss: 0.0442 - val_accuracy: 0.9667\n",
      "Epoch 1514/2500\n",
      "120/120 [==============================] - 0s 120us/step - loss: 0.0578 - accuracy: 0.9750 - val_loss: 0.0440 - val_accuracy: 0.9667\n",
      "Epoch 1515/2500\n",
      "120/120 [==============================] - 0s 113us/step - loss: 0.0586 - accuracy: 0.9833 - val_loss: 0.0510 - val_accuracy: 0.9667\n",
      "Epoch 1516/2500\n",
      "120/120 [==============================] - 0s 123us/step - loss: 0.0586 - accuracy: 0.9833 - val_loss: 0.0530 - val_accuracy: 0.9667\n",
      "Epoch 1517/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.0598 - accuracy: 0.9750 - val_loss: 0.0432 - val_accuracy: 0.9667\n",
      "Epoch 1518/2500\n",
      "120/120 [==============================] - 0s 121us/step - loss: 0.0578 - accuracy: 0.9750 - val_loss: 0.0437 - val_accuracy: 0.9667\n",
      "Epoch 1519/2500\n",
      "120/120 [==============================] - 0s 118us/step - loss: 0.0579 - accuracy: 0.9750 - val_loss: 0.0447 - val_accuracy: 0.9667\n",
      "Epoch 1520/2500\n",
      "120/120 [==============================] - 0s 138us/step - loss: 0.0576 - accuracy: 0.9833 - val_loss: 0.0502 - val_accuracy: 0.9667\n",
      "Epoch 1521/2500\n",
      "120/120 [==============================] - 0s 110us/step - loss: 0.0600 - accuracy: 0.9833 - val_loss: 0.0558 - val_accuracy: 0.9667\n",
      "Epoch 1522/2500\n",
      "120/120 [==============================] - 0s 123us/step - loss: 0.0579 - accuracy: 0.9833 - val_loss: 0.0495 - val_accuracy: 0.9667\n",
      "Epoch 1523/2500\n",
      "120/120 [==============================] - 0s 148us/step - loss: 0.0577 - accuracy: 0.9833 - val_loss: 0.0477 - val_accuracy: 0.9667\n",
      "Epoch 1524/2500\n",
      "120/120 [==============================] - 0s 138us/step - loss: 0.0590 - accuracy: 0.9833 - val_loss: 0.0420 - val_accuracy: 0.9667\n",
      "Epoch 1525/2500\n",
      "120/120 [==============================] - 0s 116us/step - loss: 0.0580 - accuracy: 0.9750 - val_loss: 0.0461 - val_accuracy: 0.9667\n",
      "Epoch 1526/2500\n",
      "120/120 [==============================] - 0s 136us/step - loss: 0.0580 - accuracy: 0.9833 - val_loss: 0.0499 - val_accuracy: 0.9667\n",
      "Epoch 1527/2500\n",
      "120/120 [==============================] - 0s 149us/step - loss: 0.0586 - accuracy: 0.9833 - val_loss: 0.0460 - val_accuracy: 0.9667\n",
      "Epoch 1528/2500\n",
      "120/120 [==============================] - 0s 133us/step - loss: 0.0575 - accuracy: 0.9833 - val_loss: 0.0490 - val_accuracy: 0.9667\n",
      "Epoch 1529/2500\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0583 - accuracy: 0.9833 - val_loss: 0.0469 - val_accuracy: 0.9667\n",
      "Epoch 1530/2500\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0575 - accuracy: 0.9833 - val_loss: 0.0483 - val_accuracy: 0.9667\n",
      "Epoch 1531/2500\n",
      "120/120 [==============================] - 0s 123us/step - loss: 0.0578 - accuracy: 0.9833 - val_loss: 0.0472 - val_accuracy: 0.9667\n",
      "Epoch 1532/2500\n",
      "120/120 [==============================] - 0s 113us/step - loss: 0.0591 - accuracy: 0.9833 - val_loss: 0.0482 - val_accuracy: 0.9667\n",
      "Epoch 1533/2500\n",
      "120/120 [==============================] - 0s 115us/step - loss: 0.0599 - accuracy: 0.9833 - val_loss: 0.0528 - val_accuracy: 0.9667\n",
      "Epoch 1534/2500\n",
      "120/120 [==============================] - 0s 118us/step - loss: 0.0613 - accuracy: 0.9750 - val_loss: 0.0417 - val_accuracy: 0.9667\n",
      "Epoch 1535/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.0578 - accuracy: 0.9750 - val_loss: 0.0465 - val_accuracy: 0.9667\n",
      "Epoch 1536/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.0584 - accuracy: 0.9833 - val_loss: 0.0462 - val_accuracy: 0.9667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1537/2500\n",
      "120/120 [==============================] - 0s 118us/step - loss: 0.0583 - accuracy: 0.9750 - val_loss: 0.0464 - val_accuracy: 0.9667\n",
      "Epoch 1538/2500\n",
      "120/120 [==============================] - 0s 120us/step - loss: 0.0576 - accuracy: 0.9833 - val_loss: 0.0515 - val_accuracy: 0.9667\n",
      "Epoch 1539/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.0576 - accuracy: 0.9833 - val_loss: 0.0508 - val_accuracy: 0.9667\n",
      "Epoch 1540/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.0576 - accuracy: 0.9833 - val_loss: 0.0471 - val_accuracy: 0.9667\n",
      "Epoch 1541/2500\n",
      "120/120 [==============================] - 0s 113us/step - loss: 0.0573 - accuracy: 0.9833 - val_loss: 0.0465 - val_accuracy: 0.9667\n",
      "Epoch 1542/2500\n",
      "120/120 [==============================] - 0s 105us/step - loss: 0.0575 - accuracy: 0.9833 - val_loss: 0.0478 - val_accuracy: 0.9667\n",
      "Epoch 1543/2500\n",
      "120/120 [==============================] - 0s 113us/step - loss: 0.0573 - accuracy: 0.9833 - val_loss: 0.0480 - val_accuracy: 0.9667\n",
      "Epoch 1544/2500\n",
      "120/120 [==============================] - 0s 122us/step - loss: 0.0581 - accuracy: 0.9833 - val_loss: 0.0520 - val_accuracy: 0.9667\n",
      "Epoch 1545/2500\n",
      "120/120 [==============================] - 0s 122us/step - loss: 0.0587 - accuracy: 0.9833 - val_loss: 0.0472 - val_accuracy: 0.9667\n",
      "Epoch 1546/2500\n",
      "120/120 [==============================] - 0s 110us/step - loss: 0.0578 - accuracy: 0.9833 - val_loss: 0.0485 - val_accuracy: 0.9667\n",
      "Epoch 1547/2500\n",
      "120/120 [==============================] - 0s 146us/step - loss: 0.0578 - accuracy: 0.9833 - val_loss: 0.0459 - val_accuracy: 0.9667\n",
      "Epoch 1548/2500\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.0602 - accuracy: 0.9750 - val_loss: 0.0415 - val_accuracy: 0.9667\n",
      "Epoch 1549/2500\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.0580 - accuracy: 0.9750 - val_loss: 0.0511 - val_accuracy: 0.9667\n",
      "Epoch 1550/2500\n",
      "120/120 [==============================] - 0s 115us/step - loss: 0.0588 - accuracy: 0.9833 - val_loss: 0.0572 - val_accuracy: 0.9667\n",
      "Epoch 1551/2500\n",
      "120/120 [==============================] - 0s 114us/step - loss: 0.0592 - accuracy: 0.9833 - val_loss: 0.0455 - val_accuracy: 0.9667\n",
      "Epoch 1552/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.0579 - accuracy: 0.9833 - val_loss: 0.0450 - val_accuracy: 0.9667\n",
      "Epoch 1553/2500\n",
      "120/120 [==============================] - 0s 110us/step - loss: 0.0583 - accuracy: 0.9833 - val_loss: 0.0441 - val_accuracy: 0.9667\n",
      "Epoch 1554/2500\n",
      "120/120 [==============================] - 0s 113us/step - loss: 0.0579 - accuracy: 0.9833 - val_loss: 0.0466 - val_accuracy: 0.9667\n",
      "Epoch 1555/2500\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.0575 - accuracy: 0.9833 - val_loss: 0.0479 - val_accuracy: 0.9667\n",
      "Epoch 1556/2500\n",
      "120/120 [==============================] - 0s 115us/step - loss: 0.0598 - accuracy: 0.9833 - val_loss: 0.0561 - val_accuracy: 0.9667\n",
      "Epoch 1557/2500\n",
      "120/120 [==============================] - 0s 123us/step - loss: 0.0595 - accuracy: 0.9833 - val_loss: 0.0446 - val_accuracy: 0.9667\n",
      "Epoch 1558/2500\n",
      "120/120 [==============================] - 0s 109us/step - loss: 0.0578 - accuracy: 0.9833 - val_loss: 0.0469 - val_accuracy: 0.9667\n",
      "Epoch 1559/2500\n",
      "120/120 [==============================] - 0s 112us/step - loss: 0.0577 - accuracy: 0.9833 - val_loss: 0.0469 - val_accuracy: 0.9667\n",
      "Epoch 1560/2500\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0571 - accuracy: 0.9833 - val_loss: 0.0496 - val_accuracy: 0.9667\n",
      "Epoch 1561/2500\n",
      "120/120 [==============================] - 0s 118us/step - loss: 0.0575 - accuracy: 0.9833 - val_loss: 0.0467 - val_accuracy: 0.9667\n",
      "Epoch 1562/2500\n",
      "120/120 [==============================] - 0s 112us/step - loss: 0.0581 - accuracy: 0.9833 - val_loss: 0.0496 - val_accuracy: 0.9667\n",
      "Epoch 1563/2500\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.0574 - accuracy: 0.9833 - val_loss: 0.0495 - val_accuracy: 0.9667\n",
      "Epoch 1564/2500\n",
      "120/120 [==============================] - 0s 109us/step - loss: 0.0584 - accuracy: 0.9833 - val_loss: 0.0523 - val_accuracy: 0.9667\n",
      "Epoch 1565/2500\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.0596 - accuracy: 0.9750 - val_loss: 0.0415 - val_accuracy: 0.9667\n",
      "Epoch 1566/2500\n",
      "120/120 [==============================] - 0s 105us/step - loss: 0.0579 - accuracy: 0.9750 - val_loss: 0.0435 - val_accuracy: 0.9667\n",
      "Epoch 1567/2500\n",
      "120/120 [==============================] - 0s 118us/step - loss: 0.0577 - accuracy: 0.9833 - val_loss: 0.0486 - val_accuracy: 0.9667\n",
      "Epoch 1568/2500\n",
      "120/120 [==============================] - 0s 103us/step - loss: 0.0573 - accuracy: 0.9833 - val_loss: 0.0462 - val_accuracy: 0.9667\n",
      "Epoch 1569/2500\n",
      "120/120 [==============================] - 0s 107us/step - loss: 0.0575 - accuracy: 0.9833 - val_loss: 0.0448 - val_accuracy: 0.9667\n",
      "Epoch 1570/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.0576 - accuracy: 0.9833 - val_loss: 0.0451 - val_accuracy: 0.9667\n",
      "Epoch 1571/2500\n",
      "120/120 [==============================] - 0s 122us/step - loss: 0.0587 - accuracy: 0.9833 - val_loss: 0.0551 - val_accuracy: 0.9667\n",
      "Epoch 1572/2500\n",
      "120/120 [==============================] - 0s 102us/step - loss: 0.0588 - accuracy: 0.9833 - val_loss: 0.0563 - val_accuracy: 0.9667\n",
      "Epoch 1573/2500\n",
      "120/120 [==============================] - 0s 102us/step - loss: 0.0586 - accuracy: 0.9833 - val_loss: 0.0435 - val_accuracy: 0.9667\n",
      "Epoch 1574/2500\n",
      "120/120 [==============================] - 0s 120us/step - loss: 0.0594 - accuracy: 0.9833 - val_loss: 0.0490 - val_accuracy: 0.9667\n",
      "Epoch 1575/2500\n",
      "120/120 [==============================] - 0s 109us/step - loss: 0.0572 - accuracy: 0.9833 - val_loss: 0.0445 - val_accuracy: 0.9667\n",
      "Epoch 1576/2500\n",
      "120/120 [==============================] - 0s 102us/step - loss: 0.0573 - accuracy: 0.9750 - val_loss: 0.0431 - val_accuracy: 0.9667\n",
      "Epoch 1577/2500\n",
      "120/120 [==============================] - 0s 116us/step - loss: 0.0581 - accuracy: 0.9750 - val_loss: 0.0409 - val_accuracy: 0.9667\n",
      "Epoch 1578/2500\n",
      "120/120 [==============================] - 0s 114us/step - loss: 0.0568 - accuracy: 0.9750 - val_loss: 0.0459 - val_accuracy: 0.9667\n",
      "Epoch 1579/2500\n",
      "120/120 [==============================] - 0s 113us/step - loss: 0.0568 - accuracy: 0.9833 - val_loss: 0.0485 - val_accuracy: 0.9667\n",
      "Epoch 1580/2500\n",
      "120/120 [==============================] - 0s 122us/step - loss: 0.0585 - accuracy: 0.9833 - val_loss: 0.0455 - val_accuracy: 0.9667\n",
      "Epoch 1581/2500\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0616 - accuracy: 0.9833 - val_loss: 0.0595 - val_accuracy: 0.9667\n",
      "Epoch 1582/2500\n",
      "120/120 [==============================] - 0s 112us/step - loss: 0.0578 - accuracy: 0.9833 - val_loss: 0.0554 - val_accuracy: 0.9667\n",
      "Epoch 1583/2500\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0613 - accuracy: 0.9750 - val_loss: 0.0399 - val_accuracy: 0.9667\n",
      "Epoch 1584/2500\n",
      "120/120 [==============================] - 0s 127us/step - loss: 0.0580 - accuracy: 0.9750 - val_loss: 0.0420 - val_accuracy: 0.9667\n",
      "Epoch 1585/2500\n",
      "120/120 [==============================] - 0s 102us/step - loss: 0.0575 - accuracy: 0.9750 - val_loss: 0.0481 - val_accuracy: 0.9667\n",
      "Epoch 1586/2500\n",
      "120/120 [==============================] - 0s 104us/step - loss: 0.0574 - accuracy: 0.9833 - val_loss: 0.0506 - val_accuracy: 0.9667\n",
      "Epoch 1587/2500\n",
      "120/120 [==============================] - 0s 114us/step - loss: 0.0571 - accuracy: 0.9833 - val_loss: 0.0487 - val_accuracy: 0.9667\n",
      "Epoch 1588/2500\n",
      "120/120 [==============================] - 0s 115us/step - loss: 0.0567 - accuracy: 0.9833 - val_loss: 0.0467 - val_accuracy: 0.9667\n",
      "Epoch 1589/2500\n",
      "120/120 [==============================] - 0s 107us/step - loss: 0.0587 - accuracy: 0.9750 - val_loss: 0.0424 - val_accuracy: 0.9667\n",
      "Epoch 1590/2500\n",
      "120/120 [==============================] - 0s 105us/step - loss: 0.0586 - accuracy: 0.9750 - val_loss: 0.0421 - val_accuracy: 0.9667\n",
      "Epoch 1591/2500\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0576 - accuracy: 0.9833 - val_loss: 0.0509 - val_accuracy: 0.9667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1592/2500\n",
      "120/120 [==============================] - 0s 121us/step - loss: 0.0575 - accuracy: 0.9833 - val_loss: 0.0527 - val_accuracy: 0.9667\n",
      "Epoch 1593/2500\n",
      "120/120 [==============================] - 0s 123us/step - loss: 0.0571 - accuracy: 0.9833 - val_loss: 0.0497 - val_accuracy: 0.9667\n",
      "Epoch 1594/2500\n",
      "120/120 [==============================] - 0s 110us/step - loss: 0.0586 - accuracy: 0.9833 - val_loss: 0.0523 - val_accuracy: 0.9667\n",
      "Epoch 1595/2500\n",
      "120/120 [==============================] - 0s 110us/step - loss: 0.0607 - accuracy: 0.9750 - val_loss: 0.0412 - val_accuracy: 0.9667\n",
      "Epoch 1596/2500\n",
      "120/120 [==============================] - 0s 102us/step - loss: 0.0575 - accuracy: 0.9750 - val_loss: 0.0432 - val_accuracy: 0.9667\n",
      "Epoch 1597/2500\n",
      "120/120 [==============================] - 0s 116us/step - loss: 0.0568 - accuracy: 0.9833 - val_loss: 0.0481 - val_accuracy: 0.9667\n",
      "Epoch 1598/2500\n",
      "120/120 [==============================] - 0s 114us/step - loss: 0.0591 - accuracy: 0.9833 - val_loss: 0.0543 - val_accuracy: 0.9667\n",
      "Epoch 1599/2500\n",
      "120/120 [==============================] - 0s 103us/step - loss: 0.0572 - accuracy: 0.9833 - val_loss: 0.0502 - val_accuracy: 0.9667\n",
      "Epoch 1600/2500\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0576 - accuracy: 0.9833 - val_loss: 0.0490 - val_accuracy: 0.9667\n",
      "Epoch 1601/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.0587 - accuracy: 0.9750 - val_loss: 0.0407 - val_accuracy: 0.9667\n",
      "Epoch 1602/2500\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.0575 - accuracy: 0.9750 - val_loss: 0.0412 - val_accuracy: 0.9667\n",
      "Epoch 1603/2500\n",
      "120/120 [==============================] - 0s 101us/step - loss: 0.0573 - accuracy: 0.9750 - val_loss: 0.0430 - val_accuracy: 0.9667\n",
      "Epoch 1604/2500\n",
      "120/120 [==============================] - 0s 128us/step - loss: 0.0568 - accuracy: 0.9833 - val_loss: 0.0473 - val_accuracy: 0.9667\n",
      "Epoch 1605/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.0588 - accuracy: 0.9833 - val_loss: 0.0536 - val_accuracy: 0.9667\n",
      "Epoch 1606/2500\n",
      "120/120 [==============================] - 0s 110us/step - loss: 0.0571 - accuracy: 0.9833 - val_loss: 0.0488 - val_accuracy: 0.9667\n",
      "Epoch 1607/2500\n",
      "120/120 [==============================] - 0s 113us/step - loss: 0.0592 - accuracy: 0.9833 - val_loss: 0.0525 - val_accuracy: 0.9667\n",
      "Epoch 1608/2500\n",
      "120/120 [==============================] - 0s 120us/step - loss: 0.0580 - accuracy: 0.9833 - val_loss: 0.0424 - val_accuracy: 0.9667\n",
      "Epoch 1609/2500\n",
      "120/120 [==============================] - 0s 105us/step - loss: 0.0580 - accuracy: 0.9750 - val_loss: 0.0470 - val_accuracy: 0.9667\n",
      "Epoch 1610/2500\n",
      "120/120 [==============================] - 0s 105us/step - loss: 0.0574 - accuracy: 0.9833 - val_loss: 0.0476 - val_accuracy: 0.9667\n",
      "Epoch 1611/2500\n",
      "120/120 [==============================] - 0s 113us/step - loss: 0.0575 - accuracy: 0.9833 - val_loss: 0.0466 - val_accuracy: 0.9667\n",
      "Epoch 1612/2500\n",
      "120/120 [==============================] - 0s 103us/step - loss: 0.0568 - accuracy: 0.9833 - val_loss: 0.0444 - val_accuracy: 0.9667\n",
      "Epoch 1613/2500\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.0571 - accuracy: 0.9833 - val_loss: 0.0444 - val_accuracy: 0.9667\n",
      "Epoch 1614/2500\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0570 - accuracy: 0.9750 - val_loss: 0.0439 - val_accuracy: 0.9667\n",
      "Epoch 1615/2500\n",
      "120/120 [==============================] - 0s 118us/step - loss: 0.0568 - accuracy: 0.9833 - val_loss: 0.0446 - val_accuracy: 0.9667\n",
      "Epoch 1616/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.0579 - accuracy: 0.9833 - val_loss: 0.0508 - val_accuracy: 0.9667\n",
      "Epoch 1617/2500\n",
      "120/120 [==============================] - 0s 131us/step - loss: 0.0572 - accuracy: 0.9833 - val_loss: 0.0461 - val_accuracy: 0.9667\n",
      "Epoch 1618/2500\n",
      "120/120 [==============================] - 0s 124us/step - loss: 0.0567 - accuracy: 0.9833 - val_loss: 0.0473 - val_accuracy: 0.9667\n",
      "Epoch 1619/2500\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0575 - accuracy: 0.9833 - val_loss: 0.0441 - val_accuracy: 0.9667\n",
      "Epoch 1620/2500\n",
      "120/120 [==============================] - 0s 105us/step - loss: 0.0569 - accuracy: 0.9833 - val_loss: 0.0460 - val_accuracy: 0.9667\n",
      "Epoch 1621/2500\n",
      "120/120 [==============================] - 0s 113us/step - loss: 0.0571 - accuracy: 0.9833 - val_loss: 0.0470 - val_accuracy: 0.9667\n",
      "Epoch 1622/2500\n",
      "120/120 [==============================] - 0s 109us/step - loss: 0.0567 - accuracy: 0.9833 - val_loss: 0.0484 - val_accuracy: 0.9667\n",
      "Epoch 1623/2500\n",
      "120/120 [==============================] - 0s 102us/step - loss: 0.0569 - accuracy: 0.9833 - val_loss: 0.0475 - val_accuracy: 0.9667\n",
      "Epoch 1624/2500\n",
      "120/120 [==============================] - 0s 115us/step - loss: 0.0568 - accuracy: 0.9833 - val_loss: 0.0470 - val_accuracy: 0.9667\n",
      "Epoch 1625/2500\n",
      "120/120 [==============================] - 0s 110us/step - loss: 0.0576 - accuracy: 0.9833 - val_loss: 0.0506 - val_accuracy: 0.9667\n",
      "Epoch 1626/2500\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0598 - accuracy: 0.9750 - val_loss: 0.0415 - val_accuracy: 0.9667\n",
      "Epoch 1627/2500\n",
      "120/120 [==============================] - 0s 109us/step - loss: 0.0573 - accuracy: 0.9750 - val_loss: 0.0483 - val_accuracy: 0.9667\n",
      "Epoch 1628/2500\n",
      "120/120 [==============================] - 0s 110us/step - loss: 0.0577 - accuracy: 0.9833 - val_loss: 0.0464 - val_accuracy: 0.9667\n",
      "Epoch 1629/2500\n",
      "120/120 [==============================] - 0s 114us/step - loss: 0.0572 - accuracy: 0.9833 - val_loss: 0.0511 - val_accuracy: 0.9667\n",
      "Epoch 1630/2500\n",
      "120/120 [==============================] - 0s 104us/step - loss: 0.0570 - accuracy: 0.9833 - val_loss: 0.0510 - val_accuracy: 0.9667\n",
      "Epoch 1631/2500\n",
      "120/120 [==============================] - 0s 114us/step - loss: 0.0570 - accuracy: 0.9833 - val_loss: 0.0482 - val_accuracy: 0.9667\n",
      "Epoch 1632/2500\n",
      "120/120 [==============================] - 0s 105us/step - loss: 0.0580 - accuracy: 0.9833 - val_loss: 0.0449 - val_accuracy: 0.9667\n",
      "Epoch 1633/2500\n",
      "120/120 [==============================] - 0s 113us/step - loss: 0.0574 - accuracy: 0.9750 - val_loss: 0.0423 - val_accuracy: 0.9667\n",
      "Epoch 1634/2500\n",
      "120/120 [==============================] - 0s 118us/step - loss: 0.0573 - accuracy: 0.9833 - val_loss: 0.0463 - val_accuracy: 0.9667\n",
      "Epoch 1635/2500\n",
      "120/120 [==============================] - 0s 116us/step - loss: 0.0569 - accuracy: 0.9833 - val_loss: 0.0462 - val_accuracy: 0.9667\n",
      "Epoch 1636/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.0567 - accuracy: 0.9833 - val_loss: 0.0475 - val_accuracy: 0.9667\n",
      "Epoch 1637/2500\n",
      "120/120 [==============================] - 0s 105us/step - loss: 0.0577 - accuracy: 0.9833 - val_loss: 0.0506 - val_accuracy: 0.9667\n",
      "Epoch 1638/2500\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0572 - accuracy: 0.9833 - val_loss: 0.0465 - val_accuracy: 0.9667\n",
      "Epoch 1639/2500\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0569 - accuracy: 0.9833 - val_loss: 0.0455 - val_accuracy: 0.9667\n",
      "Epoch 1640/2500\n",
      "120/120 [==============================] - 0s 100us/step - loss: 0.0567 - accuracy: 0.9833 - val_loss: 0.0446 - val_accuracy: 0.9667\n",
      "Epoch 1641/2500\n",
      "120/120 [==============================] - 0s 121us/step - loss: 0.0583 - accuracy: 0.9833 - val_loss: 0.0482 - val_accuracy: 0.9667\n",
      "Epoch 1642/2500\n",
      "120/120 [==============================] - 0s 113us/step - loss: 0.0565 - accuracy: 0.9833 - val_loss: 0.0450 - val_accuracy: 0.9667\n",
      "Epoch 1643/2500\n",
      "120/120 [==============================] - 0s 115us/step - loss: 0.0572 - accuracy: 0.9750 - val_loss: 0.0416 - val_accuracy: 0.9667\n",
      "Epoch 1644/2500\n",
      "120/120 [==============================] - 0s 113us/step - loss: 0.0575 - accuracy: 0.9750 - val_loss: 0.0435 - val_accuracy: 0.9667\n",
      "Epoch 1645/2500\n",
      "120/120 [==============================] - 0s 113us/step - loss: 0.0578 - accuracy: 0.9833 - val_loss: 0.0493 - val_accuracy: 0.9667\n",
      "Epoch 1646/2500\n",
      "120/120 [==============================] - 0s 99us/step - loss: 0.0567 - accuracy: 0.9833 - val_loss: 0.0493 - val_accuracy: 0.9667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1647/2500\n",
      "120/120 [==============================] - 0s 97us/step - loss: 0.0571 - accuracy: 0.9833 - val_loss: 0.0491 - val_accuracy: 0.9667\n",
      "Epoch 1648/2500\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0569 - accuracy: 0.9833 - val_loss: 0.0432 - val_accuracy: 0.9667\n",
      "Epoch 1649/2500\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.0569 - accuracy: 0.9750 - val_loss: 0.0416 - val_accuracy: 0.9667\n",
      "Epoch 1650/2500\n",
      "120/120 [==============================] - 0s 119us/step - loss: 0.0567 - accuracy: 0.9750 - val_loss: 0.0422 - val_accuracy: 0.9667\n",
      "Epoch 1651/2500\n",
      "120/120 [==============================] - 0s 120us/step - loss: 0.0585 - accuracy: 0.9750 - val_loss: 0.0441 - val_accuracy: 0.9667\n",
      "Epoch 1652/2500\n",
      "120/120 [==============================] - 0s 130us/step - loss: 0.0598 - accuracy: 0.9833 - val_loss: 0.0550 - val_accuracy: 0.9667\n",
      "Epoch 1653/2500\n",
      "120/120 [==============================] - 0s 114us/step - loss: 0.0583 - accuracy: 0.9833 - val_loss: 0.0448 - val_accuracy: 0.9667\n",
      "Epoch 1654/2500\n",
      "120/120 [==============================] - 0s 122us/step - loss: 0.0579 - accuracy: 0.9750 - val_loss: 0.0421 - val_accuracy: 0.9667\n",
      "Epoch 1655/2500\n",
      "120/120 [==============================] - 0s 110us/step - loss: 0.0566 - accuracy: 0.9833 - val_loss: 0.0459 - val_accuracy: 0.9667\n",
      "Epoch 1656/2500\n",
      "120/120 [==============================] - 0s 104us/step - loss: 0.0582 - accuracy: 0.9833 - val_loss: 0.0433 - val_accuracy: 0.9667\n",
      "Epoch 1657/2500\n",
      "120/120 [==============================] - 0s 112us/step - loss: 0.0563 - accuracy: 0.9833 - val_loss: 0.0486 - val_accuracy: 0.9667\n",
      "Epoch 1658/2500\n",
      "120/120 [==============================] - 0s 118us/step - loss: 0.0570 - accuracy: 0.9833 - val_loss: 0.0481 - val_accuracy: 0.9667\n",
      "Epoch 1659/2500\n",
      "120/120 [==============================] - 0s 121us/step - loss: 0.0567 - accuracy: 0.9833 - val_loss: 0.0511 - val_accuracy: 0.9667\n",
      "Epoch 1660/2500\n",
      "120/120 [==============================] - 0s 123us/step - loss: 0.0569 - accuracy: 0.9833 - val_loss: 0.0502 - val_accuracy: 0.9667\n",
      "Epoch 1661/2500\n",
      "120/120 [==============================] - 0s 113us/step - loss: 0.0566 - accuracy: 0.9833 - val_loss: 0.0478 - val_accuracy: 0.9667\n",
      "Epoch 1662/2500\n",
      "120/120 [==============================] - 0s 115us/step - loss: 0.0582 - accuracy: 0.9833 - val_loss: 0.0506 - val_accuracy: 0.9667\n",
      "Epoch 1663/2500\n",
      "120/120 [==============================] - 0s 104us/step - loss: 0.0579 - accuracy: 0.9750 - val_loss: 0.0420 - val_accuracy: 0.9667\n",
      "Epoch 1664/2500\n",
      "120/120 [==============================] - 0s 120us/step - loss: 0.0587 - accuracy: 0.9750 - val_loss: 0.0471 - val_accuracy: 0.9667\n",
      "Epoch 1665/2500\n",
      "120/120 [==============================] - 0s 116us/step - loss: 0.0572 - accuracy: 0.9750 - val_loss: 0.0428 - val_accuracy: 0.9667\n",
      "Epoch 1666/2500\n",
      "120/120 [==============================] - 0s 119us/step - loss: 0.0572 - accuracy: 0.9750 - val_loss: 0.0426 - val_accuracy: 0.9667\n",
      "Epoch 1667/2500\n",
      "120/120 [==============================] - 0s 118us/step - loss: 0.0575 - accuracy: 0.9750 - val_loss: 0.0441 - val_accuracy: 0.9667\n",
      "Epoch 1668/2500\n",
      "120/120 [==============================] - 0s 127us/step - loss: 0.0566 - accuracy: 0.9833 - val_loss: 0.0457 - val_accuracy: 0.9667\n",
      "Epoch 1669/2500\n",
      "120/120 [==============================] - 0s 100us/step - loss: 0.0560 - accuracy: 0.9833 - val_loss: 0.0488 - val_accuracy: 0.9667\n",
      "Epoch 1670/2500\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.0565 - accuracy: 0.9833 - val_loss: 0.0487 - val_accuracy: 0.9667\n",
      "Epoch 1671/2500\n",
      "120/120 [==============================] - 0s 128us/step - loss: 0.0566 - accuracy: 0.9833 - val_loss: 0.0476 - val_accuracy: 0.9667\n",
      "Epoch 1672/2500\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0564 - accuracy: 0.9833 - val_loss: 0.0501 - val_accuracy: 0.9667\n",
      "Epoch 1673/2500\n",
      "120/120 [==============================] - 0s 120us/step - loss: 0.0569 - accuracy: 0.9833 - val_loss: 0.0477 - val_accuracy: 0.9667\n",
      "Epoch 1674/2500\n",
      "120/120 [==============================] - 0s 126us/step - loss: 0.0586 - accuracy: 0.9750 - val_loss: 0.0407 - val_accuracy: 0.9667\n",
      "Epoch 1675/2500\n",
      "120/120 [==============================] - 0s 112us/step - loss: 0.0570 - accuracy: 0.9833 - val_loss: 0.0468 - val_accuracy: 0.9667\n",
      "Epoch 1676/2500\n",
      "120/120 [==============================] - 0s 102us/step - loss: 0.0567 - accuracy: 0.9833 - val_loss: 0.0505 - val_accuracy: 0.9667\n",
      "Epoch 1677/2500\n",
      "120/120 [==============================] - 0s 115us/step - loss: 0.0570 - accuracy: 0.9833 - val_loss: 0.0473 - val_accuracy: 0.9667\n",
      "Epoch 1678/2500\n",
      "120/120 [==============================] - 0s 112us/step - loss: 0.0588 - accuracy: 0.9750 - val_loss: 0.0405 - val_accuracy: 0.9667\n",
      "Epoch 1679/2500\n",
      "120/120 [==============================] - 0s 109us/step - loss: 0.0570 - accuracy: 0.9750 - val_loss: 0.0481 - val_accuracy: 0.9667\n",
      "Epoch 1680/2500\n",
      "120/120 [==============================] - 0s 116us/step - loss: 0.0600 - accuracy: 0.9833 - val_loss: 0.0580 - val_accuracy: 0.9667\n",
      "Epoch 1681/2500\n",
      "120/120 [==============================] - 0s 122us/step - loss: 0.0575 - accuracy: 0.9833 - val_loss: 0.0441 - val_accuracy: 0.9667\n",
      "Epoch 1682/2500\n",
      "120/120 [==============================] - 0s 110us/step - loss: 0.0574 - accuracy: 0.9750 - val_loss: 0.0394 - val_accuracy: 0.9667\n",
      "Epoch 1683/2500\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0573 - accuracy: 0.9750 - val_loss: 0.0455 - val_accuracy: 0.9667\n",
      "Epoch 1684/2500\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.0564 - accuracy: 0.9833 - val_loss: 0.0479 - val_accuracy: 0.9667\n",
      "Epoch 1685/2500\n",
      "120/120 [==============================] - 0s 110us/step - loss: 0.0576 - accuracy: 0.9833 - val_loss: 0.0421 - val_accuracy: 0.9667\n",
      "Epoch 1686/2500\n",
      "120/120 [==============================] - 0s 121us/step - loss: 0.0566 - accuracy: 0.9833 - val_loss: 0.0460 - val_accuracy: 0.9667\n",
      "Epoch 1687/2500\n",
      "120/120 [==============================] - 0s 130us/step - loss: 0.0568 - accuracy: 0.9833 - val_loss: 0.0454 - val_accuracy: 0.9667\n",
      "Epoch 1688/2500\n",
      "120/120 [==============================] - 0s 116us/step - loss: 0.0590 - accuracy: 0.9750 - val_loss: 0.0420 - val_accuracy: 0.9667\n",
      "Epoch 1689/2500\n",
      "120/120 [==============================] - 0s 121us/step - loss: 0.0565 - accuracy: 0.9750 - val_loss: 0.0498 - val_accuracy: 0.9667\n",
      "Epoch 1690/2500\n",
      "120/120 [==============================] - 0s 144us/step - loss: 0.0576 - accuracy: 0.9833 - val_loss: 0.0554 - val_accuracy: 0.9667\n",
      "Epoch 1691/2500\n",
      "120/120 [==============================] - 0s 127us/step - loss: 0.0570 - accuracy: 0.9833 - val_loss: 0.0501 - val_accuracy: 0.9667\n",
      "Epoch 1692/2500\n",
      "120/120 [==============================] - 0s 118us/step - loss: 0.0578 - accuracy: 0.9833 - val_loss: 0.0496 - val_accuracy: 0.9667\n",
      "Epoch 1693/2500\n",
      "120/120 [==============================] - 0s 164us/step - loss: 0.0571 - accuracy: 0.9833 - val_loss: 0.0411 - val_accuracy: 0.9667\n",
      "Epoch 1694/2500\n",
      "120/120 [==============================] - 0s 149us/step - loss: 0.0568 - accuracy: 0.9750 - val_loss: 0.0423 - val_accuracy: 0.9667\n",
      "Epoch 1695/2500\n",
      "120/120 [==============================] - 0s 142us/step - loss: 0.0576 - accuracy: 0.9833 - val_loss: 0.0474 - val_accuracy: 0.9667\n",
      "Epoch 1696/2500\n",
      "120/120 [==============================] - 0s 165us/step - loss: 0.0579 - accuracy: 0.9833 - val_loss: 0.0504 - val_accuracy: 0.9667\n",
      "Epoch 1697/2500\n",
      "120/120 [==============================] - 0s 144us/step - loss: 0.0567 - accuracy: 0.9833 - val_loss: 0.0449 - val_accuracy: 0.9667\n",
      "Epoch 1698/2500\n",
      "120/120 [==============================] - 0s 169us/step - loss: 0.0563 - accuracy: 0.9833 - val_loss: 0.0428 - val_accuracy: 0.9667\n",
      "Epoch 1699/2500\n",
      "120/120 [==============================] - 0s 156us/step - loss: 0.0579 - accuracy: 0.9833 - val_loss: 0.0496 - val_accuracy: 0.9667\n",
      "Epoch 1700/2500\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.0566 - accuracy: 0.9833 - val_loss: 0.0448 - val_accuracy: 0.9667\n",
      "Epoch 1701/2500\n",
      "120/120 [==============================] - 0s 145us/step - loss: 0.0562 - accuracy: 0.9833 - val_loss: 0.0440 - val_accuracy: 0.9667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1702/2500\n",
      "120/120 [==============================] - 0s 154us/step - loss: 0.0569 - accuracy: 0.9833 - val_loss: 0.0412 - val_accuracy: 0.9667\n",
      "Epoch 1703/2500\n",
      "120/120 [==============================] - 0s 140us/step - loss: 0.0579 - accuracy: 0.9750 - val_loss: 0.0501 - val_accuracy: 0.9667\n",
      "Epoch 1704/2500\n",
      "120/120 [==============================] - 0s 138us/step - loss: 0.0580 - accuracy: 0.9833 - val_loss: 0.0482 - val_accuracy: 0.9667\n",
      "Epoch 1705/2500\n",
      "120/120 [==============================] - 0s 149us/step - loss: 0.0573 - accuracy: 0.9833 - val_loss: 0.0494 - val_accuracy: 0.9667\n",
      "Epoch 1706/2500\n",
      "120/120 [==============================] - 0s 147us/step - loss: 0.0567 - accuracy: 0.9833 - val_loss: 0.0451 - val_accuracy: 0.9667\n",
      "Epoch 1707/2500\n",
      "120/120 [==============================] - 0s 121us/step - loss: 0.0569 - accuracy: 0.9750 - val_loss: 0.0410 - val_accuracy: 0.9667\n",
      "Epoch 1708/2500\n",
      "120/120 [==============================] - 0s 139us/step - loss: 0.0566 - accuracy: 0.9750 - val_loss: 0.0411 - val_accuracy: 0.9667\n",
      "Epoch 1709/2500\n",
      "120/120 [==============================] - 0s 94us/step - loss: 0.0565 - accuracy: 0.9750 - val_loss: 0.0465 - val_accuracy: 0.9667\n",
      "Epoch 1710/2500\n",
      "120/120 [==============================] - 0s 97us/step - loss: 0.0566 - accuracy: 0.9833 - val_loss: 0.0465 - val_accuracy: 0.9667\n",
      "Epoch 1711/2500\n",
      "120/120 [==============================] - 0s 115us/step - loss: 0.0585 - accuracy: 0.9750 - val_loss: 0.0412 - val_accuracy: 0.9667\n",
      "Epoch 1712/2500\n",
      "120/120 [==============================] - 0s 115us/step - loss: 0.0564 - accuracy: 0.9833 - val_loss: 0.0477 - val_accuracy: 0.9667\n",
      "Epoch 1713/2500\n",
      "120/120 [==============================] - 0s 116us/step - loss: 0.0568 - accuracy: 0.9833 - val_loss: 0.0514 - val_accuracy: 0.9667\n",
      "Epoch 1714/2500\n",
      "120/120 [==============================] - 0s 110us/step - loss: 0.0570 - accuracy: 0.9833 - val_loss: 0.0517 - val_accuracy: 0.9667\n",
      "Epoch 1715/2500\n",
      "120/120 [==============================] - 0s 105us/step - loss: 0.0562 - accuracy: 0.9833 - val_loss: 0.0482 - val_accuracy: 0.9667\n",
      "Epoch 1716/2500\n",
      "120/120 [==============================] - 0s 91us/step - loss: 0.0579 - accuracy: 0.9750 - val_loss: 0.0412 - val_accuracy: 0.9667\n",
      "Epoch 1717/2500\n",
      "120/120 [==============================] - 0s 100us/step - loss: 0.0562 - accuracy: 0.9750 - val_loss: 0.0433 - val_accuracy: 0.9667\n",
      "Epoch 1718/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.0564 - accuracy: 0.9833 - val_loss: 0.0438 - val_accuracy: 0.9667\n",
      "Epoch 1719/2500\n",
      "120/120 [==============================] - 0s 118us/step - loss: 0.0570 - accuracy: 0.9833 - val_loss: 0.0509 - val_accuracy: 0.9667\n",
      "Epoch 1720/2500\n",
      "120/120 [==============================] - 0s 121us/step - loss: 0.0570 - accuracy: 0.9833 - val_loss: 0.0491 - val_accuracy: 0.9667\n",
      "Epoch 1721/2500\n",
      "120/120 [==============================] - 0s 104us/step - loss: 0.0574 - accuracy: 0.9833 - val_loss: 0.0482 - val_accuracy: 0.9667\n",
      "Epoch 1722/2500\n",
      "120/120 [==============================] - 0s 102us/step - loss: 0.0579 - accuracy: 0.9833 - val_loss: 0.0391 - val_accuracy: 0.9667\n",
      "Epoch 1723/2500\n",
      "120/120 [==============================] - 0s 97us/step - loss: 0.0570 - accuracy: 0.9750 - val_loss: 0.0410 - val_accuracy: 0.9667\n",
      "Epoch 1724/2500\n",
      "120/120 [==============================] - 0s 104us/step - loss: 0.0559 - accuracy: 0.9833 - val_loss: 0.0472 - val_accuracy: 0.9667\n",
      "Epoch 1725/2500\n",
      "120/120 [==============================] - 0s 109us/step - loss: 0.0562 - accuracy: 0.9833 - val_loss: 0.0456 - val_accuracy: 0.9667\n",
      "Epoch 1726/2500\n",
      "120/120 [==============================] - 0s 116us/step - loss: 0.0562 - accuracy: 0.9833 - val_loss: 0.0456 - val_accuracy: 0.9667\n",
      "Epoch 1727/2500\n",
      "120/120 [==============================] - 0s 129us/step - loss: 0.0573 - accuracy: 0.9750 - val_loss: 0.0440 - val_accuracy: 0.9667\n",
      "Epoch 1728/2500\n",
      "120/120 [==============================] - 0s 128us/step - loss: 0.0558 - accuracy: 0.9833 - val_loss: 0.0460 - val_accuracy: 0.9667\n",
      "Epoch 1729/2500\n",
      "120/120 [==============================] - 0s 112us/step - loss: 0.0561 - accuracy: 0.9833 - val_loss: 0.0478 - val_accuracy: 0.9667\n",
      "Epoch 1730/2500\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.0597 - accuracy: 0.9833 - val_loss: 0.0578 - val_accuracy: 0.9667\n",
      "Epoch 1731/2500\n",
      "120/120 [==============================] - 0s 104us/step - loss: 0.0579 - accuracy: 0.9833 - val_loss: 0.0433 - val_accuracy: 0.9667\n",
      "Epoch 1732/2500\n",
      "120/120 [==============================] - 0s 103us/step - loss: 0.0562 - accuracy: 0.9750 - val_loss: 0.0403 - val_accuracy: 0.9667\n",
      "Epoch 1733/2500\n",
      "120/120 [==============================] - 0s 101us/step - loss: 0.0570 - accuracy: 0.9750 - val_loss: 0.0425 - val_accuracy: 0.9667\n",
      "Epoch 1734/2500\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.0564 - accuracy: 0.9833 - val_loss: 0.0461 - val_accuracy: 0.9667\n",
      "Epoch 1735/2500\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0572 - accuracy: 0.9833 - val_loss: 0.0497 - val_accuracy: 0.9667\n",
      "Epoch 1736/2500\n",
      "120/120 [==============================] - 0s 110us/step - loss: 0.0561 - accuracy: 0.9833 - val_loss: 0.0475 - val_accuracy: 0.9667\n",
      "Epoch 1737/2500\n",
      "120/120 [==============================] - 0s 92us/step - loss: 0.0568 - accuracy: 0.9833 - val_loss: 0.0421 - val_accuracy: 0.9667\n",
      "Epoch 1738/2500\n",
      "120/120 [==============================] - 0s 93us/step - loss: 0.0565 - accuracy: 0.9750 - val_loss: 0.0430 - val_accuracy: 0.9667\n",
      "Epoch 1739/2500\n",
      "120/120 [==============================] - 0s 113us/step - loss: 0.0565 - accuracy: 0.9833 - val_loss: 0.0475 - val_accuracy: 0.9667\n",
      "Epoch 1740/2500\n",
      "120/120 [==============================] - 0s 104us/step - loss: 0.0566 - accuracy: 0.9833 - val_loss: 0.0444 - val_accuracy: 0.9667\n",
      "Epoch 1741/2500\n",
      "120/120 [==============================] - 0s 101us/step - loss: 0.0568 - accuracy: 0.9833 - val_loss: 0.0489 - val_accuracy: 0.9667\n",
      "Epoch 1742/2500\n",
      "120/120 [==============================] - 0s 101us/step - loss: 0.0564 - accuracy: 0.9833 - val_loss: 0.0476 - val_accuracy: 0.9667\n",
      "Epoch 1743/2500\n",
      "120/120 [==============================] - 0s 124us/step - loss: 0.0562 - accuracy: 0.9833 - val_loss: 0.0432 - val_accuracy: 0.9667\n",
      "Epoch 1744/2500\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.0560 - accuracy: 0.9833 - val_loss: 0.0431 - val_accuracy: 0.9667\n",
      "Epoch 1745/2500\n",
      "120/120 [==============================] - 0s 98us/step - loss: 0.0564 - accuracy: 0.9833 - val_loss: 0.0433 - val_accuracy: 0.9667\n",
      "Epoch 1746/2500\n",
      "120/120 [==============================] - 0s 135us/step - loss: 0.0568 - accuracy: 0.9750 - val_loss: 0.0447 - val_accuracy: 0.9667\n",
      "Epoch 1747/2500\n",
      "120/120 [==============================] - 0s 100us/step - loss: 0.0559 - accuracy: 0.9833 - val_loss: 0.0458 - val_accuracy: 0.9667\n",
      "Epoch 1748/2500\n",
      "120/120 [==============================] - 0s 88us/step - loss: 0.0573 - accuracy: 0.9833 - val_loss: 0.0521 - val_accuracy: 0.9667\n",
      "Epoch 1749/2500\n",
      "120/120 [==============================] - 0s 113us/step - loss: 0.0582 - accuracy: 0.9833 - val_loss: 0.0411 - val_accuracy: 0.9667\n",
      "Epoch 1750/2500\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0559 - accuracy: 0.9750 - val_loss: 0.0420 - val_accuracy: 0.9667\n",
      "Epoch 1751/2500\n",
      "120/120 [==============================] - 0s 92us/step - loss: 0.0559 - accuracy: 0.9833 - val_loss: 0.0458 - val_accuracy: 0.9667\n",
      "Epoch 1752/2500\n",
      "120/120 [==============================] - 0s 99us/step - loss: 0.0567 - accuracy: 0.9833 - val_loss: 0.0428 - val_accuracy: 0.9667\n",
      "Epoch 1753/2500\n",
      "120/120 [==============================] - 0s 105us/step - loss: 0.0571 - accuracy: 0.9750 - val_loss: 0.0418 - val_accuracy: 0.9667\n",
      "Epoch 1754/2500\n",
      "120/120 [==============================] - 0s 121us/step - loss: 0.0569 - accuracy: 0.9833 - val_loss: 0.0521 - val_accuracy: 0.9667\n",
      "Epoch 1755/2500\n",
      "120/120 [==============================] - 0s 123us/step - loss: 0.0586 - accuracy: 0.9833 - val_loss: 0.0454 - val_accuracy: 0.9667\n",
      "Epoch 1756/2500\n",
      "120/120 [==============================] - 0s 177us/step - loss: 0.0564 - accuracy: 0.9833 - val_loss: 0.0505 - val_accuracy: 0.9667\n",
      "Epoch 1757/2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 171us/step - loss: 0.0569 - accuracy: 0.9833 - val_loss: 0.0515 - val_accuracy: 0.9667\n",
      "Epoch 1758/2500\n",
      "120/120 [==============================] - 0s 194us/step - loss: 0.0572 - accuracy: 0.9833 - val_loss: 0.0434 - val_accuracy: 0.9667\n",
      "Epoch 1759/2500\n",
      "120/120 [==============================] - 0s 161us/step - loss: 0.0567 - accuracy: 0.9833 - val_loss: 0.0402 - val_accuracy: 0.9667\n",
      "Epoch 1760/2500\n",
      "120/120 [==============================] - 0s 140us/step - loss: 0.0561 - accuracy: 0.9750 - val_loss: 0.0435 - val_accuracy: 0.9667\n",
      "Epoch 1761/2500\n",
      "120/120 [==============================] - 0s 136us/step - loss: 0.0560 - accuracy: 0.9833 - val_loss: 0.0469 - val_accuracy: 0.9667\n",
      "Epoch 1762/2500\n",
      "120/120 [==============================] - 0s 110us/step - loss: 0.0559 - accuracy: 0.9833 - val_loss: 0.0466 - val_accuracy: 0.9667\n",
      "Epoch 1763/2500\n",
      "120/120 [==============================] - 0s 96us/step - loss: 0.0589 - accuracy: 0.9833 - val_loss: 0.0539 - val_accuracy: 0.9667\n",
      "Epoch 1764/2500\n",
      "120/120 [==============================] - 0s 128us/step - loss: 0.0569 - accuracy: 0.9833 - val_loss: 0.0418 - val_accuracy: 0.9667\n",
      "Epoch 1765/2500\n",
      "120/120 [==============================] - 0s 90us/step - loss: 0.0569 - accuracy: 0.9750 - val_loss: 0.0395 - val_accuracy: 0.9667\n",
      "Epoch 1766/2500\n",
      "120/120 [==============================] - 0s 97us/step - loss: 0.0582 - accuracy: 0.9750 - val_loss: 0.0459 - val_accuracy: 0.9667\n",
      "Epoch 1767/2500\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0569 - accuracy: 0.9833 - val_loss: 0.0414 - val_accuracy: 0.9667\n",
      "Epoch 1768/2500\n",
      "120/120 [==============================] - 0s 113us/step - loss: 0.0562 - accuracy: 0.9750 - val_loss: 0.0435 - val_accuracy: 0.9667\n",
      "Epoch 1769/2500\n",
      "120/120 [==============================] - 0s 90us/step - loss: 0.0579 - accuracy: 0.9833 - val_loss: 0.0527 - val_accuracy: 0.9667\n",
      "Epoch 1770/2500\n",
      "120/120 [==============================] - 0s 87us/step - loss: 0.0592 - accuracy: 0.9833 - val_loss: 0.0418 - val_accuracy: 0.9667\n",
      "Epoch 1771/2500\n",
      "120/120 [==============================] - 0s 122us/step - loss: 0.0565 - accuracy: 0.9833 - val_loss: 0.0467 - val_accuracy: 0.9667\n",
      "Epoch 1772/2500\n",
      "120/120 [==============================] - 0s 104us/step - loss: 0.0564 - accuracy: 0.9833 - val_loss: 0.0473 - val_accuracy: 0.9667\n",
      "Epoch 1773/2500\n",
      "120/120 [==============================] - 0s 102us/step - loss: 0.0560 - accuracy: 0.9833 - val_loss: 0.0434 - val_accuracy: 0.9667\n",
      "Epoch 1774/2500\n",
      "120/120 [==============================] - 0s 104us/step - loss: 0.0560 - accuracy: 0.9833 - val_loss: 0.0434 - val_accuracy: 0.9667\n",
      "Epoch 1775/2500\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0562 - accuracy: 0.9833 - val_loss: 0.0444 - val_accuracy: 0.9667\n",
      "Epoch 1776/2500\n",
      "120/120 [==============================] - 0s 102us/step - loss: 0.0567 - accuracy: 0.9833 - val_loss: 0.0466 - val_accuracy: 0.9667\n",
      "Epoch 1777/2500\n",
      "120/120 [==============================] - 0s 91us/step - loss: 0.0566 - accuracy: 0.9833 - val_loss: 0.0482 - val_accuracy: 0.9667\n",
      "Epoch 1778/2500\n",
      "120/120 [==============================] - 0s 109us/step - loss: 0.0562 - accuracy: 0.9833 - val_loss: 0.0428 - val_accuracy: 0.9667\n",
      "Epoch 1779/2500\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.0560 - accuracy: 0.9833 - val_loss: 0.0433 - val_accuracy: 0.9667\n",
      "Epoch 1780/2500\n",
      "120/120 [==============================] - 0s 103us/step - loss: 0.0572 - accuracy: 0.9750 - val_loss: 0.0401 - val_accuracy: 0.9667\n",
      "Epoch 1781/2500\n",
      "120/120 [==============================] - 0s 91us/step - loss: 0.0569 - accuracy: 0.9833 - val_loss: 0.0492 - val_accuracy: 0.9667\n",
      "Epoch 1782/2500\n",
      "120/120 [==============================] - 0s 102us/step - loss: 0.0562 - accuracy: 0.9833 - val_loss: 0.0456 - val_accuracy: 0.9667\n",
      "Epoch 1783/2500\n",
      "120/120 [==============================] - 0s 99us/step - loss: 0.0563 - accuracy: 0.9833 - val_loss: 0.0433 - val_accuracy: 0.9667\n",
      "Epoch 1784/2500\n",
      "120/120 [==============================] - 0s 99us/step - loss: 0.0568 - accuracy: 0.9833 - val_loss: 0.0457 - val_accuracy: 0.9667\n",
      "Epoch 1785/2500\n",
      "120/120 [==============================] - 0s 94us/step - loss: 0.0563 - accuracy: 0.9833 - val_loss: 0.0476 - val_accuracy: 0.9667\n",
      "Epoch 1786/2500\n",
      "120/120 [==============================] - 0s 109us/step - loss: 0.0566 - accuracy: 0.9833 - val_loss: 0.0443 - val_accuracy: 0.9667\n",
      "Epoch 1787/2500\n",
      "120/120 [==============================] - 0s 115us/step - loss: 0.0567 - accuracy: 0.9833 - val_loss: 0.0486 - val_accuracy: 0.9667\n",
      "Epoch 1788/2500\n",
      "120/120 [==============================] - 0s 107us/step - loss: 0.0567 - accuracy: 0.9750 - val_loss: 0.0427 - val_accuracy: 0.9667\n",
      "Epoch 1789/2500\n",
      "120/120 [==============================] - 0s 105us/step - loss: 0.0572 - accuracy: 0.9750 - val_loss: 0.0454 - val_accuracy: 0.9667\n",
      "Epoch 1790/2500\n",
      "120/120 [==============================] - 0s 146us/step - loss: 0.0556 - accuracy: 0.9833 - val_loss: 0.0455 - val_accuracy: 0.9667\n",
      "Epoch 1791/2500\n",
      "120/120 [==============================] - 0s 177us/step - loss: 0.0567 - accuracy: 0.9833 - val_loss: 0.0499 - val_accuracy: 0.9667\n",
      "Epoch 1792/2500\n",
      "120/120 [==============================] - 0s 139us/step - loss: 0.0556 - accuracy: 0.9833 - val_loss: 0.0474 - val_accuracy: 0.9667\n",
      "Epoch 1793/2500\n",
      "120/120 [==============================] - 0s 240us/step - loss: 0.0560 - accuracy: 0.9833 - val_loss: 0.0446 - val_accuracy: 0.9667\n",
      "Epoch 1794/2500\n",
      "120/120 [==============================] - 0s 162us/step - loss: 0.0573 - accuracy: 0.9833 - val_loss: 0.0457 - val_accuracy: 0.9667\n",
      "Epoch 1795/2500\n",
      "120/120 [==============================] - 0s 177us/step - loss: 0.0564 - accuracy: 0.9833 - val_loss: 0.0419 - val_accuracy: 0.9667\n",
      "Epoch 1796/2500\n",
      "120/120 [==============================] - 0s 155us/step - loss: 0.0572 - accuracy: 0.9750 - val_loss: 0.0393 - val_accuracy: 0.9667\n",
      "Epoch 1797/2500\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.0560 - accuracy: 0.9750 - val_loss: 0.0416 - val_accuracy: 0.9667\n",
      "Epoch 1798/2500\n",
      "120/120 [==============================] - 0s 107us/step - loss: 0.0556 - accuracy: 0.9750 - val_loss: 0.0450 - val_accuracy: 0.9667\n",
      "Epoch 1799/2500\n",
      "120/120 [==============================] - 0s 110us/step - loss: 0.0556 - accuracy: 0.9833 - val_loss: 0.0469 - val_accuracy: 0.9667\n",
      "Epoch 1800/2500\n",
      "120/120 [==============================] - 0s 104us/step - loss: 0.0584 - accuracy: 0.9833 - val_loss: 0.0557 - val_accuracy: 0.9667\n",
      "Epoch 1801/2500\n",
      "120/120 [==============================] - 0s 116us/step - loss: 0.0569 - accuracy: 0.9833 - val_loss: 0.0449 - val_accuracy: 0.9667\n",
      "Epoch 1802/2500\n",
      "120/120 [==============================] - 0s 114us/step - loss: 0.0561 - accuracy: 0.9833 - val_loss: 0.0405 - val_accuracy: 0.9667\n",
      "Epoch 1803/2500\n",
      "120/120 [==============================] - 0s 100us/step - loss: 0.0558 - accuracy: 0.9833 - val_loss: 0.0434 - val_accuracy: 0.9667\n",
      "Epoch 1804/2500\n",
      "120/120 [==============================] - 0s 104us/step - loss: 0.0554 - accuracy: 0.9833 - val_loss: 0.0443 - val_accuracy: 0.9667\n",
      "Epoch 1805/2500\n",
      "120/120 [==============================] - 0s 102us/step - loss: 0.0563 - accuracy: 0.9833 - val_loss: 0.0410 - val_accuracy: 0.9667\n",
      "Epoch 1806/2500\n",
      "120/120 [==============================] - 0s 116us/step - loss: 0.0560 - accuracy: 0.9750 - val_loss: 0.0429 - val_accuracy: 0.9667\n",
      "Epoch 1807/2500\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0584 - accuracy: 0.9833 - val_loss: 0.0515 - val_accuracy: 0.9667\n",
      "Epoch 1808/2500\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0564 - accuracy: 0.9833 - val_loss: 0.0512 - val_accuracy: 0.9667\n",
      "Epoch 1809/2500\n",
      "120/120 [==============================] - 0s 145us/step - loss: 0.0562 - accuracy: 0.9833 - val_loss: 0.0427 - val_accuracy: 0.9667\n",
      "Epoch 1810/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.0566 - accuracy: 0.9750 - val_loss: 0.0380 - val_accuracy: 1.0000\n",
      "Epoch 1811/2500\n",
      "120/120 [==============================] - 0s 110us/step - loss: 0.0561 - accuracy: 0.9750 - val_loss: 0.0405 - val_accuracy: 0.9667\n",
      "Epoch 1812/2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 114us/step - loss: 0.0581 - accuracy: 0.9833 - val_loss: 0.0506 - val_accuracy: 0.9667\n",
      "Epoch 1813/2500\n",
      "120/120 [==============================] - 0s 97us/step - loss: 0.0574 - accuracy: 0.9833 - val_loss: 0.0546 - val_accuracy: 0.9667\n",
      "Epoch 1814/2500\n",
      "120/120 [==============================] - 0s 109us/step - loss: 0.0576 - accuracy: 0.9833 - val_loss: 0.0410 - val_accuracy: 0.9667\n",
      "Epoch 1815/2500\n",
      "120/120 [==============================] - 0s 107us/step - loss: 0.0580 - accuracy: 0.9750 - val_loss: 0.0456 - val_accuracy: 0.9667\n",
      "Epoch 1816/2500\n",
      "120/120 [==============================] - 0s 109us/step - loss: 0.0572 - accuracy: 0.9750 - val_loss: 0.0426 - val_accuracy: 0.9667\n",
      "Epoch 1817/2500\n",
      "120/120 [==============================] - 0s 98us/step - loss: 0.0561 - accuracy: 0.9833 - val_loss: 0.0466 - val_accuracy: 0.9667\n",
      "Epoch 1818/2500\n",
      "120/120 [==============================] - 0s 137us/step - loss: 0.0555 - accuracy: 0.9833 - val_loss: 0.0465 - val_accuracy: 0.9667\n",
      "Epoch 1819/2500\n",
      "120/120 [==============================] - 0s 139us/step - loss: 0.0565 - accuracy: 0.9833 - val_loss: 0.0500 - val_accuracy: 0.9667\n",
      "Epoch 1820/2500\n",
      "120/120 [==============================] - 0s 238us/step - loss: 0.0570 - accuracy: 0.9833 - val_loss: 0.0405 - val_accuracy: 0.9667\n",
      "Epoch 1821/2500\n",
      "120/120 [==============================] - 0s 165us/step - loss: 0.0559 - accuracy: 0.9750 - val_loss: 0.0407 - val_accuracy: 0.9667\n",
      "Epoch 1822/2500\n",
      "120/120 [==============================] - 0s 160us/step - loss: 0.0561 - accuracy: 0.9833 - val_loss: 0.0446 - val_accuracy: 0.9667\n",
      "Epoch 1823/2500\n",
      "120/120 [==============================] - 0s 147us/step - loss: 0.0556 - accuracy: 0.9833 - val_loss: 0.0441 - val_accuracy: 0.9667\n",
      "Epoch 1824/2500\n",
      "120/120 [==============================] - 0s 119us/step - loss: 0.0560 - accuracy: 0.9833 - val_loss: 0.0467 - val_accuracy: 0.9667\n",
      "Epoch 1825/2500\n",
      "120/120 [==============================] - 0s 119us/step - loss: 0.0567 - accuracy: 0.9833 - val_loss: 0.0440 - val_accuracy: 0.9667\n",
      "Epoch 1826/2500\n",
      "120/120 [==============================] - 0s 118us/step - loss: 0.0554 - accuracy: 0.9833 - val_loss: 0.0460 - val_accuracy: 0.9667\n",
      "Epoch 1827/2500\n",
      "120/120 [==============================] - 0s 112us/step - loss: 0.0560 - accuracy: 0.9833 - val_loss: 0.0493 - val_accuracy: 0.9667\n",
      "Epoch 1828/2500\n",
      "120/120 [==============================] - 0s 110us/step - loss: 0.0567 - accuracy: 0.9833 - val_loss: 0.0414 - val_accuracy: 0.9667\n",
      "Epoch 1829/2500\n",
      "120/120 [==============================] - 0s 105us/step - loss: 0.0562 - accuracy: 0.9750 - val_loss: 0.0466 - val_accuracy: 0.9667\n",
      "Epoch 1830/2500\n",
      "120/120 [==============================] - 0s 116us/step - loss: 0.0558 - accuracy: 0.9833 - val_loss: 0.0456 - val_accuracy: 0.9667\n",
      "Epoch 1831/2500\n",
      "120/120 [==============================] - 0s 156us/step - loss: 0.0559 - accuracy: 0.9750 - val_loss: 0.0431 - val_accuracy: 0.9667\n",
      "Epoch 1832/2500\n",
      "120/120 [==============================] - 0s 130us/step - loss: 0.0556 - accuracy: 0.9833 - val_loss: 0.0457 - val_accuracy: 0.9667\n",
      "Epoch 1833/2500\n",
      "120/120 [==============================] - 0s 122us/step - loss: 0.0574 - accuracy: 0.9833 - val_loss: 0.0510 - val_accuracy: 0.9667\n",
      "Epoch 1834/2500\n",
      "120/120 [==============================] - 0s 99us/step - loss: 0.0551 - accuracy: 0.9833 - val_loss: 0.0444 - val_accuracy: 0.9667\n",
      "Epoch 1835/2500\n",
      "120/120 [==============================] - 0s 107us/step - loss: 0.0573 - accuracy: 0.9750 - val_loss: 0.0384 - val_accuracy: 0.9667\n",
      "Epoch 1836/2500\n",
      "120/120 [==============================] - 0s 113us/step - loss: 0.0576 - accuracy: 0.9750 - val_loss: 0.0464 - val_accuracy: 0.9667\n",
      "Epoch 1837/2500\n",
      "120/120 [==============================] - 0s 105us/step - loss: 0.0556 - accuracy: 0.9833 - val_loss: 0.0436 - val_accuracy: 0.9667\n",
      "Epoch 1838/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.0557 - accuracy: 0.9833 - val_loss: 0.0439 - val_accuracy: 0.9667\n",
      "Epoch 1839/2500\n",
      "120/120 [==============================] - 0s 122us/step - loss: 0.0578 - accuracy: 0.9833 - val_loss: 0.0502 - val_accuracy: 0.9667\n",
      "Epoch 1840/2500\n",
      "120/120 [==============================] - 0s 124us/step - loss: 0.0564 - accuracy: 0.9833 - val_loss: 0.0467 - val_accuracy: 0.9667\n",
      "Epoch 1841/2500\n",
      "120/120 [==============================] - 0s 114us/step - loss: 0.0552 - accuracy: 0.9833 - val_loss: 0.0404 - val_accuracy: 0.9667\n",
      "Epoch 1842/2500\n",
      "120/120 [==============================] - 0s 114us/step - loss: 0.0568 - accuracy: 0.9750 - val_loss: 0.0425 - val_accuracy: 0.9667\n",
      "Epoch 1843/2500\n",
      "120/120 [==============================] - 0s 141us/step - loss: 0.0555 - accuracy: 0.9833 - val_loss: 0.0427 - val_accuracy: 0.9667\n",
      "Epoch 1844/2500\n",
      "120/120 [==============================] - 0s 134us/step - loss: 0.0591 - accuracy: 0.9750 - val_loss: 0.0367 - val_accuracy: 1.0000\n",
      "Epoch 1845/2500\n",
      "120/120 [==============================] - 0s 248us/step - loss: 0.0559 - accuracy: 0.9833 - val_loss: 0.0467 - val_accuracy: 0.9667\n",
      "Epoch 1846/2500\n",
      "120/120 [==============================] - 0s 140us/step - loss: 0.0563 - accuracy: 0.9833 - val_loss: 0.0524 - val_accuracy: 0.9667\n",
      "Epoch 1847/2500\n",
      "120/120 [==============================] - 0s 151us/step - loss: 0.0554 - accuracy: 0.9833 - val_loss: 0.0477 - val_accuracy: 0.9667\n",
      "Epoch 1848/2500\n",
      "120/120 [==============================] - 0s 120us/step - loss: 0.0557 - accuracy: 0.9833 - val_loss: 0.0477 - val_accuracy: 0.9667\n",
      "Epoch 1849/2500\n",
      "120/120 [==============================] - 0s 109us/step - loss: 0.0555 - accuracy: 0.9833 - val_loss: 0.0414 - val_accuracy: 0.9667\n",
      "Epoch 1850/2500\n",
      "120/120 [==============================] - 0s 118us/step - loss: 0.0570 - accuracy: 0.9750 - val_loss: 0.0380 - val_accuracy: 1.0000\n",
      "Epoch 1851/2500\n",
      "120/120 [==============================] - 0s 110us/step - loss: 0.0554 - accuracy: 0.9750 - val_loss: 0.0431 - val_accuracy: 0.9667\n",
      "Epoch 1852/2500\n",
      "120/120 [==============================] - 0s 92us/step - loss: 0.0565 - accuracy: 0.9833 - val_loss: 0.0413 - val_accuracy: 0.9667\n",
      "Epoch 1853/2500\n",
      "120/120 [==============================] - 0s 101us/step - loss: 0.0564 - accuracy: 0.9833 - val_loss: 0.0497 - val_accuracy: 0.9667\n",
      "Epoch 1854/2500\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0558 - accuracy: 0.9833 - val_loss: 0.0498 - val_accuracy: 0.9667\n",
      "Epoch 1855/2500\n",
      "120/120 [==============================] - 0s 100us/step - loss: 0.0582 - accuracy: 0.9750 - val_loss: 0.0429 - val_accuracy: 0.9667\n",
      "Epoch 1856/2500\n",
      "120/120 [==============================] - 0s 99us/step - loss: 0.0556 - accuracy: 0.9833 - val_loss: 0.0462 - val_accuracy: 0.9667\n",
      "Epoch 1857/2500\n",
      "120/120 [==============================] - 0s 119us/step - loss: 0.0558 - accuracy: 0.9833 - val_loss: 0.0445 - val_accuracy: 0.9667\n",
      "Epoch 1858/2500\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0568 - accuracy: 0.9833 - val_loss: 0.0497 - val_accuracy: 0.9667\n",
      "Epoch 1859/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.0554 - accuracy: 0.9833 - val_loss: 0.0431 - val_accuracy: 0.9667\n",
      "Epoch 1860/2500\n",
      "120/120 [==============================] - 0s 118us/step - loss: 0.0575 - accuracy: 0.9750 - val_loss: 0.0380 - val_accuracy: 1.0000\n",
      "Epoch 1861/2500\n",
      "120/120 [==============================] - 0s 112us/step - loss: 0.0566 - accuracy: 0.9750 - val_loss: 0.0457 - val_accuracy: 0.9667\n",
      "Epoch 1862/2500\n",
      "120/120 [==============================] - 0s 109us/step - loss: 0.0568 - accuracy: 0.9750 - val_loss: 0.0405 - val_accuracy: 0.9667\n",
      "Epoch 1863/2500\n",
      "120/120 [==============================] - 0s 112us/step - loss: 0.0559 - accuracy: 0.9750 - val_loss: 0.0434 - val_accuracy: 0.9667\n",
      "Epoch 1864/2500\n",
      "120/120 [==============================] - 0s 116us/step - loss: 0.0551 - accuracy: 0.9833 - val_loss: 0.0479 - val_accuracy: 0.9667\n",
      "Epoch 1865/2500\n",
      "120/120 [==============================] - 0s 110us/step - loss: 0.0558 - accuracy: 0.9833 - val_loss: 0.0503 - val_accuracy: 0.9667\n",
      "Epoch 1866/2500\n",
      "120/120 [==============================] - 0s 126us/step - loss: 0.0572 - accuracy: 0.9833 - val_loss: 0.0523 - val_accuracy: 0.9667\n",
      "Epoch 1867/2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 122us/step - loss: 0.0552 - accuracy: 0.9833 - val_loss: 0.0457 - val_accuracy: 0.9667\n",
      "Epoch 1868/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.0564 - accuracy: 0.9833 - val_loss: 0.0374 - val_accuracy: 1.0000\n",
      "Epoch 1869/2500\n",
      "120/120 [==============================] - 0s 114us/step - loss: 0.0561 - accuracy: 0.9750 - val_loss: 0.0405 - val_accuracy: 0.9667\n",
      "Epoch 1870/2500\n",
      "120/120 [==============================] - 0s 127us/step - loss: 0.0552 - accuracy: 0.9750 - val_loss: 0.0426 - val_accuracy: 0.9667\n",
      "Epoch 1871/2500\n",
      "120/120 [==============================] - 0s 139us/step - loss: 0.0559 - accuracy: 0.9833 - val_loss: 0.0451 - val_accuracy: 0.9667\n",
      "Epoch 1872/2500\n",
      "120/120 [==============================] - 0s 121us/step - loss: 0.0583 - accuracy: 0.9750 - val_loss: 0.0385 - val_accuracy: 0.9667\n",
      "Epoch 1873/2500\n",
      "120/120 [==============================] - 0s 128us/step - loss: 0.0550 - accuracy: 0.9833 - val_loss: 0.0450 - val_accuracy: 0.9667\n",
      "Epoch 1874/2500\n",
      "120/120 [==============================] - 0s 119us/step - loss: 0.0556 - accuracy: 0.9833 - val_loss: 0.0497 - val_accuracy: 0.9667\n",
      "Epoch 1875/2500\n",
      "120/120 [==============================] - 0s 112us/step - loss: 0.0558 - accuracy: 0.9833 - val_loss: 0.0479 - val_accuracy: 0.9667\n",
      "Epoch 1876/2500\n",
      "120/120 [==============================] - 0s 116us/step - loss: 0.0557 - accuracy: 0.9833 - val_loss: 0.0427 - val_accuracy: 0.9667\n",
      "Epoch 1877/2500\n",
      "120/120 [==============================] - 0s 119us/step - loss: 0.0571 - accuracy: 0.9750 - val_loss: 0.0379 - val_accuracy: 1.0000\n",
      "Epoch 1878/2500\n",
      "120/120 [==============================] - 0s 112us/step - loss: 0.0551 - accuracy: 0.9833 - val_loss: 0.0437 - val_accuracy: 0.9667\n",
      "Epoch 1879/2500\n",
      "120/120 [==============================] - 0s 122us/step - loss: 0.0579 - accuracy: 0.9833 - val_loss: 0.0517 - val_accuracy: 0.9667\n",
      "Epoch 1880/2500\n",
      "120/120 [==============================] - 0s 127us/step - loss: 0.0553 - accuracy: 0.9833 - val_loss: 0.0472 - val_accuracy: 0.9667\n",
      "Epoch 1881/2500\n",
      "120/120 [==============================] - 0s 134us/step - loss: 0.0554 - accuracy: 0.9833 - val_loss: 0.0446 - val_accuracy: 0.9667\n",
      "Epoch 1882/2500\n",
      "120/120 [==============================] - 0s 121us/step - loss: 0.0580 - accuracy: 0.9750 - val_loss: 0.0375 - val_accuracy: 1.0000\n",
      "Epoch 1883/2500\n",
      "120/120 [==============================] - 0s 120us/step - loss: 0.0560 - accuracy: 0.9750 - val_loss: 0.0417 - val_accuracy: 0.9667\n",
      "Epoch 1884/2500\n",
      "120/120 [==============================] - 0s 113us/step - loss: 0.0560 - accuracy: 0.9833 - val_loss: 0.0415 - val_accuracy: 0.9667\n",
      "Epoch 1885/2500\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0557 - accuracy: 0.9750 - val_loss: 0.0468 - val_accuracy: 0.9667\n",
      "Epoch 1886/2500\n",
      "120/120 [==============================] - 0s 110us/step - loss: 0.0560 - accuracy: 0.9833 - val_loss: 0.0487 - val_accuracy: 0.9667\n",
      "Epoch 1887/2500\n",
      "120/120 [==============================] - 0s 115us/step - loss: 0.0591 - accuracy: 0.9750 - val_loss: 0.0403 - val_accuracy: 0.9667\n",
      "Epoch 1888/2500\n",
      "120/120 [==============================] - 0s 107us/step - loss: 0.0560 - accuracy: 0.9750 - val_loss: 0.0475 - val_accuracy: 0.9667\n",
      "Epoch 1889/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.0564 - accuracy: 0.9833 - val_loss: 0.0523 - val_accuracy: 0.9667\n",
      "Epoch 1890/2500\n",
      "120/120 [==============================] - 0s 99us/step - loss: 0.0559 - accuracy: 0.9833 - val_loss: 0.0432 - val_accuracy: 0.9667\n",
      "Epoch 1891/2500\n",
      "120/120 [==============================] - 0s 88us/step - loss: 0.0563 - accuracy: 0.9833 - val_loss: 0.0407 - val_accuracy: 0.9667\n",
      "Epoch 1892/2500\n",
      "120/120 [==============================] - 0s 105us/step - loss: 0.0553 - accuracy: 0.9750 - val_loss: 0.0442 - val_accuracy: 0.9667\n",
      "Epoch 1893/2500\n",
      "120/120 [==============================] - 0s 126us/step - loss: 0.0559 - accuracy: 0.9833 - val_loss: 0.0468 - val_accuracy: 0.9667\n",
      "Epoch 1894/2500\n",
      "120/120 [==============================] - 0s 102us/step - loss: 0.0553 - accuracy: 0.9833 - val_loss: 0.0460 - val_accuracy: 0.9667\n",
      "Epoch 1895/2500\n",
      "120/120 [==============================] - 0s 113us/step - loss: 0.0557 - accuracy: 0.9833 - val_loss: 0.0436 - val_accuracy: 0.9667\n",
      "Epoch 1896/2500\n",
      "120/120 [==============================] - 0s 136us/step - loss: 0.0563 - accuracy: 0.9833 - val_loss: 0.0392 - val_accuracy: 0.9667\n",
      "Epoch 1897/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.0577 - accuracy: 0.9750 - val_loss: 0.0472 - val_accuracy: 0.9667\n",
      "Epoch 1898/2500\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0567 - accuracy: 0.9833 - val_loss: 0.0406 - val_accuracy: 0.9667\n",
      "Epoch 1899/2500\n",
      "120/120 [==============================] - 0s 109us/step - loss: 0.0549 - accuracy: 0.9833 - val_loss: 0.0430 - val_accuracy: 0.9667\n",
      "Epoch 1900/2500\n",
      "120/120 [==============================] - 0s 102us/step - loss: 0.0552 - accuracy: 0.9833 - val_loss: 0.0468 - val_accuracy: 0.9667\n",
      "Epoch 1901/2500\n",
      "120/120 [==============================] - 0s 115us/step - loss: 0.0563 - accuracy: 0.9833 - val_loss: 0.0422 - val_accuracy: 0.9667\n",
      "Epoch 1902/2500\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0564 - accuracy: 0.9750 - val_loss: 0.0393 - val_accuracy: 0.9667\n",
      "Epoch 1903/2500\n",
      "120/120 [==============================] - 0s 124us/step - loss: 0.0554 - accuracy: 0.9833 - val_loss: 0.0459 - val_accuracy: 0.9667\n",
      "Epoch 1904/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.0560 - accuracy: 0.9833 - val_loss: 0.0513 - val_accuracy: 0.9667\n",
      "Epoch 1905/2500\n",
      "120/120 [==============================] - 0s 107us/step - loss: 0.0584 - accuracy: 0.9833 - val_loss: 0.0552 - val_accuracy: 0.9667\n",
      "Epoch 1906/2500\n",
      "120/120 [==============================] - 0s 127us/step - loss: 0.0607 - accuracy: 0.9833 - val_loss: 0.0354 - val_accuracy: 1.0000\n",
      "Epoch 1907/2500\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0581 - accuracy: 0.9750 - val_loss: 0.0406 - val_accuracy: 0.9667\n",
      "Epoch 1908/2500\n",
      "120/120 [==============================] - 0s 122us/step - loss: 0.0560 - accuracy: 0.9833 - val_loss: 0.0431 - val_accuracy: 0.9667\n",
      "Epoch 1909/2500\n",
      "120/120 [==============================] - 0s 134us/step - loss: 0.0567 - accuracy: 0.9833 - val_loss: 0.0388 - val_accuracy: 0.9667\n",
      "Epoch 1910/2500\n",
      "120/120 [==============================] - 0s 140us/step - loss: 0.0551 - accuracy: 0.9750 - val_loss: 0.0398 - val_accuracy: 0.9667\n",
      "Epoch 1911/2500\n",
      "120/120 [==============================] - 0s 129us/step - loss: 0.0549 - accuracy: 0.9750 - val_loss: 0.0437 - val_accuracy: 0.9667\n",
      "Epoch 1912/2500\n",
      "120/120 [==============================] - 0s 115us/step - loss: 0.0557 - accuracy: 0.9833 - val_loss: 0.0499 - val_accuracy: 0.9667\n",
      "Epoch 1913/2500\n",
      "120/120 [==============================] - 0s 115us/step - loss: 0.0552 - accuracy: 0.9833 - val_loss: 0.0450 - val_accuracy: 0.9667\n",
      "Epoch 1914/2500\n",
      "120/120 [==============================] - 0s 118us/step - loss: 0.0552 - accuracy: 0.9833 - val_loss: 0.0437 - val_accuracy: 0.9667\n",
      "Epoch 1915/2500\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0557 - accuracy: 0.9833 - val_loss: 0.0427 - val_accuracy: 0.9667\n",
      "Epoch 1916/2500\n",
      "120/120 [==============================] - 0s 105us/step - loss: 0.0554 - accuracy: 0.9833 - val_loss: 0.0439 - val_accuracy: 0.9667\n",
      "Epoch 1917/2500\n",
      "120/120 [==============================] - 0s 96us/step - loss: 0.0580 - accuracy: 0.9750 - val_loss: 0.0384 - val_accuracy: 0.9667\n",
      "Epoch 1918/2500\n",
      "120/120 [==============================] - 0s 107us/step - loss: 0.0560 - accuracy: 0.9833 - val_loss: 0.0493 - val_accuracy: 0.9667\n",
      "Epoch 1919/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.0570 - accuracy: 0.9833 - val_loss: 0.0453 - val_accuracy: 0.9667\n",
      "Epoch 1920/2500\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.0552 - accuracy: 0.9833 - val_loss: 0.0467 - val_accuracy: 0.9667\n",
      "Epoch 1921/2500\n",
      "120/120 [==============================] - 0s 104us/step - loss: 0.0564 - accuracy: 0.9833 - val_loss: 0.0469 - val_accuracy: 0.9667\n",
      "Epoch 1922/2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 109us/step - loss: 0.0559 - accuracy: 0.9833 - val_loss: 0.0460 - val_accuracy: 0.9667\n",
      "Epoch 1923/2500\n",
      "120/120 [==============================] - 0s 100us/step - loss: 0.0559 - accuracy: 0.9750 - val_loss: 0.0401 - val_accuracy: 0.9667\n",
      "Epoch 1924/2500\n",
      "120/120 [==============================] - 0s 100us/step - loss: 0.0564 - accuracy: 0.9750 - val_loss: 0.0396 - val_accuracy: 0.9667\n",
      "Epoch 1925/2500\n",
      "120/120 [==============================] - 0s 96us/step - loss: 0.0552 - accuracy: 0.9833 - val_loss: 0.0427 - val_accuracy: 0.9667\n",
      "Epoch 1926/2500\n",
      "120/120 [==============================] - 0s 113us/step - loss: 0.0562 - accuracy: 0.9750 - val_loss: 0.0418 - val_accuracy: 0.9667\n",
      "Epoch 1927/2500\n",
      "120/120 [==============================] - 0s 115us/step - loss: 0.0566 - accuracy: 0.9833 - val_loss: 0.0512 - val_accuracy: 0.9667\n",
      "Epoch 1928/2500\n",
      "120/120 [==============================] - 0s 105us/step - loss: 0.0560 - accuracy: 0.9833 - val_loss: 0.0453 - val_accuracy: 0.9667\n",
      "Epoch 1929/2500\n",
      "120/120 [==============================] - 0s 109us/step - loss: 0.0553 - accuracy: 0.9833 - val_loss: 0.0478 - val_accuracy: 0.9667\n",
      "Epoch 1930/2500\n",
      "120/120 [==============================] - 0s 103us/step - loss: 0.0556 - accuracy: 0.9750 - val_loss: 0.0412 - val_accuracy: 0.9667\n",
      "Epoch 1931/2500\n",
      "120/120 [==============================] - 0s 91us/step - loss: 0.0554 - accuracy: 0.9750 - val_loss: 0.0437 - val_accuracy: 0.9667\n",
      "Epoch 1932/2500\n",
      "120/120 [==============================] - 0s 99us/step - loss: 0.0549 - accuracy: 0.9833 - val_loss: 0.0439 - val_accuracy: 0.9667\n",
      "Epoch 1933/2500\n",
      "120/120 [==============================] - 0s 109us/step - loss: 0.0551 - accuracy: 0.9833 - val_loss: 0.0445 - val_accuracy: 0.9667\n",
      "Epoch 1934/2500\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0575 - accuracy: 0.9750 - val_loss: 0.0379 - val_accuracy: 1.0000\n",
      "Epoch 1935/2500\n",
      "120/120 [==============================] - 0s 96us/step - loss: 0.0554 - accuracy: 0.9833 - val_loss: 0.0453 - val_accuracy: 0.9667\n",
      "Epoch 1936/2500\n",
      "120/120 [==============================] - 0s 91us/step - loss: 0.0569 - accuracy: 0.9833 - val_loss: 0.0509 - val_accuracy: 0.9667\n",
      "Epoch 1937/2500\n",
      "120/120 [==============================] - 0s 114us/step - loss: 0.0552 - accuracy: 0.9833 - val_loss: 0.0454 - val_accuracy: 0.9667\n",
      "Epoch 1938/2500\n",
      "120/120 [==============================] - 0s 104us/step - loss: 0.0562 - accuracy: 0.9750 - val_loss: 0.0410 - val_accuracy: 0.9667\n",
      "Epoch 1939/2500\n",
      "120/120 [==============================] - 0s 101us/step - loss: 0.0554 - accuracy: 0.9750 - val_loss: 0.0406 - val_accuracy: 0.9667\n",
      "Epoch 1940/2500\n",
      "120/120 [==============================] - 0s 113us/step - loss: 0.0556 - accuracy: 0.9750 - val_loss: 0.0400 - val_accuracy: 0.9667\n",
      "Epoch 1941/2500\n",
      "120/120 [==============================] - 0s 101us/step - loss: 0.0578 - accuracy: 0.9750 - val_loss: 0.0523 - val_accuracy: 0.9667\n",
      "Epoch 1942/2500\n",
      "120/120 [==============================] - 0s 105us/step - loss: 0.0566 - accuracy: 0.9833 - val_loss: 0.0447 - val_accuracy: 0.9667\n",
      "Epoch 1943/2500\n",
      "120/120 [==============================] - 0s 102us/step - loss: 0.0554 - accuracy: 0.9833 - val_loss: 0.0436 - val_accuracy: 0.9667\n",
      "Epoch 1944/2500\n",
      "120/120 [==============================] - 0s 110us/step - loss: 0.0556 - accuracy: 0.9833 - val_loss: 0.0445 - val_accuracy: 0.9667\n",
      "Epoch 1945/2500\n",
      "120/120 [==============================] - 0s 102us/step - loss: 0.0552 - accuracy: 0.9833 - val_loss: 0.0413 - val_accuracy: 0.9667\n",
      "Epoch 1946/2500\n",
      "120/120 [==============================] - 0s 93us/step - loss: 0.0550 - accuracy: 0.9833 - val_loss: 0.0433 - val_accuracy: 0.9667\n",
      "Epoch 1947/2500\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.0572 - accuracy: 0.9833 - val_loss: 0.0498 - val_accuracy: 0.9667\n",
      "Epoch 1948/2500\n",
      "120/120 [==============================] - 0s 121us/step - loss: 0.0549 - accuracy: 0.9833 - val_loss: 0.0456 - val_accuracy: 0.9667\n",
      "Epoch 1949/2500\n",
      "120/120 [==============================] - 0s 113us/step - loss: 0.0555 - accuracy: 0.9833 - val_loss: 0.0443 - val_accuracy: 0.9667\n",
      "Epoch 1950/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.0547 - accuracy: 0.9833 - val_loss: 0.0407 - val_accuracy: 0.9667\n",
      "Epoch 1951/2500\n",
      "120/120 [==============================] - 0s 109us/step - loss: 0.0574 - accuracy: 0.9750 - val_loss: 0.0467 - val_accuracy: 0.9667\n",
      "Epoch 1952/2500\n",
      "120/120 [==============================] - 0s 107us/step - loss: 0.0551 - accuracy: 0.9833 - val_loss: 0.0439 - val_accuracy: 0.9667\n",
      "Epoch 1953/2500\n",
      "120/120 [==============================] - 0s 96us/step - loss: 0.0551 - accuracy: 0.9833 - val_loss: 0.0434 - val_accuracy: 0.9667\n",
      "Epoch 1954/2500\n",
      "120/120 [==============================] - 0s 115us/step - loss: 0.0544 - accuracy: 0.9833 - val_loss: 0.0410 - val_accuracy: 0.9667\n",
      "Epoch 1955/2500\n",
      "120/120 [==============================] - 0s 110us/step - loss: 0.0561 - accuracy: 0.9750 - val_loss: 0.0378 - val_accuracy: 1.0000\n",
      "Epoch 1956/2500\n",
      "120/120 [==============================] - 0s 116us/step - loss: 0.0557 - accuracy: 0.9750 - val_loss: 0.0421 - val_accuracy: 0.9667\n",
      "Epoch 1957/2500\n",
      "120/120 [==============================] - 0s 116us/step - loss: 0.0571 - accuracy: 0.9750 - val_loss: 0.0374 - val_accuracy: 1.0000\n",
      "Epoch 1958/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.0578 - accuracy: 0.9833 - val_loss: 0.0501 - val_accuracy: 0.9667\n",
      "Epoch 1959/2500\n",
      "120/120 [==============================] - 0s 109us/step - loss: 0.0551 - accuracy: 0.9833 - val_loss: 0.0465 - val_accuracy: 0.9667\n",
      "Epoch 1960/2500\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.0557 - accuracy: 0.9833 - val_loss: 0.0431 - val_accuracy: 0.9667\n",
      "Epoch 1961/2500\n",
      "120/120 [==============================] - 0s 103us/step - loss: 0.0552 - accuracy: 0.9833 - val_loss: 0.0454 - val_accuracy: 0.9667\n",
      "Epoch 1962/2500\n",
      "120/120 [==============================] - 0s 93us/step - loss: 0.0580 - accuracy: 0.9750 - val_loss: 0.0362 - val_accuracy: 1.0000\n",
      "Epoch 1963/2500\n",
      "120/120 [==============================] - 0s 102us/step - loss: 0.0550 - accuracy: 0.9750 - val_loss: 0.0409 - val_accuracy: 0.9667\n",
      "Epoch 1964/2500\n",
      "120/120 [==============================] - 0s 109us/step - loss: 0.0553 - accuracy: 0.9750 - val_loss: 0.0407 - val_accuracy: 0.9667\n",
      "Epoch 1965/2500\n",
      "120/120 [==============================] - 0s 130us/step - loss: 0.0555 - accuracy: 0.9833 - val_loss: 0.0500 - val_accuracy: 0.9667\n",
      "Epoch 1966/2500\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.0556 - accuracy: 0.9833 - val_loss: 0.0496 - val_accuracy: 0.9667\n",
      "Epoch 1967/2500\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0559 - accuracy: 0.9833 - val_loss: 0.0424 - val_accuracy: 0.9667\n",
      "Epoch 1968/2500\n",
      "120/120 [==============================] - 0s 133us/step - loss: 0.0555 - accuracy: 0.9750 - val_loss: 0.0420 - val_accuracy: 0.9667\n",
      "Epoch 1969/2500\n",
      "120/120 [==============================] - 0s 113us/step - loss: 0.0560 - accuracy: 0.9833 - val_loss: 0.0484 - val_accuracy: 0.9667\n",
      "Epoch 1970/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.0548 - accuracy: 0.9833 - val_loss: 0.0440 - val_accuracy: 0.9667\n",
      "Epoch 1971/2500\n",
      "120/120 [==============================] - 0s 140us/step - loss: 0.0549 - accuracy: 0.9833 - val_loss: 0.0404 - val_accuracy: 0.9667\n",
      "Epoch 1972/2500\n",
      "120/120 [==============================] - 0s 124us/step - loss: 0.0569 - accuracy: 0.9833 - val_loss: 0.0458 - val_accuracy: 0.9667\n",
      "Epoch 1973/2500\n",
      "120/120 [==============================] - 0s 119us/step - loss: 0.0553 - accuracy: 0.9833 - val_loss: 0.0399 - val_accuracy: 0.9667\n",
      "Epoch 1974/2500\n",
      "120/120 [==============================] - 0s 157us/step - loss: 0.0554 - accuracy: 0.9750 - val_loss: 0.0396 - val_accuracy: 0.9667\n",
      "Epoch 1975/2500\n",
      "120/120 [==============================] - 0s 113us/step - loss: 0.0549 - accuracy: 0.9833 - val_loss: 0.0445 - val_accuracy: 0.9667\n",
      "Epoch 1976/2500\n",
      "120/120 [==============================] - 0s 120us/step - loss: 0.0557 - accuracy: 0.9833 - val_loss: 0.0495 - val_accuracy: 0.9667\n",
      "Epoch 1977/2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 133us/step - loss: 0.0573 - accuracy: 0.9833 - val_loss: 0.0407 - val_accuracy: 0.9667\n",
      "Epoch 1978/2500\n",
      "120/120 [==============================] - 0s 133us/step - loss: 0.0550 - accuracy: 0.9750 - val_loss: 0.0417 - val_accuracy: 0.9667\n",
      "Epoch 1979/2500\n",
      "120/120 [==============================] - 0s 128us/step - loss: 0.0555 - accuracy: 0.9750 - val_loss: 0.0405 - val_accuracy: 0.9667\n",
      "Epoch 1980/2500\n",
      "120/120 [==============================] - 0s 131us/step - loss: 0.0567 - accuracy: 0.9833 - val_loss: 0.0498 - val_accuracy: 0.9667\n",
      "Epoch 1981/2500\n",
      "120/120 [==============================] - 0s 224us/step - loss: 0.0548 - accuracy: 0.9833 - val_loss: 0.0453 - val_accuracy: 0.9667\n",
      "Epoch 1982/2500\n",
      "120/120 [==============================] - 0s 145us/step - loss: 0.0548 - accuracy: 0.9833 - val_loss: 0.0429 - val_accuracy: 0.9667\n",
      "Epoch 1983/2500\n",
      "120/120 [==============================] - 0s 124us/step - loss: 0.0557 - accuracy: 0.9833 - val_loss: 0.0454 - val_accuracy: 0.9667\n",
      "Epoch 1984/2500\n",
      "120/120 [==============================] - 0s 121us/step - loss: 0.0565 - accuracy: 0.9833 - val_loss: 0.0460 - val_accuracy: 0.9667\n",
      "Epoch 1985/2500\n",
      "120/120 [==============================] - 0s 119us/step - loss: 0.0561 - accuracy: 0.9833 - val_loss: 0.0376 - val_accuracy: 1.0000\n",
      "Epoch 1986/2500\n",
      "120/120 [==============================] - 0s 104us/step - loss: 0.0557 - accuracy: 0.9750 - val_loss: 0.0420 - val_accuracy: 0.9667\n",
      "Epoch 1987/2500\n",
      "120/120 [==============================] - 0s 113us/step - loss: 0.0548 - accuracy: 0.9833 - val_loss: 0.0435 - val_accuracy: 0.9667\n",
      "Epoch 1988/2500\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.0571 - accuracy: 0.9750 - val_loss: 0.0377 - val_accuracy: 1.0000\n",
      "Epoch 1989/2500\n",
      "120/120 [==============================] - 0s 110us/step - loss: 0.0550 - accuracy: 0.9750 - val_loss: 0.0404 - val_accuracy: 0.9667\n",
      "Epoch 1990/2500\n",
      "120/120 [==============================] - 0s 97us/step - loss: 0.0546 - accuracy: 0.9750 - val_loss: 0.0434 - val_accuracy: 0.9667\n",
      "Epoch 1991/2500\n",
      "120/120 [==============================] - 0s 112us/step - loss: 0.0566 - accuracy: 0.9833 - val_loss: 0.0532 - val_accuracy: 0.9667\n",
      "Epoch 1992/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.0557 - accuracy: 0.9833 - val_loss: 0.0458 - val_accuracy: 0.9667\n",
      "Epoch 1993/2500\n",
      "120/120 [==============================] - 0s 99us/step - loss: 0.0549 - accuracy: 0.9833 - val_loss: 0.0438 - val_accuracy: 0.9667\n",
      "Epoch 1994/2500\n",
      "120/120 [==============================] - 0s 115us/step - loss: 0.0545 - accuracy: 0.9833 - val_loss: 0.0431 - val_accuracy: 0.9667\n",
      "Epoch 1995/2500\n",
      "120/120 [==============================] - 0s 131us/step - loss: 0.0559 - accuracy: 0.9833 - val_loss: 0.0458 - val_accuracy: 0.9667\n",
      "Epoch 1996/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.0576 - accuracy: 0.9750 - val_loss: 0.0395 - val_accuracy: 0.9667\n",
      "Epoch 1997/2500\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.0545 - accuracy: 0.9750 - val_loss: 0.0408 - val_accuracy: 0.9667\n",
      "Epoch 1998/2500\n",
      "120/120 [==============================] - 0s 123us/step - loss: 0.0552 - accuracy: 0.9833 - val_loss: 0.0462 - val_accuracy: 0.9667\n",
      "Epoch 1999/2500\n",
      "120/120 [==============================] - 0s 130us/step - loss: 0.0565 - accuracy: 0.9833 - val_loss: 0.0407 - val_accuracy: 0.9667\n",
      "Epoch 2000/2500\n",
      "120/120 [==============================] - 0s 136us/step - loss: 0.0560 - accuracy: 0.9750 - val_loss: 0.0470 - val_accuracy: 0.9667\n",
      "Epoch 2001/2500\n",
      "120/120 [==============================] - 0s 139us/step - loss: 0.0561 - accuracy: 0.9833 - val_loss: 0.0489 - val_accuracy: 0.9667\n",
      "Epoch 2002/2500\n",
      "120/120 [==============================] - 0s 116us/step - loss: 0.0543 - accuracy: 0.9833 - val_loss: 0.0435 - val_accuracy: 0.9667\n",
      "Epoch 2003/2500\n",
      "120/120 [==============================] - 0s 114us/step - loss: 0.0565 - accuracy: 0.9750 - val_loss: 0.0374 - val_accuracy: 1.0000\n",
      "Epoch 2004/2500\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.0572 - accuracy: 0.9750 - val_loss: 0.0353 - val_accuracy: 1.0000\n",
      "Epoch 2005/2500\n",
      "120/120 [==============================] - 0s 109us/step - loss: 0.0568 - accuracy: 0.9750 - val_loss: 0.0462 - val_accuracy: 0.9667\n",
      "Epoch 2006/2500\n",
      "120/120 [==============================] - 0s 98us/step - loss: 0.0549 - accuracy: 0.9833 - val_loss: 0.0474 - val_accuracy: 0.9667\n",
      "Epoch 2007/2500\n",
      "120/120 [==============================] - 0s 110us/step - loss: 0.0547 - accuracy: 0.9833 - val_loss: 0.0448 - val_accuracy: 0.9667\n",
      "Epoch 2008/2500\n",
      "120/120 [==============================] - 0s 99us/step - loss: 0.0558 - accuracy: 0.9833 - val_loss: 0.0399 - val_accuracy: 0.9667\n",
      "Epoch 2009/2500\n",
      "120/120 [==============================] - 0s 112us/step - loss: 0.0556 - accuracy: 0.9750 - val_loss: 0.0455 - val_accuracy: 0.9667\n",
      "Epoch 2010/2500\n",
      "120/120 [==============================] - 0s 119us/step - loss: 0.0561 - accuracy: 0.9833 - val_loss: 0.0407 - val_accuracy: 0.9667\n",
      "Epoch 2011/2500\n",
      "120/120 [==============================] - 0s 116us/step - loss: 0.0556 - accuracy: 0.9833 - val_loss: 0.0461 - val_accuracy: 0.9667\n",
      "Epoch 2012/2500\n",
      "120/120 [==============================] - 0s 105us/step - loss: 0.0559 - accuracy: 0.9833 - val_loss: 0.0500 - val_accuracy: 0.9667\n",
      "Epoch 2013/2500\n",
      "120/120 [==============================] - 0s 105us/step - loss: 0.0546 - accuracy: 0.9833 - val_loss: 0.0448 - val_accuracy: 0.9667\n",
      "Epoch 2014/2500\n",
      "120/120 [==============================] - 0s 120us/step - loss: 0.0547 - accuracy: 0.9833 - val_loss: 0.0398 - val_accuracy: 0.9667\n",
      "Epoch 2015/2500\n",
      "120/120 [==============================] - 0s 101us/step - loss: 0.0575 - accuracy: 0.9750 - val_loss: 0.0343 - val_accuracy: 1.0000\n",
      "Epoch 2016/2500\n",
      "120/120 [==============================] - 0s 100us/step - loss: 0.0580 - accuracy: 0.9750 - val_loss: 0.0454 - val_accuracy: 0.9667\n",
      "Epoch 2017/2500\n",
      "120/120 [==============================] - 0s 114us/step - loss: 0.0546 - accuracy: 0.9833 - val_loss: 0.0443 - val_accuracy: 0.9667\n",
      "Epoch 2018/2500\n",
      "120/120 [==============================] - 0s 114us/step - loss: 0.0549 - accuracy: 0.9833 - val_loss: 0.0438 - val_accuracy: 0.9667\n",
      "Epoch 2019/2500\n",
      "120/120 [==============================] - 0s 120us/step - loss: 0.0557 - accuracy: 0.9833 - val_loss: 0.0420 - val_accuracy: 0.9667\n",
      "Epoch 2020/2500\n",
      "120/120 [==============================] - 0s 144us/step - loss: 0.0549 - accuracy: 0.9833 - val_loss: 0.0407 - val_accuracy: 0.9667\n",
      "Epoch 2021/2500\n",
      "120/120 [==============================] - 0s 180us/step - loss: 0.0549 - accuracy: 0.9750 - val_loss: 0.0396 - val_accuracy: 0.9667\n",
      "Epoch 2022/2500\n",
      "120/120 [==============================] - 0s 165us/step - loss: 0.0548 - accuracy: 0.9833 - val_loss: 0.0439 - val_accuracy: 0.9667\n",
      "Epoch 2023/2500\n",
      "120/120 [==============================] - 0s 129us/step - loss: 0.0556 - accuracy: 0.9833 - val_loss: 0.0474 - val_accuracy: 0.9667\n",
      "Epoch 2024/2500\n",
      "120/120 [==============================] - 0s 180us/step - loss: 0.0567 - accuracy: 0.9750 - val_loss: 0.0407 - val_accuracy: 0.9667\n",
      "Epoch 2025/2500\n",
      "120/120 [==============================] - 0s 136us/step - loss: 0.0543 - accuracy: 0.9833 - val_loss: 0.0416 - val_accuracy: 0.9667\n",
      "Epoch 2026/2500\n",
      "120/120 [==============================] - 0s 140us/step - loss: 0.0549 - accuracy: 0.9750 - val_loss: 0.0401 - val_accuracy: 0.9667\n",
      "Epoch 2027/2500\n",
      "120/120 [==============================] - 0s 121us/step - loss: 0.0548 - accuracy: 0.9750 - val_loss: 0.0406 - val_accuracy: 0.9667\n",
      "Epoch 2028/2500\n",
      "120/120 [==============================] - 0s 155us/step - loss: 0.0546 - accuracy: 0.9833 - val_loss: 0.0452 - val_accuracy: 0.9667\n",
      "Epoch 2029/2500\n",
      "120/120 [==============================] - 0s 157us/step - loss: 0.0556 - accuracy: 0.9833 - val_loss: 0.0434 - val_accuracy: 0.9667\n",
      "Epoch 2030/2500\n",
      "120/120 [==============================] - 0s 167us/step - loss: 0.0546 - accuracy: 0.9833 - val_loss: 0.0448 - val_accuracy: 0.9667\n",
      "Epoch 2031/2500\n",
      "120/120 [==============================] - 0s 153us/step - loss: 0.0543 - accuracy: 0.9833 - val_loss: 0.0452 - val_accuracy: 0.9667\n",
      "Epoch 2032/2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 145us/step - loss: 0.0591 - accuracy: 0.9750 - val_loss: 0.0375 - val_accuracy: 1.0000\n",
      "Epoch 2033/2500\n",
      "120/120 [==============================] - 0s 188us/step - loss: 0.0540 - accuracy: 0.9750 - val_loss: 0.0432 - val_accuracy: 0.9667\n",
      "Epoch 2034/2500\n",
      "120/120 [==============================] - 0s 165us/step - loss: 0.0544 - accuracy: 0.9833 - val_loss: 0.0439 - val_accuracy: 0.9667\n",
      "Epoch 2035/2500\n",
      "120/120 [==============================] - 0s 222us/step - loss: 0.0562 - accuracy: 0.9833 - val_loss: 0.0426 - val_accuracy: 0.9667\n",
      "Epoch 2036/2500\n",
      "120/120 [==============================] - 0s 211us/step - loss: 0.0562 - accuracy: 0.9833 - val_loss: 0.0508 - val_accuracy: 0.9667\n",
      "Epoch 2037/2500\n",
      "120/120 [==============================] - 0s 167us/step - loss: 0.0555 - accuracy: 0.9833 - val_loss: 0.0451 - val_accuracy: 0.9667\n",
      "Epoch 2038/2500\n",
      "120/120 [==============================] - 0s 150us/step - loss: 0.0563 - accuracy: 0.9833 - val_loss: 0.0472 - val_accuracy: 0.9667\n",
      "Epoch 2039/2500\n",
      "120/120 [==============================] - 0s 165us/step - loss: 0.0544 - accuracy: 0.9833 - val_loss: 0.0402 - val_accuracy: 0.9667\n",
      "Epoch 2040/2500\n",
      "120/120 [==============================] - 0s 152us/step - loss: 0.0567 - accuracy: 0.9750 - val_loss: 0.0357 - val_accuracy: 1.0000\n",
      "Epoch 2041/2500\n",
      "120/120 [==============================] - 0s 169us/step - loss: 0.0574 - accuracy: 0.9750 - val_loss: 0.0451 - val_accuracy: 0.9667\n",
      "Epoch 2042/2500\n",
      "120/120 [==============================] - 0s 149us/step - loss: 0.0546 - accuracy: 0.9833 - val_loss: 0.0446 - val_accuracy: 0.9667\n",
      "Epoch 2043/2500\n",
      "120/120 [==============================] - 0s 161us/step - loss: 0.0545 - accuracy: 0.9833 - val_loss: 0.0431 - val_accuracy: 0.9667\n",
      "Epoch 2044/2500\n",
      "120/120 [==============================] - 0s 155us/step - loss: 0.0575 - accuracy: 0.9833 - val_loss: 0.0500 - val_accuracy: 0.9667\n",
      "Epoch 2045/2500\n",
      "120/120 [==============================] - 0s 119us/step - loss: 0.0571 - accuracy: 0.9833 - val_loss: 0.0371 - val_accuracy: 1.0000\n",
      "Epoch 2046/2500\n",
      "120/120 [==============================] - 0s 119us/step - loss: 0.0595 - accuracy: 0.9750 - val_loss: 0.0475 - val_accuracy: 0.9667\n",
      "Epoch 2047/2500\n",
      "120/120 [==============================] - 0s 114us/step - loss: 0.0546 - accuracy: 0.9833 - val_loss: 0.0412 - val_accuracy: 0.9667\n",
      "Epoch 2048/2500\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0545 - accuracy: 0.9833 - val_loss: 0.0407 - val_accuracy: 0.9667\n",
      "Epoch 2049/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.0560 - accuracy: 0.9833 - val_loss: 0.0432 - val_accuracy: 0.9667\n",
      "Epoch 2050/2500\n",
      "120/120 [==============================] - 0s 120us/step - loss: 0.0548 - accuracy: 0.9833 - val_loss: 0.0403 - val_accuracy: 0.9667\n",
      "Epoch 2051/2500\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0550 - accuracy: 0.9750 - val_loss: 0.0400 - val_accuracy: 0.9667\n",
      "Epoch 2052/2500\n",
      "120/120 [==============================] - 0s 95us/step - loss: 0.0549 - accuracy: 0.9750 - val_loss: 0.0409 - val_accuracy: 0.9667\n",
      "Epoch 2053/2500\n",
      "120/120 [==============================] - 0s 115us/step - loss: 0.0555 - accuracy: 0.9750 - val_loss: 0.0383 - val_accuracy: 0.9667\n",
      "Epoch 2054/2500\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.0545 - accuracy: 0.9750 - val_loss: 0.0430 - val_accuracy: 0.9667\n",
      "Epoch 2055/2500\n",
      "120/120 [==============================] - 0s 104us/step - loss: 0.0555 - accuracy: 0.9833 - val_loss: 0.0493 - val_accuracy: 0.9667\n",
      "Epoch 2056/2500\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0548 - accuracy: 0.9833 - val_loss: 0.0450 - val_accuracy: 0.9667\n",
      "Epoch 2057/2500\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0557 - accuracy: 0.9833 - val_loss: 0.0373 - val_accuracy: 1.0000\n",
      "Epoch 2058/2500\n",
      "120/120 [==============================] - 0s 104us/step - loss: 0.0545 - accuracy: 0.9750 - val_loss: 0.0399 - val_accuracy: 0.9667\n",
      "Epoch 2059/2500\n",
      "120/120 [==============================] - 0s 102us/step - loss: 0.0543 - accuracy: 0.9833 - val_loss: 0.0441 - val_accuracy: 0.9667\n",
      "Epoch 2060/2500\n",
      "120/120 [==============================] - 0s 140us/step - loss: 0.0544 - accuracy: 0.9833 - val_loss: 0.0454 - val_accuracy: 0.9667\n",
      "Epoch 2061/2500\n",
      "120/120 [==============================] - 0s 118us/step - loss: 0.0564 - accuracy: 0.9750 - val_loss: 0.0393 - val_accuracy: 0.9667\n",
      "Epoch 2062/2500\n",
      "120/120 [==============================] - 0s 99us/step - loss: 0.0581 - accuracy: 0.9833 - val_loss: 0.0491 - val_accuracy: 0.9667\n",
      "Epoch 2063/2500\n",
      "120/120 [==============================] - 0s 112us/step - loss: 0.0567 - accuracy: 0.9750 - val_loss: 0.0391 - val_accuracy: 0.9667\n",
      "Epoch 2064/2500\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.0542 - accuracy: 0.9750 - val_loss: 0.0403 - val_accuracy: 0.9667\n",
      "Epoch 2065/2500\n",
      "120/120 [==============================] - 0s 107us/step - loss: 0.0547 - accuracy: 0.9833 - val_loss: 0.0453 - val_accuracy: 0.9667\n",
      "Epoch 2066/2500\n",
      "120/120 [==============================] - 0s 119us/step - loss: 0.0550 - accuracy: 0.9750 - val_loss: 0.0405 - val_accuracy: 0.9667\n",
      "Epoch 2067/2500\n",
      "120/120 [==============================] - 0s 110us/step - loss: 0.0550 - accuracy: 0.9833 - val_loss: 0.0434 - val_accuracy: 0.9667\n",
      "Epoch 2068/2500\n",
      "120/120 [==============================] - 0s 115us/step - loss: 0.0542 - accuracy: 0.9833 - val_loss: 0.0424 - val_accuracy: 0.9667\n",
      "Epoch 2069/2500\n",
      "120/120 [==============================] - 0s 109us/step - loss: 0.0564 - accuracy: 0.9833 - val_loss: 0.0469 - val_accuracy: 0.9667\n",
      "Epoch 2070/2500\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0543 - accuracy: 0.9833 - val_loss: 0.0421 - val_accuracy: 0.9667\n",
      "Epoch 2071/2500\n",
      "120/120 [==============================] - 0s 90us/step - loss: 0.0587 - accuracy: 0.9750 - val_loss: 0.0343 - val_accuracy: 1.0000\n",
      "Epoch 2072/2500\n",
      "120/120 [==============================] - 0s 102us/step - loss: 0.0543 - accuracy: 0.9750 - val_loss: 0.0391 - val_accuracy: 0.9667\n",
      "Epoch 2073/2500\n",
      "120/120 [==============================] - 0s 115us/step - loss: 0.0541 - accuracy: 0.9833 - val_loss: 0.0464 - val_accuracy: 0.9667\n",
      "Epoch 2074/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.0569 - accuracy: 0.9833 - val_loss: 0.0514 - val_accuracy: 0.9667\n",
      "Epoch 2075/2500\n",
      "120/120 [==============================] - 0s 113us/step - loss: 0.0602 - accuracy: 0.9750 - val_loss: 0.0379 - val_accuracy: 1.0000\n",
      "Epoch 2076/2500\n",
      "120/120 [==============================] - 0s 104us/step - loss: 0.0546 - accuracy: 0.9833 - val_loss: 0.0428 - val_accuracy: 0.9667\n",
      "Epoch 2077/2500\n",
      "120/120 [==============================] - 0s 119us/step - loss: 0.0543 - accuracy: 0.9833 - val_loss: 0.0431 - val_accuracy: 0.9667\n",
      "Epoch 2078/2500\n",
      "120/120 [==============================] - 0s 91us/step - loss: 0.0558 - accuracy: 0.9750 - val_loss: 0.0399 - val_accuracy: 0.9667\n",
      "Epoch 2079/2500\n",
      "120/120 [==============================] - 0s 107us/step - loss: 0.0549 - accuracy: 0.9833 - val_loss: 0.0415 - val_accuracy: 0.9667\n",
      "Epoch 2080/2500\n",
      "120/120 [==============================] - 0s 126us/step - loss: 0.0548 - accuracy: 0.9833 - val_loss: 0.0454 - val_accuracy: 0.9667\n",
      "Epoch 2081/2500\n",
      "120/120 [==============================] - 0s 124us/step - loss: 0.0541 - accuracy: 0.9833 - val_loss: 0.0448 - val_accuracy: 0.9667\n",
      "Epoch 2082/2500\n",
      "120/120 [==============================] - 0s 114us/step - loss: 0.0559 - accuracy: 0.9833 - val_loss: 0.0476 - val_accuracy: 0.9667\n",
      "Epoch 2083/2500\n",
      "120/120 [==============================] - 0s 116us/step - loss: 0.0549 - accuracy: 0.9833 - val_loss: 0.0460 - val_accuracy: 0.9667\n",
      "Epoch 2084/2500\n",
      "120/120 [==============================] - 0s 126us/step - loss: 0.0538 - accuracy: 0.9833 - val_loss: 0.0390 - val_accuracy: 0.9667\n",
      "Epoch 2085/2500\n",
      "120/120 [==============================] - 0s 104us/step - loss: 0.0574 - accuracy: 0.9750 - val_loss: 0.0329 - val_accuracy: 1.0000\n",
      "Epoch 2086/2500\n",
      "120/120 [==============================] - 0s 100us/step - loss: 0.0551 - accuracy: 0.9750 - val_loss: 0.0414 - val_accuracy: 0.9667\n",
      "Epoch 2087/2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 125us/step - loss: 0.0550 - accuracy: 0.9833 - val_loss: 0.0410 - val_accuracy: 0.9667\n",
      "Epoch 2088/2500\n",
      "120/120 [==============================] - 0s 99us/step - loss: 0.0556 - accuracy: 0.9750 - val_loss: 0.0423 - val_accuracy: 0.9667\n",
      "Epoch 2089/2500\n",
      "120/120 [==============================] - 0s 112us/step - loss: 0.0546 - accuracy: 0.9833 - val_loss: 0.0484 - val_accuracy: 0.9667\n",
      "Epoch 2090/2500\n",
      "120/120 [==============================] - 0s 113us/step - loss: 0.0557 - accuracy: 0.9833 - val_loss: 0.0489 - val_accuracy: 0.9667\n",
      "Epoch 2091/2500\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.0559 - accuracy: 0.9833 - val_loss: 0.0488 - val_accuracy: 0.9667\n",
      "Epoch 2092/2500\n",
      "120/120 [==============================] - 0s 104us/step - loss: 0.0537 - accuracy: 0.9833 - val_loss: 0.0424 - val_accuracy: 0.9667\n",
      "Epoch 2093/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.0546 - accuracy: 0.9833 - val_loss: 0.0414 - val_accuracy: 0.9667\n",
      "Epoch 2094/2500\n",
      "120/120 [==============================] - 0s 133us/step - loss: 0.0549 - accuracy: 0.9750 - val_loss: 0.0353 - val_accuracy: 1.0000\n",
      "Epoch 2095/2500\n",
      "120/120 [==============================] - 0s 99us/step - loss: 0.0561 - accuracy: 0.9750 - val_loss: 0.0402 - val_accuracy: 0.9667\n",
      "Epoch 2096/2500\n",
      "120/120 [==============================] - 0s 100us/step - loss: 0.0546 - accuracy: 0.9750 - val_loss: 0.0397 - val_accuracy: 0.9667\n",
      "Epoch 2097/2500\n",
      "120/120 [==============================] - 0s 141us/step - loss: 0.0547 - accuracy: 0.9833 - val_loss: 0.0432 - val_accuracy: 0.9667\n",
      "Epoch 2098/2500\n",
      "120/120 [==============================] - 0s 155us/step - loss: 0.0553 - accuracy: 0.9833 - val_loss: 0.0455 - val_accuracy: 0.9667\n",
      "Epoch 2099/2500\n",
      "120/120 [==============================] - 0s 189us/step - loss: 0.0549 - accuracy: 0.9833 - val_loss: 0.0416 - val_accuracy: 0.9667\n",
      "Epoch 2100/2500\n",
      "120/120 [==============================] - 0s 129us/step - loss: 0.0553 - accuracy: 0.9833 - val_loss: 0.0431 - val_accuracy: 0.9667\n",
      "Epoch 2101/2500\n",
      "120/120 [==============================] - 0s 170us/step - loss: 0.0546 - accuracy: 0.9833 - val_loss: 0.0393 - val_accuracy: 0.9667\n",
      "Epoch 2102/2500\n",
      "120/120 [==============================] - 0s 144us/step - loss: 0.0544 - accuracy: 0.9750 - val_loss: 0.0395 - val_accuracy: 0.9667\n",
      "Epoch 2103/2500\n",
      "120/120 [==============================] - 0s 132us/step - loss: 0.0551 - accuracy: 0.9833 - val_loss: 0.0454 - val_accuracy: 0.9667\n",
      "Epoch 2104/2500\n",
      "120/120 [==============================] - 0s 132us/step - loss: 0.0556 - accuracy: 0.9750 - val_loss: 0.0397 - val_accuracy: 0.9667\n",
      "Epoch 2105/2500\n",
      "120/120 [==============================] - 0s 151us/step - loss: 0.0553 - accuracy: 0.9750 - val_loss: 0.0387 - val_accuracy: 0.9667\n",
      "Epoch 2106/2500\n",
      "120/120 [==============================] - 0s 130us/step - loss: 0.0562 - accuracy: 0.9750 - val_loss: 0.0488 - val_accuracy: 0.9667\n",
      "Epoch 2107/2500\n",
      "120/120 [==============================] - 0s 136us/step - loss: 0.0548 - accuracy: 0.9833 - val_loss: 0.0481 - val_accuracy: 0.9667\n",
      "Epoch 2108/2500\n",
      "120/120 [==============================] - 0s 130us/step - loss: 0.0541 - accuracy: 0.9833 - val_loss: 0.0404 - val_accuracy: 0.9667\n",
      "Epoch 2109/2500\n",
      "120/120 [==============================] - 0s 122us/step - loss: 0.0548 - accuracy: 0.9833 - val_loss: 0.0407 - val_accuracy: 0.9667\n",
      "Epoch 2110/2500\n",
      "120/120 [==============================] - 0s 107us/step - loss: 0.0548 - accuracy: 0.9750 - val_loss: 0.0365 - val_accuracy: 1.0000\n",
      "Epoch 2111/2500\n",
      "120/120 [==============================] - 0s 110us/step - loss: 0.0548 - accuracy: 0.9833 - val_loss: 0.0409 - val_accuracy: 0.9667\n",
      "Epoch 2112/2500\n",
      "120/120 [==============================] - 0s 112us/step - loss: 0.0557 - accuracy: 0.9750 - val_loss: 0.0373 - val_accuracy: 1.0000\n",
      "Epoch 2113/2500\n",
      "120/120 [==============================] - 0s 115us/step - loss: 0.0543 - accuracy: 0.9750 - val_loss: 0.0446 - val_accuracy: 0.9667\n",
      "Epoch 2114/2500\n",
      "120/120 [==============================] - 0s 126us/step - loss: 0.0547 - accuracy: 0.9833 - val_loss: 0.0477 - val_accuracy: 0.9667\n",
      "Epoch 2115/2500\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0551 - accuracy: 0.9833 - val_loss: 0.0443 - val_accuracy: 0.9667\n",
      "Epoch 2116/2500\n",
      "120/120 [==============================] - 0s 124us/step - loss: 0.0546 - accuracy: 0.9833 - val_loss: 0.0391 - val_accuracy: 0.9667\n",
      "Epoch 2117/2500\n",
      "120/120 [==============================] - 0s 119us/step - loss: 0.0547 - accuracy: 0.9750 - val_loss: 0.0416 - val_accuracy: 0.9667\n",
      "Epoch 2118/2500\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0557 - accuracy: 0.9750 - val_loss: 0.0371 - val_accuracy: 1.0000\n",
      "Epoch 2119/2500\n",
      "120/120 [==============================] - 0s 118us/step - loss: 0.0551 - accuracy: 0.9750 - val_loss: 0.0379 - val_accuracy: 1.0000\n",
      "Epoch 2120/2500\n",
      "120/120 [==============================] - 0s 120us/step - loss: 0.0543 - accuracy: 0.9750 - val_loss: 0.0414 - val_accuracy: 0.9667\n",
      "Epoch 2121/2500\n",
      "120/120 [==============================] - 0s 113us/step - loss: 0.0561 - accuracy: 0.9833 - val_loss: 0.0526 - val_accuracy: 0.9667\n",
      "Epoch 2122/2500\n",
      "120/120 [==============================] - 0s 112us/step - loss: 0.0553 - accuracy: 0.9833 - val_loss: 0.0497 - val_accuracy: 0.9667\n",
      "Epoch 2123/2500\n",
      "120/120 [==============================] - 0s 119us/step - loss: 0.0540 - accuracy: 0.9833 - val_loss: 0.0434 - val_accuracy: 0.9667\n",
      "Epoch 2124/2500\n",
      "120/120 [==============================] - 0s 118us/step - loss: 0.0544 - accuracy: 0.9833 - val_loss: 0.0392 - val_accuracy: 0.9667\n",
      "Epoch 2125/2500\n",
      "120/120 [==============================] - 0s 113us/step - loss: 0.0548 - accuracy: 0.9750 - val_loss: 0.0357 - val_accuracy: 1.0000\n",
      "Epoch 2126/2500\n",
      "120/120 [==============================] - 0s 132us/step - loss: 0.0562 - accuracy: 0.9750 - val_loss: 0.0338 - val_accuracy: 1.0000\n",
      "Epoch 2127/2500\n",
      "120/120 [==============================] - 0s 126us/step - loss: 0.0544 - accuracy: 0.9750 - val_loss: 0.0417 - val_accuracy: 0.9667\n",
      "Epoch 2128/2500\n",
      "120/120 [==============================] - 0s 109us/step - loss: 0.0556 - accuracy: 0.9833 - val_loss: 0.0508 - val_accuracy: 0.9667\n",
      "Epoch 2129/2500\n",
      "120/120 [==============================] - 0s 112us/step - loss: 0.0556 - accuracy: 0.9833 - val_loss: 0.0448 - val_accuracy: 0.9667\n",
      "Epoch 2130/2500\n",
      "120/120 [==============================] - 0s 118us/step - loss: 0.0549 - accuracy: 0.9750 - val_loss: 0.0399 - val_accuracy: 0.9667\n",
      "Epoch 2131/2500\n",
      "120/120 [==============================] - 0s 114us/step - loss: 0.0544 - accuracy: 0.9750 - val_loss: 0.0418 - val_accuracy: 0.9667\n",
      "Epoch 2132/2500\n",
      "120/120 [==============================] - 0s 115us/step - loss: 0.0552 - accuracy: 0.9833 - val_loss: 0.0467 - val_accuracy: 0.9667\n",
      "Epoch 2133/2500\n",
      "120/120 [==============================] - 0s 124us/step - loss: 0.0546 - accuracy: 0.9833 - val_loss: 0.0453 - val_accuracy: 0.9667\n",
      "Epoch 2134/2500\n",
      "120/120 [==============================] - 0s 114us/step - loss: 0.0555 - accuracy: 0.9833 - val_loss: 0.0386 - val_accuracy: 0.9667\n",
      "Epoch 2135/2500\n",
      "120/120 [==============================] - 0s 110us/step - loss: 0.0557 - accuracy: 0.9750 - val_loss: 0.0364 - val_accuracy: 1.0000\n",
      "Epoch 2136/2500\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0563 - accuracy: 0.9833 - val_loss: 0.0472 - val_accuracy: 0.9667\n",
      "Epoch 2137/2500\n",
      "120/120 [==============================] - 0s 110us/step - loss: 0.0540 - accuracy: 0.9833 - val_loss: 0.0441 - val_accuracy: 0.9667\n",
      "Epoch 2138/2500\n",
      "120/120 [==============================] - 0s 114us/step - loss: 0.0547 - accuracy: 0.9833 - val_loss: 0.0399 - val_accuracy: 0.9667\n",
      "Epoch 2139/2500\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0543 - accuracy: 0.9833 - val_loss: 0.0433 - val_accuracy: 0.9667\n",
      "Epoch 2140/2500\n",
      "120/120 [==============================] - 0s 115us/step - loss: 0.0546 - accuracy: 0.9833 - val_loss: 0.0436 - val_accuracy: 0.9667\n",
      "Epoch 2141/2500\n",
      "120/120 [==============================] - 0s 114us/step - loss: 0.0545 - accuracy: 0.9833 - val_loss: 0.0397 - val_accuracy: 0.9667\n",
      "Epoch 2142/2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 135us/step - loss: 0.0540 - accuracy: 0.9833 - val_loss: 0.0411 - val_accuracy: 0.9667\n",
      "Epoch 2143/2500\n",
      "120/120 [==============================] - 0s 109us/step - loss: 0.0550 - accuracy: 0.9833 - val_loss: 0.0435 - val_accuracy: 0.9667\n",
      "Epoch 2144/2500\n",
      "120/120 [==============================] - 0s 113us/step - loss: 0.0553 - accuracy: 0.9833 - val_loss: 0.0379 - val_accuracy: 1.0000\n",
      "Epoch 2145/2500\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0544 - accuracy: 0.9750 - val_loss: 0.0383 - val_accuracy: 1.0000\n",
      "Epoch 2146/2500\n",
      "120/120 [==============================] - 0s 126us/step - loss: 0.0593 - accuracy: 0.9750 - val_loss: 0.0505 - val_accuracy: 0.9667\n",
      "Epoch 2147/2500\n",
      "120/120 [==============================] - 0s 120us/step - loss: 0.0556 - accuracy: 0.9833 - val_loss: 0.0491 - val_accuracy: 0.9667\n",
      "Epoch 2148/2500\n",
      "120/120 [==============================] - 0s 109us/step - loss: 0.0562 - accuracy: 0.9750 - val_loss: 0.0374 - val_accuracy: 1.0000\n",
      "Epoch 2149/2500\n",
      "120/120 [==============================] - 0s 110us/step - loss: 0.0547 - accuracy: 0.9750 - val_loss: 0.0348 - val_accuracy: 1.0000\n",
      "Epoch 2150/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.0568 - accuracy: 0.9750 - val_loss: 0.0432 - val_accuracy: 0.9667\n",
      "Epoch 2151/2500\n",
      "120/120 [==============================] - 0s 101us/step - loss: 0.0549 - accuracy: 0.9750 - val_loss: 0.0389 - val_accuracy: 0.9667\n",
      "Epoch 2152/2500\n",
      "120/120 [==============================] - 0s 112us/step - loss: 0.0554 - accuracy: 0.9750 - val_loss: 0.0391 - val_accuracy: 0.9667\n",
      "Epoch 2153/2500\n",
      "120/120 [==============================] - 0s 116us/step - loss: 0.0560 - accuracy: 0.9833 - val_loss: 0.0498 - val_accuracy: 0.9667\n",
      "Epoch 2154/2500\n",
      "120/120 [==============================] - 0s 116us/step - loss: 0.0544 - accuracy: 0.9833 - val_loss: 0.0478 - val_accuracy: 0.9667\n",
      "Epoch 2155/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.0546 - accuracy: 0.9833 - val_loss: 0.0405 - val_accuracy: 0.9667\n",
      "Epoch 2156/2500\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0567 - accuracy: 0.9833 - val_loss: 0.0445 - val_accuracy: 0.9667\n",
      "Epoch 2157/2500\n",
      "120/120 [==============================] - 0s 105us/step - loss: 0.0550 - accuracy: 0.9750 - val_loss: 0.0366 - val_accuracy: 1.0000\n",
      "Epoch 2158/2500\n",
      "120/120 [==============================] - 0s 96us/step - loss: 0.0560 - accuracy: 0.9750 - val_loss: 0.0414 - val_accuracy: 0.9667\n",
      "Epoch 2159/2500\n",
      "120/120 [==============================] - 0s 120us/step - loss: 0.0543 - accuracy: 0.9833 - val_loss: 0.0423 - val_accuracy: 0.9667\n",
      "Epoch 2160/2500\n",
      "120/120 [==============================] - 0s 118us/step - loss: 0.0554 - accuracy: 0.9833 - val_loss: 0.0449 - val_accuracy: 0.9667\n",
      "Epoch 2161/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.0555 - accuracy: 0.9833 - val_loss: 0.0384 - val_accuracy: 0.9667\n",
      "Epoch 2162/2500\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.0555 - accuracy: 0.9833 - val_loss: 0.0420 - val_accuracy: 0.9667\n",
      "Epoch 2163/2500\n",
      "120/120 [==============================] - 0s 109us/step - loss: 0.0544 - accuracy: 0.9833 - val_loss: 0.0423 - val_accuracy: 0.9667\n",
      "Epoch 2164/2500\n",
      "120/120 [==============================] - 0s 102us/step - loss: 0.0550 - accuracy: 0.9833 - val_loss: 0.0376 - val_accuracy: 1.0000\n",
      "Epoch 2165/2500\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0545 - accuracy: 0.9833 - val_loss: 0.0409 - val_accuracy: 0.9667\n",
      "Epoch 2166/2500\n",
      "120/120 [==============================] - 0s 107us/step - loss: 0.0582 - accuracy: 0.9833 - val_loss: 0.0482 - val_accuracy: 0.9667\n",
      "Epoch 2167/2500\n",
      "120/120 [==============================] - 0s 95us/step - loss: 0.0538 - accuracy: 0.9833 - val_loss: 0.0407 - val_accuracy: 0.9667\n",
      "Epoch 2168/2500\n",
      "120/120 [==============================] - 0s 101us/step - loss: 0.0587 - accuracy: 0.9750 - val_loss: 0.0321 - val_accuracy: 1.0000\n",
      "Epoch 2169/2500\n",
      "120/120 [==============================] - 0s 97us/step - loss: 0.0549 - accuracy: 0.9750 - val_loss: 0.0387 - val_accuracy: 0.9667\n",
      "Epoch 2170/2500\n",
      "120/120 [==============================] - 0s 114us/step - loss: 0.0539 - accuracy: 0.9750 - val_loss: 0.0432 - val_accuracy: 0.9667\n",
      "Epoch 2171/2500\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0541 - accuracy: 0.9833 - val_loss: 0.0460 - val_accuracy: 0.9667\n",
      "Epoch 2172/2500\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.0554 - accuracy: 0.9833 - val_loss: 0.0484 - val_accuracy: 0.9667\n",
      "Epoch 2173/2500\n",
      "120/120 [==============================] - 0s 110us/step - loss: 0.0560 - accuracy: 0.9750 - val_loss: 0.0382 - val_accuracy: 1.0000\n",
      "Epoch 2174/2500\n",
      "120/120 [==============================] - 0s 113us/step - loss: 0.0546 - accuracy: 0.9750 - val_loss: 0.0405 - val_accuracy: 0.9667\n",
      "Epoch 2175/2500\n",
      "120/120 [==============================] - 0s 96us/step - loss: 0.0544 - accuracy: 0.9833 - val_loss: 0.0395 - val_accuracy: 0.9667\n",
      "Epoch 2176/2500\n",
      "120/120 [==============================] - 0s 109us/step - loss: 0.0540 - accuracy: 0.9833 - val_loss: 0.0435 - val_accuracy: 0.9667\n",
      "Epoch 2177/2500\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.0542 - accuracy: 0.9833 - val_loss: 0.0441 - val_accuracy: 0.9667\n",
      "Epoch 2178/2500\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0563 - accuracy: 0.9833 - val_loss: 0.0491 - val_accuracy: 0.9667\n",
      "Epoch 2179/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.0557 - accuracy: 0.9833 - val_loss: 0.0393 - val_accuracy: 0.9667\n",
      "Epoch 2180/2500\n",
      "120/120 [==============================] - 0s 102us/step - loss: 0.0556 - accuracy: 0.9833 - val_loss: 0.0413 - val_accuracy: 0.9667\n",
      "Epoch 2181/2500\n",
      "120/120 [==============================] - 0s 101us/step - loss: 0.0549 - accuracy: 0.9750 - val_loss: 0.0360 - val_accuracy: 1.0000\n",
      "Epoch 2182/2500\n",
      "120/120 [==============================] - 0s 107us/step - loss: 0.0552 - accuracy: 0.9833 - val_loss: 0.0415 - val_accuracy: 0.9667\n",
      "Epoch 2183/2500\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.0560 - accuracy: 0.9833 - val_loss: 0.0454 - val_accuracy: 0.9667\n",
      "Epoch 2184/2500\n",
      "120/120 [==============================] - 0s 113us/step - loss: 0.0559 - accuracy: 0.9833 - val_loss: 0.0363 - val_accuracy: 1.0000\n",
      "Epoch 2185/2500\n",
      "120/120 [==============================] - 0s 104us/step - loss: 0.0543 - accuracy: 0.9750 - val_loss: 0.0379 - val_accuracy: 1.0000\n",
      "Epoch 2186/2500\n",
      "120/120 [==============================] - 0s 87us/step - loss: 0.0542 - accuracy: 0.9750 - val_loss: 0.0426 - val_accuracy: 0.9667\n",
      "Epoch 2187/2500\n",
      "120/120 [==============================] - 0s 97us/step - loss: 0.0544 - accuracy: 0.9833 - val_loss: 0.0402 - val_accuracy: 0.9667\n",
      "Epoch 2188/2500\n",
      "120/120 [==============================] - 0s 101us/step - loss: 0.0540 - accuracy: 0.9833 - val_loss: 0.0407 - val_accuracy: 0.9667\n",
      "Epoch 2189/2500\n",
      "120/120 [==============================] - 0s 94us/step - loss: 0.0546 - accuracy: 0.9833 - val_loss: 0.0410 - val_accuracy: 0.9667\n",
      "Epoch 2190/2500\n",
      "120/120 [==============================] - 0s 90us/step - loss: 0.0535 - accuracy: 0.9833 - val_loss: 0.0432 - val_accuracy: 0.9667\n",
      "Epoch 2191/2500\n",
      "120/120 [==============================] - 0s 101us/step - loss: 0.0541 - accuracy: 0.9833 - val_loss: 0.0432 - val_accuracy: 0.9667\n",
      "Epoch 2192/2500\n",
      "120/120 [==============================] - 0s 98us/step - loss: 0.0541 - accuracy: 0.9833 - val_loss: 0.0426 - val_accuracy: 0.9667\n",
      "Epoch 2193/2500\n",
      "120/120 [==============================] - 0s 98us/step - loss: 0.0546 - accuracy: 0.9833 - val_loss: 0.0396 - val_accuracy: 0.9667\n",
      "Epoch 2194/2500\n",
      "120/120 [==============================] - 0s 94us/step - loss: 0.0550 - accuracy: 0.9833 - val_loss: 0.0389 - val_accuracy: 0.9667\n",
      "Epoch 2195/2500\n",
      "120/120 [==============================] - 0s 93us/step - loss: 0.0565 - accuracy: 0.9750 - val_loss: 0.0472 - val_accuracy: 0.9667\n",
      "Epoch 2196/2500\n",
      "120/120 [==============================] - 0s 104us/step - loss: 0.0554 - accuracy: 0.9833 - val_loss: 0.0393 - val_accuracy: 0.9667\n",
      "Epoch 2197/2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 93us/step - loss: 0.0546 - accuracy: 0.9833 - val_loss: 0.0436 - val_accuracy: 0.9667\n",
      "Epoch 2198/2500\n",
      "120/120 [==============================] - 0s 94us/step - loss: 0.0543 - accuracy: 0.9833 - val_loss: 0.0407 - val_accuracy: 0.9667\n",
      "Epoch 2199/2500\n",
      "120/120 [==============================] - 0s 105us/step - loss: 0.0566 - accuracy: 0.9750 - val_loss: 0.0377 - val_accuracy: 1.0000\n",
      "Epoch 2200/2500\n",
      "120/120 [==============================] - 0s 86us/step - loss: 0.0546 - accuracy: 0.9750 - val_loss: 0.0388 - val_accuracy: 0.9667\n",
      "Epoch 2201/2500\n",
      "120/120 [==============================] - 0s 97us/step - loss: 0.0539 - accuracy: 0.9750 - val_loss: 0.0410 - val_accuracy: 0.9667\n",
      "Epoch 2202/2500\n",
      "120/120 [==============================] - 0s 95us/step - loss: 0.0591 - accuracy: 0.9833 - val_loss: 0.0542 - val_accuracy: 0.9667\n",
      "Epoch 2203/2500\n",
      "120/120 [==============================] - 0s 114us/step - loss: 0.0554 - accuracy: 0.9833 - val_loss: 0.0425 - val_accuracy: 0.9667\n",
      "Epoch 2204/2500\n",
      "120/120 [==============================] - 0s 87us/step - loss: 0.0539 - accuracy: 0.9833 - val_loss: 0.0401 - val_accuracy: 0.9667\n",
      "Epoch 2205/2500\n",
      "120/120 [==============================] - 0s 89us/step - loss: 0.0561 - accuracy: 0.9750 - val_loss: 0.0355 - val_accuracy: 1.0000\n",
      "Epoch 2206/2500\n",
      "120/120 [==============================] - 0s 95us/step - loss: 0.0545 - accuracy: 0.9750 - val_loss: 0.0406 - val_accuracy: 0.9667\n",
      "Epoch 2207/2500\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.0550 - accuracy: 0.9750 - val_loss: 0.0385 - val_accuracy: 0.9667\n",
      "Epoch 2208/2500\n",
      "120/120 [==============================] - 0s 87us/step - loss: 0.0601 - accuracy: 0.9750 - val_loss: 0.0536 - val_accuracy: 0.9667\n",
      "Epoch 2209/2500\n",
      "120/120 [==============================] - 0s 96us/step - loss: 0.0550 - accuracy: 0.9833 - val_loss: 0.0418 - val_accuracy: 0.9667\n",
      "Epoch 2210/2500\n",
      "120/120 [==============================] - 0s 100us/step - loss: 0.0550 - accuracy: 0.9750 - val_loss: 0.0371 - val_accuracy: 1.0000\n",
      "Epoch 2211/2500\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0542 - accuracy: 0.9750 - val_loss: 0.0371 - val_accuracy: 1.0000\n",
      "Epoch 2212/2500\n",
      "120/120 [==============================] - 0s 87us/step - loss: 0.0540 - accuracy: 0.9750 - val_loss: 0.0407 - val_accuracy: 0.9667\n",
      "Epoch 2213/2500\n",
      "120/120 [==============================] - 0s 93us/step - loss: 0.0550 - accuracy: 0.9833 - val_loss: 0.0463 - val_accuracy: 0.9667\n",
      "Epoch 2214/2500\n",
      "120/120 [==============================] - 0s 99us/step - loss: 0.0539 - accuracy: 0.9833 - val_loss: 0.0411 - val_accuracy: 0.9667\n",
      "Epoch 2215/2500\n",
      "120/120 [==============================] - 0s 110us/step - loss: 0.0542 - accuracy: 0.9833 - val_loss: 0.0430 - val_accuracy: 0.9667\n",
      "Epoch 2216/2500\n",
      "120/120 [==============================] - 0s 101us/step - loss: 0.0543 - accuracy: 0.9833 - val_loss: 0.0378 - val_accuracy: 1.0000\n",
      "Epoch 2217/2500\n",
      "120/120 [==============================] - 0s 85us/step - loss: 0.0547 - accuracy: 0.9833 - val_loss: 0.0416 - val_accuracy: 0.9667\n",
      "Epoch 2218/2500\n",
      "120/120 [==============================] - 0s 96us/step - loss: 0.0545 - accuracy: 0.9750 - val_loss: 0.0385 - val_accuracy: 1.0000\n",
      "Epoch 2219/2500\n",
      "120/120 [==============================] - 0s 114us/step - loss: 0.0541 - accuracy: 0.9750 - val_loss: 0.0387 - val_accuracy: 0.9667\n",
      "Epoch 2220/2500\n",
      "120/120 [==============================] - 0s 100us/step - loss: 0.0542 - accuracy: 0.9833 - val_loss: 0.0454 - val_accuracy: 0.9667\n",
      "Epoch 2221/2500\n",
      "120/120 [==============================] - 0s 102us/step - loss: 0.0541 - accuracy: 0.9833 - val_loss: 0.0443 - val_accuracy: 0.9667\n",
      "Epoch 2222/2500\n",
      "120/120 [==============================] - 0s 97us/step - loss: 0.0543 - accuracy: 0.9833 - val_loss: 0.0444 - val_accuracy: 0.9667\n",
      "Epoch 2223/2500\n",
      "120/120 [==============================] - 0s 105us/step - loss: 0.0538 - accuracy: 0.9833 - val_loss: 0.0412 - val_accuracy: 0.9667\n",
      "Epoch 2224/2500\n",
      "120/120 [==============================] - 0s 95us/step - loss: 0.0546 - accuracy: 0.9833 - val_loss: 0.0422 - val_accuracy: 0.9667\n",
      "Epoch 2225/2500\n",
      "120/120 [==============================] - 0s 99us/step - loss: 0.0546 - accuracy: 0.9833 - val_loss: 0.0373 - val_accuracy: 1.0000\n",
      "Epoch 2226/2500\n",
      "120/120 [==============================] - 0s 121us/step - loss: 0.0544 - accuracy: 0.9750 - val_loss: 0.0358 - val_accuracy: 1.0000\n",
      "Epoch 2227/2500\n",
      "120/120 [==============================] - 0s 113us/step - loss: 0.0549 - accuracy: 0.9750 - val_loss: 0.0381 - val_accuracy: 1.0000\n",
      "Epoch 2228/2500\n",
      "120/120 [==============================] - 0s 127us/step - loss: 0.0532 - accuracy: 0.9750 - val_loss: 0.0437 - val_accuracy: 0.9667\n",
      "Epoch 2229/2500\n",
      "120/120 [==============================] - 0s 159us/step - loss: 0.0540 - accuracy: 0.9833 - val_loss: 0.0469 - val_accuracy: 0.9667\n",
      "Epoch 2230/2500\n",
      "120/120 [==============================] - 0s 144us/step - loss: 0.0541 - accuracy: 0.9833 - val_loss: 0.0432 - val_accuracy: 0.9667\n",
      "Epoch 2231/2500\n",
      "120/120 [==============================] - 0s 171us/step - loss: 0.0538 - accuracy: 0.9833 - val_loss: 0.0428 - val_accuracy: 0.9667\n",
      "Epoch 2232/2500\n",
      "120/120 [==============================] - 0s 152us/step - loss: 0.0535 - accuracy: 0.9833 - val_loss: 0.0408 - val_accuracy: 0.9667\n",
      "Epoch 2233/2500\n",
      "120/120 [==============================] - 0s 198us/step - loss: 0.0546 - accuracy: 0.9833 - val_loss: 0.0430 - val_accuracy: 0.9667\n",
      "Epoch 2234/2500\n",
      "120/120 [==============================] - 0s 157us/step - loss: 0.0537 - accuracy: 0.9833 - val_loss: 0.0403 - val_accuracy: 0.9667\n",
      "Epoch 2235/2500\n",
      "120/120 [==============================] - 0s 103us/step - loss: 0.0553 - accuracy: 0.9750 - val_loss: 0.0380 - val_accuracy: 1.0000\n",
      "Epoch 2236/2500\n",
      "120/120 [==============================] - 0s 124us/step - loss: 0.0553 - accuracy: 0.9833 - val_loss: 0.0440 - val_accuracy: 0.9667\n",
      "Epoch 2237/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.0547 - accuracy: 0.9833 - val_loss: 0.0381 - val_accuracy: 1.0000\n",
      "Epoch 2238/2500\n",
      "120/120 [==============================] - 0s 109us/step - loss: 0.0545 - accuracy: 0.9750 - val_loss: 0.0371 - val_accuracy: 1.0000\n",
      "Epoch 2239/2500\n",
      "120/120 [==============================] - 0s 110us/step - loss: 0.0561 - accuracy: 0.9750 - val_loss: 0.0362 - val_accuracy: 1.0000\n",
      "Epoch 2240/2500\n",
      "120/120 [==============================] - 0s 104us/step - loss: 0.0536 - accuracy: 0.9750 - val_loss: 0.0444 - val_accuracy: 0.9667\n",
      "Epoch 2241/2500\n",
      "120/120 [==============================] - 0s 107us/step - loss: 0.0569 - accuracy: 0.9833 - val_loss: 0.0545 - val_accuracy: 0.9667\n",
      "Epoch 2242/2500\n",
      "120/120 [==============================] - 0s 99us/step - loss: 0.0550 - accuracy: 0.9833 - val_loss: 0.0503 - val_accuracy: 0.9667\n",
      "Epoch 2243/2500\n",
      "120/120 [==============================] - 0s 101us/step - loss: 0.0534 - accuracy: 0.9833 - val_loss: 0.0428 - val_accuracy: 0.9667\n",
      "Epoch 2244/2500\n",
      "120/120 [==============================] - 0s 110us/step - loss: 0.0562 - accuracy: 0.9750 - val_loss: 0.0330 - val_accuracy: 1.0000\n",
      "Epoch 2245/2500\n",
      "120/120 [==============================] - 0s 102us/step - loss: 0.0563 - accuracy: 0.9750 - val_loss: 0.0381 - val_accuracy: 1.0000\n",
      "Epoch 2246/2500\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.0543 - accuracy: 0.9833 - val_loss: 0.0405 - val_accuracy: 0.9667\n",
      "Epoch 2247/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.0549 - accuracy: 0.9833 - val_loss: 0.0365 - val_accuracy: 1.0000\n",
      "Epoch 2248/2500\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.0561 - accuracy: 0.9833 - val_loss: 0.0464 - val_accuracy: 0.9667\n",
      "Epoch 2249/2500\n",
      "120/120 [==============================] - 0s 112us/step - loss: 0.0544 - accuracy: 0.9833 - val_loss: 0.0466 - val_accuracy: 0.9667\n",
      "Epoch 2250/2500\n",
      "120/120 [==============================] - 0s 129us/step - loss: 0.0544 - accuracy: 0.9833 - val_loss: 0.0380 - val_accuracy: 1.0000\n",
      "Epoch 2251/2500\n",
      "120/120 [==============================] - 0s 109us/step - loss: 0.0539 - accuracy: 0.9750 - val_loss: 0.0397 - val_accuracy: 0.9667\n",
      "Epoch 2252/2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 115us/step - loss: 0.0544 - accuracy: 0.9833 - val_loss: 0.0419 - val_accuracy: 0.9667\n",
      "Epoch 2253/2500\n",
      "120/120 [==============================] - 0s 113us/step - loss: 0.0538 - accuracy: 0.9833 - val_loss: 0.0409 - val_accuracy: 0.9667\n",
      "Epoch 2254/2500\n",
      "120/120 [==============================] - 0s 104us/step - loss: 0.0546 - accuracy: 0.9833 - val_loss: 0.0400 - val_accuracy: 0.9667\n",
      "Epoch 2255/2500\n",
      "120/120 [==============================] - 0s 102us/step - loss: 0.0554 - accuracy: 0.9833 - val_loss: 0.0447 - val_accuracy: 0.9667\n",
      "Epoch 2256/2500\n",
      "120/120 [==============================] - 0s 110us/step - loss: 0.0537 - accuracy: 0.9833 - val_loss: 0.0400 - val_accuracy: 0.9667\n",
      "Epoch 2257/2500\n",
      "120/120 [==============================] - 0s 110us/step - loss: 0.0541 - accuracy: 0.9833 - val_loss: 0.0404 - val_accuracy: 0.9667\n",
      "Epoch 2258/2500\n",
      "120/120 [==============================] - 0s 105us/step - loss: 0.0542 - accuracy: 0.9833 - val_loss: 0.0412 - val_accuracy: 0.9667\n",
      "Epoch 2259/2500\n",
      "120/120 [==============================] - 0s 96us/step - loss: 0.0557 - accuracy: 0.9750 - val_loss: 0.0357 - val_accuracy: 1.0000\n",
      "Epoch 2260/2500\n",
      "120/120 [==============================] - 0s 102us/step - loss: 0.0538 - accuracy: 0.9750 - val_loss: 0.0394 - val_accuracy: 0.9667\n",
      "Epoch 2261/2500\n",
      "120/120 [==============================] - 0s 104us/step - loss: 0.0539 - accuracy: 0.9833 - val_loss: 0.0411 - val_accuracy: 0.9667\n",
      "Epoch 2262/2500\n",
      "120/120 [==============================] - 0s 101us/step - loss: 0.0544 - accuracy: 0.9833 - val_loss: 0.0393 - val_accuracy: 0.9667\n",
      "Epoch 2263/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.0540 - accuracy: 0.9833 - val_loss: 0.0417 - val_accuracy: 0.9667\n",
      "Epoch 2264/2500\n",
      "120/120 [==============================] - 0s 118us/step - loss: 0.0534 - accuracy: 0.9833 - val_loss: 0.0425 - val_accuracy: 0.9667\n",
      "Epoch 2265/2500\n",
      "120/120 [==============================] - 0s 98us/step - loss: 0.0542 - accuracy: 0.9833 - val_loss: 0.0434 - val_accuracy: 0.9667\n",
      "Epoch 2266/2500\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0536 - accuracy: 0.9833 - val_loss: 0.0397 - val_accuracy: 0.9667\n",
      "Epoch 2267/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.0555 - accuracy: 0.9750 - val_loss: 0.0367 - val_accuracy: 1.0000\n",
      "Epoch 2268/2500\n",
      "120/120 [==============================] - 0s 110us/step - loss: 0.0544 - accuracy: 0.9750 - val_loss: 0.0404 - val_accuracy: 0.9667\n",
      "Epoch 2269/2500\n",
      "120/120 [==============================] - 0s 104us/step - loss: 0.0537 - accuracy: 0.9833 - val_loss: 0.0408 - val_accuracy: 0.9667\n",
      "Epoch 2270/2500\n",
      "120/120 [==============================] - 0s 100us/step - loss: 0.0536 - accuracy: 0.9833 - val_loss: 0.0435 - val_accuracy: 0.9667\n",
      "Epoch 2271/2500\n",
      "120/120 [==============================] - 0s 112us/step - loss: 0.0539 - accuracy: 0.9833 - val_loss: 0.0419 - val_accuracy: 0.9667\n",
      "Epoch 2272/2500\n",
      "120/120 [==============================] - 0s 96us/step - loss: 0.0536 - accuracy: 0.9833 - val_loss: 0.0404 - val_accuracy: 0.9667\n",
      "Epoch 2273/2500\n",
      "120/120 [==============================] - 0s 101us/step - loss: 0.0540 - accuracy: 0.9833 - val_loss: 0.0411 - val_accuracy: 0.9667\n",
      "Epoch 2274/2500\n",
      "120/120 [==============================] - 0s 104us/step - loss: 0.0548 - accuracy: 0.9833 - val_loss: 0.0446 - val_accuracy: 0.9667\n",
      "Epoch 2275/2500\n",
      "120/120 [==============================] - 0s 130us/step - loss: 0.0542 - accuracy: 0.9833 - val_loss: 0.0396 - val_accuracy: 0.9667\n",
      "Epoch 2276/2500\n",
      "120/120 [==============================] - 0s 133us/step - loss: 0.0557 - accuracy: 0.9833 - val_loss: 0.0426 - val_accuracy: 0.9667\n",
      "Epoch 2277/2500\n",
      "120/120 [==============================] - 0s 120us/step - loss: 0.0536 - accuracy: 0.9833 - val_loss: 0.0376 - val_accuracy: 1.0000\n",
      "Epoch 2278/2500\n",
      "120/120 [==============================] - 0s 120us/step - loss: 0.0544 - accuracy: 0.9750 - val_loss: 0.0362 - val_accuracy: 1.0000\n",
      "Epoch 2279/2500\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.0539 - accuracy: 0.9750 - val_loss: 0.0370 - val_accuracy: 1.0000\n",
      "Epoch 2280/2500\n",
      "120/120 [==============================] - 0s 94us/step - loss: 0.0539 - accuracy: 0.9833 - val_loss: 0.0419 - val_accuracy: 0.9667\n",
      "Epoch 2281/2500\n",
      "120/120 [==============================] - 0s 113us/step - loss: 0.0536 - accuracy: 0.9833 - val_loss: 0.0423 - val_accuracy: 0.9667\n",
      "Epoch 2282/2500\n",
      "120/120 [==============================] - 0s 103us/step - loss: 0.0538 - accuracy: 0.9833 - val_loss: 0.0417 - val_accuracy: 0.9667\n",
      "Epoch 2283/2500\n",
      "120/120 [==============================] - 0s 101us/step - loss: 0.0559 - accuracy: 0.9750 - val_loss: 0.0373 - val_accuracy: 1.0000\n",
      "Epoch 2284/2500\n",
      "120/120 [==============================] - 0s 103us/step - loss: 0.0539 - accuracy: 0.9750 - val_loss: 0.0430 - val_accuracy: 0.9667\n",
      "Epoch 2285/2500\n",
      "120/120 [==============================] - 0s 116us/step - loss: 0.0557 - accuracy: 0.9833 - val_loss: 0.0392 - val_accuracy: 0.9667\n",
      "Epoch 2286/2500\n",
      "120/120 [==============================] - 0s 103us/step - loss: 0.0548 - accuracy: 0.9833 - val_loss: 0.0464 - val_accuracy: 0.9667\n",
      "Epoch 2287/2500\n",
      "120/120 [==============================] - 0s 99us/step - loss: 0.0540 - accuracy: 0.9833 - val_loss: 0.0458 - val_accuracy: 0.9667\n",
      "Epoch 2288/2500\n",
      "120/120 [==============================] - 0s 123us/step - loss: 0.0542 - accuracy: 0.9833 - val_loss: 0.0396 - val_accuracy: 0.9667\n",
      "Epoch 2289/2500\n",
      "120/120 [==============================] - 0s 114us/step - loss: 0.0534 - accuracy: 0.9833 - val_loss: 0.0396 - val_accuracy: 0.9667\n",
      "Epoch 2290/2500\n",
      "120/120 [==============================] - 0s 103us/step - loss: 0.0537 - accuracy: 0.9750 - val_loss: 0.0379 - val_accuracy: 1.0000\n",
      "Epoch 2291/2500\n",
      "120/120 [==============================] - 0s 102us/step - loss: 0.0536 - accuracy: 0.9833 - val_loss: 0.0419 - val_accuracy: 0.9667\n",
      "Epoch 2292/2500\n",
      "120/120 [==============================] - 0s 122us/step - loss: 0.0536 - accuracy: 0.9833 - val_loss: 0.0413 - val_accuracy: 0.9667\n",
      "Epoch 2293/2500\n",
      "120/120 [==============================] - 0s 104us/step - loss: 0.0538 - accuracy: 0.9833 - val_loss: 0.0401 - val_accuracy: 0.9667\n",
      "Epoch 2294/2500\n",
      "120/120 [==============================] - 0s 100us/step - loss: 0.0539 - accuracy: 0.9833 - val_loss: 0.0419 - val_accuracy: 0.9667\n",
      "Epoch 2295/2500\n",
      "120/120 [==============================] - 0s 119us/step - loss: 0.0543 - accuracy: 0.9833 - val_loss: 0.0432 - val_accuracy: 0.9667\n",
      "Epoch 2296/2500\n",
      "120/120 [==============================] - 0s 110us/step - loss: 0.0563 - accuracy: 0.9750 - val_loss: 0.0358 - val_accuracy: 1.0000\n",
      "Epoch 2297/2500\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.0573 - accuracy: 0.9750 - val_loss: 0.0465 - val_accuracy: 0.9667\n",
      "Epoch 2298/2500\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0556 - accuracy: 0.9750 - val_loss: 0.0368 - val_accuracy: 1.0000\n",
      "Epoch 2299/2500\n",
      "120/120 [==============================] - 0s 130us/step - loss: 0.0534 - accuracy: 0.9750 - val_loss: 0.0396 - val_accuracy: 0.9667\n",
      "Epoch 2300/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.0535 - accuracy: 0.9833 - val_loss: 0.0389 - val_accuracy: 0.9667\n",
      "Epoch 2301/2500\n",
      "120/120 [==============================] - 0s 113us/step - loss: 0.0545 - accuracy: 0.9750 - val_loss: 0.0381 - val_accuracy: 1.0000\n",
      "Epoch 2302/2500\n",
      "120/120 [==============================] - 0s 127us/step - loss: 0.0546 - accuracy: 0.9833 - val_loss: 0.0446 - val_accuracy: 0.9667\n",
      "Epoch 2303/2500\n",
      "120/120 [==============================] - 0s 87us/step - loss: 0.0547 - accuracy: 0.9833 - val_loss: 0.0412 - val_accuracy: 0.9667\n",
      "Epoch 2304/2500\n",
      "120/120 [==============================] - 0s 90us/step - loss: 0.0550 - accuracy: 0.9833 - val_loss: 0.0479 - val_accuracy: 0.9667\n",
      "Epoch 2305/2500\n",
      "120/120 [==============================] - 0s 102us/step - loss: 0.0564 - accuracy: 0.9833 - val_loss: 0.0396 - val_accuracy: 0.9667\n",
      "Epoch 2306/2500\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0542 - accuracy: 0.9833 - val_loss: 0.0405 - val_accuracy: 0.9667\n",
      "Epoch 2307/2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 97us/step - loss: 0.0538 - accuracy: 0.9833 - val_loss: 0.0400 - val_accuracy: 0.9667\n",
      "Epoch 2308/2500\n",
      "120/120 [==============================] - 0s 91us/step - loss: 0.0542 - accuracy: 0.9833 - val_loss: 0.0421 - val_accuracy: 0.9667\n",
      "Epoch 2309/2500\n",
      "120/120 [==============================] - 0s 101us/step - loss: 0.0543 - accuracy: 0.9833 - val_loss: 0.0365 - val_accuracy: 1.0000\n",
      "Epoch 2310/2500\n",
      "120/120 [==============================] - 0s 94us/step - loss: 0.0538 - accuracy: 0.9833 - val_loss: 0.0390 - val_accuracy: 0.9667\n",
      "Epoch 2311/2500\n",
      "120/120 [==============================] - 0s 94us/step - loss: 0.0533 - accuracy: 0.9833 - val_loss: 0.0391 - val_accuracy: 0.9667\n",
      "Epoch 2312/2500\n",
      "120/120 [==============================] - 0s 93us/step - loss: 0.0578 - accuracy: 0.9833 - val_loss: 0.0502 - val_accuracy: 0.9667\n",
      "Epoch 2313/2500\n",
      "120/120 [==============================] - 0s 86us/step - loss: 0.0542 - accuracy: 0.9833 - val_loss: 0.0398 - val_accuracy: 0.9667\n",
      "Epoch 2314/2500\n",
      "120/120 [==============================] - 0s 98us/step - loss: 0.0536 - accuracy: 0.9833 - val_loss: 0.0397 - val_accuracy: 0.9667\n",
      "Epoch 2315/2500\n",
      "120/120 [==============================] - 0s 97us/step - loss: 0.0539 - accuracy: 0.9833 - val_loss: 0.0384 - val_accuracy: 1.0000\n",
      "Epoch 2316/2500\n",
      "120/120 [==============================] - 0s 82us/step - loss: 0.0537 - accuracy: 0.9750 - val_loss: 0.0359 - val_accuracy: 1.0000\n",
      "Epoch 2317/2500\n",
      "120/120 [==============================] - 0s 92us/step - loss: 0.0538 - accuracy: 0.9750 - val_loss: 0.0373 - val_accuracy: 1.0000\n",
      "Epoch 2318/2500\n",
      "120/120 [==============================] - 0s 101us/step - loss: 0.0535 - accuracy: 0.9750 - val_loss: 0.0397 - val_accuracy: 0.9667\n",
      "Epoch 2319/2500\n",
      "120/120 [==============================] - 0s 99us/step - loss: 0.0540 - accuracy: 0.9833 - val_loss: 0.0402 - val_accuracy: 0.9667\n",
      "Epoch 2320/2500\n",
      "120/120 [==============================] - 0s 87us/step - loss: 0.0535 - accuracy: 0.9833 - val_loss: 0.0419 - val_accuracy: 0.9667\n",
      "Epoch 2321/2500\n",
      "120/120 [==============================] - 0s 94us/step - loss: 0.0551 - accuracy: 0.9750 - val_loss: 0.0373 - val_accuracy: 1.0000\n",
      "Epoch 2322/2500\n",
      "120/120 [==============================] - 0s 100us/step - loss: 0.0533 - accuracy: 0.9833 - val_loss: 0.0421 - val_accuracy: 0.9667\n",
      "Epoch 2323/2500\n",
      "120/120 [==============================] - 0s 99us/step - loss: 0.0565 - accuracy: 0.9833 - val_loss: 0.0508 - val_accuracy: 0.9667\n",
      "Epoch 2324/2500\n",
      "120/120 [==============================] - 0s 87us/step - loss: 0.0539 - accuracy: 0.9833 - val_loss: 0.0424 - val_accuracy: 0.9667\n",
      "Epoch 2325/2500\n",
      "120/120 [==============================] - 0s 100us/step - loss: 0.0556 - accuracy: 0.9750 - val_loss: 0.0349 - val_accuracy: 1.0000\n",
      "Epoch 2326/2500\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.0048 - accuracy: 1.00 - 0s 102us/step - loss: 0.0541 - accuracy: 0.9750 - val_loss: 0.0361 - val_accuracy: 1.0000\n",
      "Epoch 2327/2500\n",
      "120/120 [==============================] - 0s 102us/step - loss: 0.0544 - accuracy: 0.9833 - val_loss: 0.0434 - val_accuracy: 0.9667\n",
      "Epoch 2328/2500\n",
      "120/120 [==============================] - 0s 94us/step - loss: 0.0535 - accuracy: 0.9833 - val_loss: 0.0416 - val_accuracy: 0.9667\n",
      "Epoch 2329/2500\n",
      "120/120 [==============================] - 0s 104us/step - loss: 0.0541 - accuracy: 0.9833 - val_loss: 0.0380 - val_accuracy: 1.0000\n",
      "Epoch 2330/2500\n",
      "120/120 [==============================] - 0s 89us/step - loss: 0.0535 - accuracy: 0.9750 - val_loss: 0.0413 - val_accuracy: 0.9667\n",
      "Epoch 2331/2500\n",
      "120/120 [==============================] - 0s 95us/step - loss: 0.0553 - accuracy: 0.9833 - val_loss: 0.0471 - val_accuracy: 0.9667\n",
      "Epoch 2332/2500\n",
      "120/120 [==============================] - 0s 98us/step - loss: 0.0561 - accuracy: 0.9750 - val_loss: 0.0360 - val_accuracy: 1.0000\n",
      "Epoch 2333/2500\n",
      "120/120 [==============================] - 0s 96us/step - loss: 0.0535 - accuracy: 0.9750 - val_loss: 0.0387 - val_accuracy: 0.9667\n",
      "Epoch 2334/2500\n",
      "120/120 [==============================] - 0s 90us/step - loss: 0.0542 - accuracy: 0.9833 - val_loss: 0.0425 - val_accuracy: 0.9667\n",
      "Epoch 2335/2500\n",
      "120/120 [==============================] - 0s 97us/step - loss: 0.0551 - accuracy: 0.9750 - val_loss: 0.0385 - val_accuracy: 1.0000\n",
      "Epoch 2336/2500\n",
      "120/120 [==============================] - 0s 100us/step - loss: 0.0564 - accuracy: 0.9750 - val_loss: 0.0344 - val_accuracy: 1.0000\n",
      "Epoch 2337/2500\n",
      "120/120 [==============================] - 0s 116us/step - loss: 0.0602 - accuracy: 0.9833 - val_loss: 0.0501 - val_accuracy: 0.9667\n",
      "Epoch 2338/2500\n",
      "120/120 [==============================] - 0s 97us/step - loss: 0.0574 - accuracy: 0.9833 - val_loss: 0.0378 - val_accuracy: 1.0000\n",
      "Epoch 2339/2500\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.0560 - accuracy: 0.9833 - val_loss: 0.0434 - val_accuracy: 0.9667\n",
      "Epoch 2340/2500\n",
      "120/120 [==============================] - 0s 116us/step - loss: 0.0533 - accuracy: 0.9833 - val_loss: 0.0396 - val_accuracy: 0.9667\n",
      "Epoch 2341/2500\n",
      "120/120 [==============================] - 0s 122us/step - loss: 0.0538 - accuracy: 0.9750 - val_loss: 0.0381 - val_accuracy: 1.0000\n",
      "Epoch 2342/2500\n",
      "120/120 [==============================] - 0s 109us/step - loss: 0.0533 - accuracy: 0.9750 - val_loss: 0.0387 - val_accuracy: 0.9667\n",
      "Epoch 2343/2500\n",
      "120/120 [==============================] - 0s 112us/step - loss: 0.0534 - accuracy: 0.9833 - val_loss: 0.0397 - val_accuracy: 0.9667\n",
      "Epoch 2344/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.0545 - accuracy: 0.9833 - val_loss: 0.0458 - val_accuracy: 0.9667\n",
      "Epoch 2345/2500\n",
      "120/120 [==============================] - 0s 102us/step - loss: 0.0539 - accuracy: 0.9833 - val_loss: 0.0457 - val_accuracy: 0.9667\n",
      "Epoch 2346/2500\n",
      "120/120 [==============================] - 0s 94us/step - loss: 0.0534 - accuracy: 0.9833 - val_loss: 0.0409 - val_accuracy: 0.9667\n",
      "Epoch 2347/2500\n",
      "120/120 [==============================] - 0s 116us/step - loss: 0.0532 - accuracy: 0.9833 - val_loss: 0.0385 - val_accuracy: 1.0000\n",
      "Epoch 2348/2500\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0541 - accuracy: 0.9833 - val_loss: 0.0400 - val_accuracy: 0.9667\n",
      "Epoch 2349/2500\n",
      "120/120 [==============================] - 0s 105us/step - loss: 0.0534 - accuracy: 0.9750 - val_loss: 0.0372 - val_accuracy: 1.0000\n",
      "Epoch 2350/2500\n",
      "120/120 [==============================] - 0s 107us/step - loss: 0.0543 - accuracy: 0.9750 - val_loss: 0.0344 - val_accuracy: 1.0000\n",
      "Epoch 2351/2500\n",
      "120/120 [==============================] - 0s 113us/step - loss: 0.0541 - accuracy: 0.9750 - val_loss: 0.0360 - val_accuracy: 1.0000\n",
      "Epoch 2352/2500\n",
      "120/120 [==============================] - 0s 92us/step - loss: 0.0549 - accuracy: 0.9750 - val_loss: 0.0354 - val_accuracy: 1.0000\n",
      "Epoch 2353/2500\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0536 - accuracy: 0.9750 - val_loss: 0.0470 - val_accuracy: 0.9667\n",
      "Epoch 2354/2500\n",
      "120/120 [==============================] - 0s 127us/step - loss: 0.0557 - accuracy: 0.9833 - val_loss: 0.0486 - val_accuracy: 0.9667\n",
      "Epoch 2355/2500\n",
      "120/120 [==============================] - 0s 116us/step - loss: 0.0541 - accuracy: 0.9833 - val_loss: 0.0428 - val_accuracy: 0.9667\n",
      "Epoch 2356/2500\n",
      "120/120 [==============================] - 0s 110us/step - loss: 0.0538 - accuracy: 0.9833 - val_loss: 0.0423 - val_accuracy: 0.9667\n",
      "Epoch 2357/2500\n",
      "120/120 [==============================] - 0s 133us/step - loss: 0.0533 - accuracy: 0.9750 - val_loss: 0.0361 - val_accuracy: 1.0000\n",
      "Epoch 2358/2500\n",
      "120/120 [==============================] - 0s 124us/step - loss: 0.0536 - accuracy: 0.9750 - val_loss: 0.0365 - val_accuracy: 1.0000\n",
      "Epoch 2359/2500\n",
      "120/120 [==============================] - 0s 116us/step - loss: 0.0536 - accuracy: 0.9750 - val_loss: 0.0373 - val_accuracy: 1.0000\n",
      "Epoch 2360/2500\n",
      "120/120 [==============================] - 0s 142us/step - loss: 0.0548 - accuracy: 0.9833 - val_loss: 0.0446 - val_accuracy: 0.9667\n",
      "Epoch 2361/2500\n",
      "120/120 [==============================] - 0s 127us/step - loss: 0.0535 - accuracy: 0.9833 - val_loss: 0.0396 - val_accuracy: 0.9667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2362/2500\n",
      "120/120 [==============================] - 0s 118us/step - loss: 0.0557 - accuracy: 0.9833 - val_loss: 0.0426 - val_accuracy: 0.9667\n",
      "Epoch 2363/2500\n",
      "120/120 [==============================] - 0s 119us/step - loss: 0.0533 - accuracy: 0.9833 - val_loss: 0.0376 - val_accuracy: 1.0000\n",
      "Epoch 2364/2500\n",
      "120/120 [==============================] - 0s 132us/step - loss: 0.0543 - accuracy: 0.9750 - val_loss: 0.0356 - val_accuracy: 1.0000\n",
      "Epoch 2365/2500\n",
      "120/120 [==============================] - 0s 114us/step - loss: 0.0531 - accuracy: 0.9750 - val_loss: 0.0382 - val_accuracy: 1.0000\n",
      "Epoch 2366/2500\n",
      "120/120 [==============================] - 0s 126us/step - loss: 0.0546 - accuracy: 0.9833 - val_loss: 0.0437 - val_accuracy: 0.9667\n",
      "Epoch 2367/2500\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.0546 - accuracy: 0.9833 - val_loss: 0.0388 - val_accuracy: 0.9667\n",
      "Epoch 2368/2500\n",
      "120/120 [==============================] - 0s 107us/step - loss: 0.0540 - accuracy: 0.9833 - val_loss: 0.0430 - val_accuracy: 0.9667\n",
      "Epoch 2369/2500\n",
      "120/120 [==============================] - 0s 110us/step - loss: 0.0546 - accuracy: 0.9833 - val_loss: 0.0445 - val_accuracy: 0.9667\n",
      "Epoch 2370/2500\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0532 - accuracy: 0.9833 - val_loss: 0.0390 - val_accuracy: 0.9667\n",
      "Epoch 2371/2500\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.0534 - accuracy: 0.9833 - val_loss: 0.0378 - val_accuracy: 1.0000\n",
      "Epoch 2372/2500\n",
      "120/120 [==============================] - 0s 114us/step - loss: 0.0535 - accuracy: 0.9833 - val_loss: 0.0376 - val_accuracy: 1.0000\n",
      "Epoch 2373/2500\n",
      "120/120 [==============================] - 0s 137us/step - loss: 0.0539 - accuracy: 0.9750 - val_loss: 0.0379 - val_accuracy: 1.0000\n",
      "Epoch 2374/2500\n",
      "120/120 [==============================] - 0s 112us/step - loss: 0.0538 - accuracy: 0.9750 - val_loss: 0.0380 - val_accuracy: 1.0000\n",
      "Epoch 2375/2500\n",
      "120/120 [==============================] - 0s 136us/step - loss: 0.0542 - accuracy: 0.9833 - val_loss: 0.0435 - val_accuracy: 0.9667\n",
      "Epoch 2376/2500\n",
      "120/120 [==============================] - 0s 162us/step - loss: 0.0535 - accuracy: 0.9833 - val_loss: 0.0428 - val_accuracy: 0.9667\n",
      "Epoch 2377/2500\n",
      "120/120 [==============================] - 0s 151us/step - loss: 0.0559 - accuracy: 0.9750 - val_loss: 0.0371 - val_accuracy: 1.0000\n",
      "Epoch 2378/2500\n",
      "120/120 [==============================] - 0s 182us/step - loss: 0.0535 - accuracy: 0.9750 - val_loss: 0.0405 - val_accuracy: 0.9667\n",
      "Epoch 2379/2500\n",
      "120/120 [==============================] - 0s 161us/step - loss: 0.0534 - accuracy: 0.9833 - val_loss: 0.0417 - val_accuracy: 0.9667\n",
      "Epoch 2380/2500\n",
      "120/120 [==============================] - 0s 150us/step - loss: 0.0543 - accuracy: 0.9833 - val_loss: 0.0437 - val_accuracy: 0.9667\n",
      "Epoch 2381/2500\n",
      "120/120 [==============================] - 0s 127us/step - loss: 0.0547 - accuracy: 0.9833 - val_loss: 0.0362 - val_accuracy: 1.0000\n",
      "Epoch 2382/2500\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.0560 - accuracy: 0.9750 - val_loss: 0.0324 - val_accuracy: 1.0000\n",
      "Epoch 2383/2500\n",
      "120/120 [==============================] - 0s 161us/step - loss: 0.0558 - accuracy: 0.9833 - val_loss: 0.0432 - val_accuracy: 0.9667\n",
      "Epoch 2384/2500\n",
      "120/120 [==============================] - 0s 161us/step - loss: 0.0531 - accuracy: 0.9833 - val_loss: 0.0442 - val_accuracy: 0.9667\n",
      "Epoch 2385/2500\n",
      "120/120 [==============================] - 0s 168us/step - loss: 0.0541 - accuracy: 0.9833 - val_loss: 0.0459 - val_accuracy: 0.9667\n",
      "Epoch 2386/2500\n",
      "120/120 [==============================] - 0s 182us/step - loss: 0.0533 - accuracy: 0.9833 - val_loss: 0.0423 - val_accuracy: 0.9667\n",
      "Epoch 2387/2500\n",
      "120/120 [==============================] - 0s 172us/step - loss: 0.0533 - accuracy: 0.9833 - val_loss: 0.0365 - val_accuracy: 1.0000\n",
      "Epoch 2388/2500\n",
      "120/120 [==============================] - 0s 177us/step - loss: 0.0560 - accuracy: 0.9750 - val_loss: 0.0331 - val_accuracy: 1.0000\n",
      "Epoch 2389/2500\n",
      "120/120 [==============================] - 0s 157us/step - loss: 0.0537 - accuracy: 0.9750 - val_loss: 0.0397 - val_accuracy: 0.9667\n",
      "Epoch 2390/2500\n",
      "120/120 [==============================] - 0s 161us/step - loss: 0.0537 - accuracy: 0.9833 - val_loss: 0.0435 - val_accuracy: 0.9667\n",
      "Epoch 2391/2500\n",
      "120/120 [==============================] - 0s 156us/step - loss: 0.0547 - accuracy: 0.9750 - val_loss: 0.0377 - val_accuracy: 1.0000\n",
      "Epoch 2392/2500\n",
      "120/120 [==============================] - 0s 196us/step - loss: 0.0543 - accuracy: 0.9833 - val_loss: 0.0427 - val_accuracy: 0.9667\n",
      "Epoch 2393/2500\n",
      "120/120 [==============================] - 0s 176us/step - loss: 0.0535 - accuracy: 0.9833 - val_loss: 0.0381 - val_accuracy: 1.0000\n",
      "Epoch 2394/2500\n",
      "120/120 [==============================] - 0s 193us/step - loss: 0.0559 - accuracy: 0.9833 - val_loss: 0.0445 - val_accuracy: 0.9667\n",
      "Epoch 2395/2500\n",
      "120/120 [==============================] - 0s 163us/step - loss: 0.0554 - accuracy: 0.9750 - val_loss: 0.0353 - val_accuracy: 1.0000\n",
      "Epoch 2396/2500\n",
      "120/120 [==============================] - 0s 183us/step - loss: 0.0551 - accuracy: 0.9750 - val_loss: 0.0415 - val_accuracy: 0.9667\n",
      "Epoch 2397/2500\n",
      "120/120 [==============================] - 0s 193us/step - loss: 0.0541 - accuracy: 0.9750 - val_loss: 0.0362 - val_accuracy: 1.0000\n",
      "Epoch 2398/2500\n",
      "120/120 [==============================] - 0s 198us/step - loss: 0.0563 - accuracy: 0.9833 - val_loss: 0.0445 - val_accuracy: 0.9667\n",
      "Epoch 2399/2500\n",
      "120/120 [==============================] - 0s 151us/step - loss: 0.0572 - accuracy: 0.9750 - val_loss: 0.0347 - val_accuracy: 1.0000\n",
      "Epoch 2400/2500\n",
      "120/120 [==============================] - 0s 176us/step - loss: 0.0536 - accuracy: 0.9750 - val_loss: 0.0361 - val_accuracy: 1.0000\n",
      "Epoch 2401/2500\n",
      "120/120 [==============================] - 0s 156us/step - loss: 0.0532 - accuracy: 0.9750 - val_loss: 0.0406 - val_accuracy: 0.9667\n",
      "Epoch 2402/2500\n",
      "120/120 [==============================] - 0s 150us/step - loss: 0.0535 - accuracy: 0.9833 - val_loss: 0.0443 - val_accuracy: 0.9667\n",
      "Epoch 2403/2500\n",
      "120/120 [==============================] - 0s 168us/step - loss: 0.0537 - accuracy: 0.9833 - val_loss: 0.0402 - val_accuracy: 0.9667\n",
      "Epoch 2404/2500\n",
      "120/120 [==============================] - 0s 143us/step - loss: 0.0529 - accuracy: 0.9833 - val_loss: 0.0402 - val_accuracy: 0.9667\n",
      "Epoch 2405/2500\n",
      "120/120 [==============================] - 0s 186us/step - loss: 0.0533 - accuracy: 0.9833 - val_loss: 0.0397 - val_accuracy: 0.9667\n",
      "Epoch 2406/2500\n",
      "120/120 [==============================] - 0s 157us/step - loss: 0.0537 - accuracy: 0.9833 - val_loss: 0.0419 - val_accuracy: 0.9667\n",
      "Epoch 2407/2500\n",
      "120/120 [==============================] - 0s 205us/step - loss: 0.0545 - accuracy: 0.9833 - val_loss: 0.0447 - val_accuracy: 0.9667\n",
      "Epoch 2408/2500\n",
      "120/120 [==============================] - 0s 175us/step - loss: 0.0526 - accuracy: 0.9833 - val_loss: 0.0386 - val_accuracy: 1.0000\n",
      "Epoch 2409/2500\n",
      "120/120 [==============================] - 0s 199us/step - loss: 0.0531 - accuracy: 0.9750 - val_loss: 0.0353 - val_accuracy: 1.0000\n",
      "Epoch 2410/2500\n",
      "120/120 [==============================] - 0s 175us/step - loss: 0.0537 - accuracy: 0.9750 - val_loss: 0.0364 - val_accuracy: 1.0000\n",
      "Epoch 2411/2500\n",
      "120/120 [==============================] - 0s 157us/step - loss: 0.0555 - accuracy: 0.9750 - val_loss: 0.0326 - val_accuracy: 1.0000\n",
      "Epoch 2412/2500\n",
      "120/120 [==============================] - 0s 134us/step - loss: 0.0553 - accuracy: 0.9833 - val_loss: 0.0424 - val_accuracy: 0.9667\n",
      "Epoch 2413/2500\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0549 - accuracy: 0.9833 - val_loss: 0.0396 - val_accuracy: 0.9667\n",
      "Epoch 2414/2500\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.0540 - accuracy: 0.9833 - val_loss: 0.0424 - val_accuracy: 0.9667\n",
      "Epoch 2415/2500\n",
      "120/120 [==============================] - 0s 120us/step - loss: 0.0537 - accuracy: 0.9833 - val_loss: 0.0427 - val_accuracy: 0.9667\n",
      "Epoch 2416/2500\n",
      "120/120 [==============================] - 0s 128us/step - loss: 0.0538 - accuracy: 0.9833 - val_loss: 0.0375 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2417/2500\n",
      "120/120 [==============================] - 0s 120us/step - loss: 0.0573 - accuracy: 0.9750 - val_loss: 0.0320 - val_accuracy: 1.0000\n",
      "Epoch 2418/2500\n",
      "120/120 [==============================] - 0s 123us/step - loss: 0.0533 - accuracy: 0.9833 - val_loss: 0.0409 - val_accuracy: 0.9667\n",
      "Epoch 2419/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.0545 - accuracy: 0.9833 - val_loss: 0.0471 - val_accuracy: 0.9667\n",
      "Epoch 2420/2500\n",
      "120/120 [==============================] - 0s 122us/step - loss: 0.0539 - accuracy: 0.9833 - val_loss: 0.0476 - val_accuracy: 0.9667\n",
      "Epoch 2421/2500\n",
      "120/120 [==============================] - 0s 116us/step - loss: 0.0548 - accuracy: 0.9833 - val_loss: 0.0380 - val_accuracy: 1.0000\n",
      "Epoch 2422/2500\n",
      "120/120 [==============================] - 0s 98us/step - loss: 0.0542 - accuracy: 0.9750 - val_loss: 0.0352 - val_accuracy: 1.0000\n",
      "Epoch 2423/2500\n",
      "120/120 [==============================] - 0s 93us/step - loss: 0.0539 - accuracy: 0.9750 - val_loss: 0.0364 - val_accuracy: 1.0000\n",
      "Epoch 2424/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.0539 - accuracy: 0.9750 - val_loss: 0.0425 - val_accuracy: 0.9667\n",
      "Epoch 2425/2500\n",
      "120/120 [==============================] - 0s 109us/step - loss: 0.0541 - accuracy: 0.9833 - val_loss: 0.0412 - val_accuracy: 0.9667\n",
      "Epoch 2426/2500\n",
      "120/120 [==============================] - 0s 105us/step - loss: 0.0539 - accuracy: 0.9833 - val_loss: 0.0383 - val_accuracy: 1.0000\n",
      "Epoch 2427/2500\n",
      "120/120 [==============================] - 0s 112us/step - loss: 0.0534 - accuracy: 0.9833 - val_loss: 0.0418 - val_accuracy: 0.9667\n",
      "Epoch 2428/2500\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.0534 - accuracy: 0.9833 - val_loss: 0.0396 - val_accuracy: 0.9667\n",
      "Epoch 2429/2500\n",
      "120/120 [==============================] - 0s 95us/step - loss: 0.0528 - accuracy: 0.9833 - val_loss: 0.0397 - val_accuracy: 0.9667\n",
      "Epoch 2430/2500\n",
      "120/120 [==============================] - 0s 94us/step - loss: 0.0552 - accuracy: 0.9833 - val_loss: 0.0453 - val_accuracy: 0.9667\n",
      "Epoch 2431/2500\n",
      "120/120 [==============================] - 0s 124us/step - loss: 0.0531 - accuracy: 0.9833 - val_loss: 0.0411 - val_accuracy: 0.9667\n",
      "Epoch 2432/2500\n",
      "120/120 [==============================] - 0s 102us/step - loss: 0.0539 - accuracy: 0.9750 - val_loss: 0.0343 - val_accuracy: 1.0000\n",
      "Epoch 2433/2500\n",
      "120/120 [==============================] - 0s 98us/step - loss: 0.0535 - accuracy: 0.9750 - val_loss: 0.0361 - val_accuracy: 1.0000\n",
      "Epoch 2434/2500\n",
      "120/120 [==============================] - 0s 114us/step - loss: 0.0539 - accuracy: 0.9750 - val_loss: 0.0399 - val_accuracy: 0.9667\n",
      "Epoch 2435/2500\n",
      "120/120 [==============================] - 0s 116us/step - loss: 0.0529 - accuracy: 0.9833 - val_loss: 0.0397 - val_accuracy: 0.9667\n",
      "Epoch 2436/2500\n",
      "120/120 [==============================] - 0s 101us/step - loss: 0.0555 - accuracy: 0.9833 - val_loss: 0.0426 - val_accuracy: 0.9667\n",
      "Epoch 2437/2500\n",
      "120/120 [==============================] - 0s 96us/step - loss: 0.0543 - accuracy: 0.9833 - val_loss: 0.0418 - val_accuracy: 0.9667\n",
      "Epoch 2438/2500\n",
      "120/120 [==============================] - 0s 118us/step - loss: 0.0525 - accuracy: 0.9833 - val_loss: 0.0362 - val_accuracy: 1.0000\n",
      "Epoch 2439/2500\n",
      "120/120 [==============================] - 0s 112us/step - loss: 0.0550 - accuracy: 0.9750 - val_loss: 0.0320 - val_accuracy: 1.0000\n",
      "Epoch 2440/2500\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.0537 - accuracy: 0.9750 - val_loss: 0.0345 - val_accuracy: 1.0000\n",
      "Epoch 2441/2500\n",
      "120/120 [==============================] - 0s 119us/step - loss: 0.0536 - accuracy: 0.9750 - val_loss: 0.0408 - val_accuracy: 0.9667\n",
      "Epoch 2442/2500\n",
      "120/120 [==============================] - 0s 126us/step - loss: 0.0543 - accuracy: 0.9833 - val_loss: 0.0452 - val_accuracy: 0.9667\n",
      "Epoch 2443/2500\n",
      "120/120 [==============================] - 0s 115us/step - loss: 0.0540 - accuracy: 0.9833 - val_loss: 0.0391 - val_accuracy: 0.9667\n",
      "Epoch 2444/2500\n",
      "120/120 [==============================] - 0s 116us/step - loss: 0.0531 - accuracy: 0.9833 - val_loss: 0.0389 - val_accuracy: 0.9667\n",
      "Epoch 2445/2500\n",
      "120/120 [==============================] - 0s 120us/step - loss: 0.0532 - accuracy: 0.9833 - val_loss: 0.0392 - val_accuracy: 0.9667\n",
      "Epoch 2446/2500\n",
      "120/120 [==============================] - 0s 129us/step - loss: 0.0537 - accuracy: 0.9833 - val_loss: 0.0391 - val_accuracy: 0.9667\n",
      "Epoch 2447/2500\n",
      "120/120 [==============================] - 0s 129us/step - loss: 0.0530 - accuracy: 0.9833 - val_loss: 0.0369 - val_accuracy: 1.0000\n",
      "Epoch 2448/2500\n",
      "120/120 [==============================] - 0s 147us/step - loss: 0.0532 - accuracy: 0.9833 - val_loss: 0.0399 - val_accuracy: 0.9667\n",
      "Epoch 2449/2500\n",
      "120/120 [==============================] - 0s 131us/step - loss: 0.0528 - accuracy: 0.9833 - val_loss: 0.0396 - val_accuracy: 0.9667\n",
      "Epoch 2450/2500\n",
      "120/120 [==============================] - 0s 140us/step - loss: 0.0532 - accuracy: 0.9833 - val_loss: 0.0397 - val_accuracy: 0.9667\n",
      "Epoch 2451/2500\n",
      "120/120 [==============================] - 0s 155us/step - loss: 0.0530 - accuracy: 0.9833 - val_loss: 0.0392 - val_accuracy: 0.9667\n",
      "Epoch 2452/2500\n",
      "120/120 [==============================] - 0s 140us/step - loss: 0.0535 - accuracy: 0.9833 - val_loss: 0.0374 - val_accuracy: 1.0000\n",
      "Epoch 2453/2500\n",
      "120/120 [==============================] - 0s 146us/step - loss: 0.0531 - accuracy: 0.9750 - val_loss: 0.0374 - val_accuracy: 1.0000\n",
      "Epoch 2454/2500\n",
      "120/120 [==============================] - 0s 142us/step - loss: 0.0536 - accuracy: 0.9833 - val_loss: 0.0419 - val_accuracy: 0.9667\n",
      "Epoch 2455/2500\n",
      "120/120 [==============================] - 0s 157us/step - loss: 0.0533 - accuracy: 0.9833 - val_loss: 0.0398 - val_accuracy: 0.9667\n",
      "Epoch 2456/2500\n",
      "120/120 [==============================] - 0s 136us/step - loss: 0.0543 - accuracy: 0.9833 - val_loss: 0.0361 - val_accuracy: 1.0000\n",
      "Epoch 2457/2500\n",
      "120/120 [==============================] - 0s 139us/step - loss: 0.0534 - accuracy: 0.9833 - val_loss: 0.0409 - val_accuracy: 0.9667\n",
      "Epoch 2458/2500\n",
      "120/120 [==============================] - 0s 123us/step - loss: 0.0540 - accuracy: 0.9833 - val_loss: 0.0375 - val_accuracy: 1.0000\n",
      "Epoch 2459/2500\n",
      "120/120 [==============================] - 0s 107us/step - loss: 0.0569 - accuracy: 0.9833 - val_loss: 0.0474 - val_accuracy: 0.9667\n",
      "Epoch 2460/2500\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.0533 - accuracy: 0.9833 - val_loss: 0.0396 - val_accuracy: 0.9667\n",
      "Epoch 2461/2500\n",
      "120/120 [==============================] - 0s 143us/step - loss: 0.0543 - accuracy: 0.9833 - val_loss: 0.0414 - val_accuracy: 0.9667\n",
      "Epoch 2462/2500\n",
      "120/120 [==============================] - 0s 131us/step - loss: 0.0538 - accuracy: 0.9750 - val_loss: 0.0330 - val_accuracy: 1.0000\n",
      "Epoch 2463/2500\n",
      "120/120 [==============================] - 0s 124us/step - loss: 0.0537 - accuracy: 0.9750 - val_loss: 0.0338 - val_accuracy: 1.0000\n",
      "Epoch 2464/2500\n",
      "120/120 [==============================] - 0s 143us/step - loss: 0.0537 - accuracy: 0.9750 - val_loss: 0.0349 - val_accuracy: 1.0000\n",
      "Epoch 2465/2500\n",
      "120/120 [==============================] - 0s 120us/step - loss: 0.0532 - accuracy: 0.9833 - val_loss: 0.0408 - val_accuracy: 0.9667\n",
      "Epoch 2466/2500\n",
      "120/120 [==============================] - 0s 115us/step - loss: 0.0551 - accuracy: 0.9833 - val_loss: 0.0372 - val_accuracy: 1.0000\n",
      "Epoch 2467/2500\n",
      "120/120 [==============================] - 0s 119us/step - loss: 0.0544 - accuracy: 0.9750 - val_loss: 0.0373 - val_accuracy: 1.0000\n",
      "Epoch 2468/2500\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.0537 - accuracy: 0.9833 - val_loss: 0.0459 - val_accuracy: 0.9667\n",
      "Epoch 2469/2500\n",
      "120/120 [==============================] - 0s 109us/step - loss: 0.0545 - accuracy: 0.9833 - val_loss: 0.0437 - val_accuracy: 0.9667\n",
      "Epoch 2470/2500\n",
      "120/120 [==============================] - 0s 121us/step - loss: 0.0535 - accuracy: 0.9833 - val_loss: 0.0421 - val_accuracy: 0.9667\n",
      "Epoch 2471/2500\n",
      "120/120 [==============================] - 0s 137us/step - loss: 0.0536 - accuracy: 0.9833 - val_loss: 0.0414 - val_accuracy: 0.9667\n",
      "Epoch 2472/2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 180us/step - loss: 0.0571 - accuracy: 0.9750 - val_loss: 0.0318 - val_accuracy: 1.0000\n",
      "Epoch 2473/2500\n",
      "120/120 [==============================] - 0s 154us/step - loss: 0.0546 - accuracy: 0.9750 - val_loss: 0.0374 - val_accuracy: 1.0000\n",
      "Epoch 2474/2500\n",
      "120/120 [==============================] - 0s 115us/step - loss: 0.0535 - accuracy: 0.9833 - val_loss: 0.0404 - val_accuracy: 0.9667\n",
      "Epoch 2475/2500\n",
      "120/120 [==============================] - 0s 138us/step - loss: 0.0532 - accuracy: 0.9833 - val_loss: 0.0421 - val_accuracy: 0.9667\n",
      "Epoch 2476/2500\n",
      "120/120 [==============================] - 0s 142us/step - loss: 0.0528 - accuracy: 0.9833 - val_loss: 0.0384 - val_accuracy: 1.0000\n",
      "Epoch 2477/2500\n",
      "120/120 [==============================] - 0s 135us/step - loss: 0.0533 - accuracy: 0.9750 - val_loss: 0.0356 - val_accuracy: 1.0000\n",
      "Epoch 2478/2500\n",
      "120/120 [==============================] - 0s 141us/step - loss: 0.0533 - accuracy: 0.9750 - val_loss: 0.0384 - val_accuracy: 1.0000\n",
      "Epoch 2479/2500\n",
      "120/120 [==============================] - 0s 145us/step - loss: 0.0529 - accuracy: 0.9833 - val_loss: 0.0399 - val_accuracy: 0.9667\n",
      "Epoch 2480/2500\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.0536 - accuracy: 0.9833 - val_loss: 0.0416 - val_accuracy: 0.9667\n",
      "Epoch 2481/2500\n",
      "120/120 [==============================] - 0s 161us/step - loss: 0.0541 - accuracy: 0.9833 - val_loss: 0.0429 - val_accuracy: 0.9667\n",
      "Epoch 2482/2500\n",
      "120/120 [==============================] - 0s 162us/step - loss: 0.0560 - accuracy: 0.9750 - val_loss: 0.0328 - val_accuracy: 1.0000\n",
      "Epoch 2483/2500\n",
      "120/120 [==============================] - 0s 175us/step - loss: 0.0544 - accuracy: 0.9750 - val_loss: 0.0344 - val_accuracy: 1.0000\n",
      "Epoch 2484/2500\n",
      "120/120 [==============================] - 0s 114us/step - loss: 0.0576 - accuracy: 0.9833 - val_loss: 0.0467 - val_accuracy: 0.9667\n",
      "Epoch 2485/2500\n",
      "120/120 [==============================] - 0s 154us/step - loss: 0.0547 - accuracy: 0.9833 - val_loss: 0.0460 - val_accuracy: 0.9667\n",
      "Epoch 2486/2500\n",
      "120/120 [==============================] - 0s 134us/step - loss: 0.0543 - accuracy: 0.9833 - val_loss: 0.0363 - val_accuracy: 1.0000\n",
      "Epoch 2487/2500\n",
      "120/120 [==============================] - 0s 134us/step - loss: 0.0528 - accuracy: 0.9750 - val_loss: 0.0367 - val_accuracy: 1.0000\n",
      "Epoch 2488/2500\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.0548 - accuracy: 0.9833 - val_loss: 0.0416 - val_accuracy: 0.9667\n",
      "Epoch 2489/2500\n",
      "120/120 [==============================] - 0s 128us/step - loss: 0.0533 - accuracy: 0.9833 - val_loss: 0.0413 - val_accuracy: 0.9667\n",
      "Epoch 2490/2500\n",
      "120/120 [==============================] - 0s 116us/step - loss: 0.0534 - accuracy: 0.9833 - val_loss: 0.0400 - val_accuracy: 0.9667\n",
      "Epoch 2491/2500\n",
      "120/120 [==============================] - 0s 115us/step - loss: 0.0544 - accuracy: 0.9833 - val_loss: 0.0333 - val_accuracy: 1.0000\n",
      "Epoch 2492/2500\n",
      "120/120 [==============================] - 0s 113us/step - loss: 0.0534 - accuracy: 0.9750 - val_loss: 0.0333 - val_accuracy: 1.0000\n",
      "Epoch 2493/2500\n",
      "120/120 [==============================] - 0s 105us/step - loss: 0.0549 - accuracy: 0.9833 - val_loss: 0.0418 - val_accuracy: 0.9667\n",
      "Epoch 2494/2500\n",
      "120/120 [==============================] - 0s 99us/step - loss: 0.0529 - accuracy: 0.9833 - val_loss: 0.0425 - val_accuracy: 0.9667\n",
      "Epoch 2495/2500\n",
      "120/120 [==============================] - 0s 118us/step - loss: 0.0540 - accuracy: 0.9750 - val_loss: 0.0363 - val_accuracy: 1.0000\n",
      "Epoch 2496/2500\n",
      "120/120 [==============================] - 0s 110us/step - loss: 0.0535 - accuracy: 0.9750 - val_loss: 0.0384 - val_accuracy: 1.0000\n",
      "Epoch 2497/2500\n",
      "120/120 [==============================] - 0s 102us/step - loss: 0.0551 - accuracy: 0.9750 - val_loss: 0.0340 - val_accuracy: 1.0000\n",
      "Epoch 2498/2500\n",
      "120/120 [==============================] - 0s 126us/step - loss: 0.0551 - accuracy: 0.9833 - val_loss: 0.0413 - val_accuracy: 0.9667\n",
      "Epoch 2499/2500\n",
      "120/120 [==============================] - 0s 109us/step - loss: 0.0534 - accuracy: 0.9833 - val_loss: 0.0408 - val_accuracy: 0.9667\n",
      "Epoch 2500/2500\n",
      "120/120 [==============================] - 0s 109us/step - loss: 0.0534 - accuracy: 0.9833 - val_loss: 0.0418 - val_accuracy: 0.9667\n",
      "Predict:  [[5.1 3.4 1.4 1.6]]\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "# Neural-Network\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "def handle_non_numerical_data(df):\n",
    "    columns = df.columns.values\n",
    "    for column in columns:\n",
    "        text_digit_vals = {}\n",
    "        def convert_to_int(val):\n",
    "            return text_digit_vals[val]\n",
    "\n",
    "        if df[column].dtype != np.int64 and df[column].dtype != np.float64:\n",
    "            column_contents = df[column].values.tolist()\n",
    "            unique_elements = set(column_contents)\n",
    "            x = 0\n",
    "            for unique in unique_elements:\n",
    "                if unique not in text_digit_vals:\n",
    "                    text_digit_vals[unique] = x\n",
    "                    x+=1\n",
    "\n",
    "            df[column] = list(map(convert_to_int, df[column]))\n",
    "\n",
    "    return df\n",
    "\n",
    "df1 = df.copy()\n",
    "df1 = handle_non_numerical_data(df1)\n",
    "print(\"Altered data set:\")\n",
    "print(df1)\n",
    "X1 = np.array(df1.drop(['iris'], 1))\n",
    "y1 = np.array(df1['iris'])\n",
    "\n",
    "np.random.seed(2)\n",
    "classifications = 2\n",
    "\n",
    "X_train1, X_test1, y_train1, y_test1 = cross_validation.train_test_split(X1, y1, test_size=0.2, random_state=2)\n",
    "\n",
    "# convert output values to one-shot\n",
    "y_train1 = keras.utils.to_categorical(y_train1-1, classifications)\n",
    "y_test1 = keras.utils.to_categorical(y_test1-1, classifications)\n",
    "\n",
    "\n",
    "# creating model\n",
    "cfl = Sequential()\n",
    "cfl.add(Dense(4, input_dim=4, activation='relu'))\n",
    "cfl.add(Dense(3, activation='relu'))\n",
    "cfl.add(Dense(classifications, activation='softmax'))\n",
    "\n",
    "# compile and fit model\n",
    "cfl.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])\n",
    "# batch size is to find the gradient quickly\n",
    "#model.fit(X_train1, y_train1, batch_size=15, epochs=2500, validation_data=(X_test1, y_test1))\n",
    "# To try with varying epochs and its corresponding accuracies\n",
    "cfl.fit(X_train1, y_train1, batch_size=15, epochs=2500, validation_data=(X_test1, y_test1))\n",
    "#print('Random Forest Accuracy is :', clf.score(X_test1,y_test1))\n",
    "print(\"Predict: \", example_measures)\n",
    "prediction = clf.predict(example_measures)\n",
    "print(prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.31333333333333335\n",
      "Predict:  [[5.1 3.4 1.4 1.6]]\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "#K-Means Cluster\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "from sklearn.cluster import KMeans\n",
    "style.use('ggplot')\n",
    "\n",
    "#Find 2 clusters\n",
    "clf = KMeans(n_clusters=2)\n",
    "clf.fit(X1)#X will be grouped into 2 clusters\n",
    "\n",
    "#Find cluster centers and assign labels\n",
    "centroids = clf.cluster_centers_\n",
    "labels = clf.labels_\n",
    "\n",
    "colors = [\"g.\",\"r.\",\"c.\",\"y.\"]\n",
    "for i in range(len(X1)):\n",
    "    plt.plot(X1[i][0], X1[i][1], colors[labels[i]], markersize = 10)\n",
    "plt.scatter(centroids[:, 0],centroids[:, 1], marker = \"x\", s=150, linewidths = 5, zorder = 10)\n",
    "plt.show()\n",
    "\n",
    "correct = 0\n",
    "for i in range(len(X1)):\n",
    "    predict_me = np.array(X1[i].astype(float))\n",
    "    predict_me = predict_me.reshape(-1, len(predict_me))\n",
    "    prediction = clf.predict(predict_me)\n",
    "    if prediction[0] == y1[i]:\n",
    "        correct += 1\n",
    "\n",
    "print('Accuracy',float(correct)/float(len(X1)))\n",
    "print(\"Predict: \", example_measures)\n",
    "prediction = clf.predict(example_measures)\n",
    "print(prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
